{"traceEvents": [{"ph": "M", "pid": 10440, "tid": 10440, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 10440, "tid": 20604, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 10440, "tid": 20604, "ts": 24525061608.736, "ph": "X", "cat": "fee", "dur": 1155.497, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525061585.536, "ph": "X", "cat": "fee", "dur": 1188.497, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525061414.136, "ph": "X", "cat": "fee", "dur": 1360.197, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525061393.436, "ph": "X", "cat": "fee", "dur": 1387.697, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525061389.736, "ph": "X", "cat": "fee", "dur": 1902.496, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dis.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525061383.036, "ph": "X", "cat": "fee", "dur": 1909.696, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525061382.736, "ph": "X", "cat": "fee", "dur": 1910.196, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525060740.837, "ph": "X", "cat": "fee", "dur": 2552.795, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525060717.237, "ph": "X", "cat": "fee", "dur": 2578.295, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525060540.838, "ph": "X", "cat": "fee", "dur": 2754.995, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525060518.738, "ph": "X", "cat": "fee", "dur": 2786.095, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525060514.738, "ph": "X", "cat": "fee", "dur": 5070.49, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\inspect.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525060506.438, "ph": "X", "cat": "fee", "dur": 5079.29, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525060506.138, "ph": "X", "cat": "fee", "dur": 5079.99, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525059565.439, "ph": "X", "cat": "fee", "dur": 6020.989, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525059541.139, "ph": "X", "cat": "fee", "dur": 6047.189, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525059365.44, "ph": "X", "cat": "fee", "dur": 6223.188, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525059343.14, "ph": "X", "cat": "fee", "dur": 6254.288, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525059338.04, "ph": "X", "cat": "fee", "dur": 6681.988, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dataclasses.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525059332.64, "ph": "X", "cat": "fee", "dur": 6687.988, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525059332.34, "ph": "X", "cat": "fee", "dur": 6688.588, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525058667.641, "ph": "X", "cat": "fee", "dur": 7353.586, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525058643.341, "ph": "X", "cat": "fee", "dur": 7379.686, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525058448.141, "ph": "X", "cat": "fee", "dur": 7575.286, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525058421.341, "ph": "X", "cat": "fee", "dur": 7610.586, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525067041.326, "ph": "X", "cat": "fee", "dur": 1098.698, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525067018.726, "ph": "X", "cat": "fee", "dur": 1127.097, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525071998.817, "ph": "X", "cat": "fee", "dur": 1787.396, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525071974.717, "ph": "X", "cat": "fee", "dur": 1818.196, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525071796.018, "ph": "X", "cat": "fee", "dur": 1997.196, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525071774.218, "ph": "X", "cat": "fee", "dur": 2027.796, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525071713.318, "ph": "X", "cat": "fee", "dur": 2637.895, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\urllib\\parse.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525071706.718, "ph": "X", "cat": "fee", "dur": 2645.195, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525071706.418, "ph": "X", "cat": "fee", "dur": 2645.695, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525071013.719, "ph": "X", "cat": "fee", "dur": 3338.594, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525070989.719, "ph": "X", "cat": "fee", "dur": 3364.494, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525069993.121, "ph": "X", "cat": "fee", "dur": 4363.292, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525069972.121, "ph": "X", "cat": "fee", "dur": 4392.292, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525069958.721, "ph": "X", "cat": "fee", "dur": 4691.191, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\pathlib.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525069953.821, "ph": "X", "cat": "fee", "dur": 4696.591, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525069953.521, "ph": "X", "cat": "fee", "dur": 4697.091, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525069186.122, "ph": "X", "cat": "fee", "dur": 5464.79, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525069162.222, "ph": "X", "cat": "fee", "dur": 5490.49, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525068964.023, "ph": "X", "cat": "fee", "dur": 5688.989, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525068945.023, "ph": "X", "cat": "fee", "dur": 5716.489, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525076380.409, "ph": "X", "cat": "fee", "dur": 1546.297, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525076355.909, "ph": "X", "cat": "fee", "dur": 1575.697, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525076109.71, "ph": "X", "cat": "fee", "dur": 1824.496, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525076087.91, "ph": "X", "cat": "fee", "dur": 1853.296, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525075898.61, "ph": "X", "cat": "fee", "dur": 2047.096, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\zipfile\\__init__.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525075892.51, "ph": "X", "cat": "fee", "dur": 2053.596, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525075892.21, "ph": "X", "cat": "fee", "dur": 2054.296, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525074931.512, "ph": "X", "cat": "fee", "dur": 3015.294, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525074906.812, "ph": "X", "cat": "fee", "dur": 3041.594, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525074682.612, "ph": "X", "cat": "fee", "dur": 3265.994, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525074663.312, "ph": "X", "cat": "fee", "dur": 3299.694, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525080438.602, "ph": "X", "cat": "fee", "dur": 1044.598, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525080409.002, "ph": "X", "cat": "fee", "dur": 1086.598, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525083613.897, "ph": "X", "cat": "fee", "dur": 1586.297, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525083587.697, "ph": "X", "cat": "fee", "dur": 1617.397, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525083385.497, "ph": "X", "cat": "fee", "dur": 1819.996, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525083358.097, "ph": "X", "cat": "fee", "dur": 1857.896, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525083353.197, "ph": "X", "cat": "fee", "dur": 1886.996, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_parseaddr.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525083350.097, "ph": "X", "cat": "fee", "dur": 1890.396, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525083349.697, "ph": "X", "cat": "fee", "dur": 1890.996, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525082600.798, "ph": "X", "cat": "fee", "dur": 2640.095, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525082574.098, "ph": "X", "cat": "fee", "dur": 2668.195, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525082472.999, "ph": "X", "cat": "fee", "dur": 2771.995, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525082444.599, "ph": "X", "cat": "fee", "dur": 2805.394, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525086168.092, "ph": "X", "cat": "fee", "dur": 2493.595, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\charset.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525086163.992, "ph": "X", "cat": "fee", "dur": 2498.295, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525086163.692, "ph": "X", "cat": "fee", "dur": 2498.695, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525085407.993, "ph": "X", "cat": "fee", "dur": 3254.594, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525085379.993, "ph": "X", "cat": "fee", "dur": 3283.894, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525085276.394, "ph": "X", "cat": "fee", "dur": 3388.793, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525085254.494, "ph": "X", "cat": "fee", "dur": 3417.493, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525082432.999, "ph": "X", "cat": "fee", "dur": 7213.287, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\utils.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525082428.899, "ph": "X", "cat": "fee", "dur": 7217.987, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525082428.599, "ph": "X", "cat": "fee", "dur": 7218.387, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525081662.2, "ph": "X", "cat": "fee", "dur": 7985.085, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525081635.1, "ph": "X", "cat": "fee", "dur": 8015.085, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525081529.5, "ph": "X", "cat": "fee", "dur": 8122.985, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525081504.8, "ph": "X", "cat": "fee", "dur": 8157.585, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525081503.4, "ph": "X", "cat": "fee", "dur": 8160.185, "name": "builtins.__import__"}, {"pid": 10440, "tid": 20604, "ts": 24525081503.3, "ph": "X", "cat": "fee", "dur": 8160.385, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525081500.4, "ph": "X", "cat": "fee", "dur": 8163.785, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1390)"}, {"pid": 10440, "tid": 20604, "ts": 24525090443.184, "ph": "X", "cat": "fee", "dur": 1058.698, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525090420.084, "ph": "X", "cat": "fee", "dur": 1084.998, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525090324.285, "ph": "X", "cat": "fee", "dur": 1182.897, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525090303.285, "ph": "X", "cat": "fee", "dur": 1212.697, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525090302.285, "ph": "X", "cat": "fee", "dur": 1214.597, "name": "builtins.__import__"}, {"pid": 10440, "tid": 20604, "ts": 24525090302.185, "ph": "X", "cat": "fee", "dur": 1214.897, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525090299.885, "ph": "X", "cat": "fee", "dur": 1217.497, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1390)"}, {"pid": 10440, "tid": 20604, "ts": 24525090297.485, "ph": "X", "cat": "fee", "dur": 1326.997, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_policybase.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525090295.285, "ph": "X", "cat": "fee", "dur": 1329.597, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525090294.985, "ph": "X", "cat": "fee", "dur": 1329.997, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525089803.885, "ph": "X", "cat": "fee", "dur": 1821.396, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525089776.486, "ph": "X", "cat": "fee", "dur": 1850.396, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525089689.286, "ph": "X", "cat": "fee", "dur": 1939.496, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525089668.386, "ph": "X", "cat": "fee", "dur": 1966.996, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525080404.202, "ph": "X", "cat": "fee", "dur": 12641.477, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\message.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525080400.802, "ph": "X", "cat": "fee", "dur": 12645.477, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525080400.402, "ph": "X", "cat": "fee", "dur": 12646.077, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525079414.804, "ph": "X", "cat": "fee", "dur": 13631.975, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525079385.704, "ph": "X", "cat": "fee", "dur": 13671.175, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525079093.905, "ph": "X", "cat": "fee", "dur": 13964.675, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525079055.605, "ph": "X", "cat": "fee", "dur": 14008.675, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525093086.68, "ph": "X", "cat": "fee", "dur": 1024.998, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525093068.88, "ph": "X", "cat": "fee", "dur": 1048.198, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525079048.205, "ph": "X", "cat": "fee", "dur": 15168.772, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\_adapters.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525079045.105, "ph": "X", "cat": "fee", "dur": 15172.872, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525079044.605, "ph": "X", "cat": "fee", "dur": 15173.472, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525078285.606, "ph": "X", "cat": "fee", "dur": 15932.671, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525078256.406, "ph": "X", "cat": "fee", "dur": 15963.371, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525078010.306, "ph": "X", "cat": "fee", "dur": 16211.071, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525077994.107, "ph": "X", "cat": "fee", "dur": 16233.971, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525077992.907, "ph": "X", "cat": "fee", "dur": 16236.871, "name": "builtins.__import__"}, {"pid": 10440, "tid": 20604, "ts": 24525077992.707, "ph": "X", "cat": "fee", "dur": 16237.271, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525094231.978, "ph": "X", "cat": "fee", "dur": 1000.398, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525077990.807, "ph": "X", "cat": "fee", "dur": 17242.069, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1390)"}, {"pid": 10440, "tid": 20604, "ts": 24525098027.871, "ph": "X", "cat": "fee", "dur": 1708.596, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\_common.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525098024.571, "ph": "X", "cat": "fee", "dur": 1712.196, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525098024.371, "ph": "X", "cat": "fee", "dur": 1712.496, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525097663.771, "ph": "X", "cat": "fee", "dur": 2073.396, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525097638.872, "ph": "X", "cat": "fee", "dur": 2099.696, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525097406.172, "ph": "X", "cat": "fee", "dur": 2334.495, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525097385.772, "ph": "X", "cat": "fee", "dur": 2361.495, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525097383.172, "ph": "X", "cat": "fee", "dur": 3015.894, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\__init__.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525097381.572, "ph": "X", "cat": "fee", "dur": 3017.994, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525097381.372, "ph": "X", "cat": "fee", "dur": 3018.294, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525097070.673, "ph": "X", "cat": "fee", "dur": 3329.294, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525097049.573, "ph": "X", "cat": "fee", "dur": 3351.494, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525096925.773, "ph": "X", "cat": "fee", "dur": 3477.093, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525096891.573, "ph": "X", "cat": "fee", "dur": 3515.793, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525096878.673, "ph": "X", "cat": "fee", "dur": 3714.493, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\abc.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525096874.173, "ph": "X", "cat": "fee", "dur": 3719.393, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525096873.973, "ph": "X", "cat": "fee", "dur": 3719.993, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525096529.173, "ph": "X", "cat": "fee", "dur": 4064.992, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525096507.574, "ph": "X", "cat": "fee", "dur": 4088.092, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525096417.474, "ph": "X", "cat": "fee", "dur": 4179.492, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525096400.474, "ph": "X", "cat": "fee", "dur": 4203.792, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525067014.826, "ph": "X", "cat": "fee", "dur": 34359.438, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525067009.826, "ph": "X", "cat": "fee", "dur": 34365.438, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525067009.526, "ph": "X", "cat": "fee", "dur": 34366.038, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525066233.427, "ph": "X", "cat": "fee", "dur": 35142.437, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525066207.128, "ph": "X", "cat": "fee", "dur": 35170.737, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525066056.528, "ph": "X", "cat": "fee", "dur": 35323.337, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525066037.128, "ph": "X", "cat": "fee", "dur": 35351.836, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525102069.464, "ph": "X", "cat": "fee", "dur": 2035.596, "name": "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525102067.564, "ph": "X", "cat": "fee", "dur": 2037.996, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525102067.364, "ph": "X", "cat": "fee", "dur": 2038.396, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525101650.964, "ph": "X", "cat": "fee", "dur": 2454.995, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525101626.664, "ph": "X", "cat": "fee", "dur": 2481.395, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525101411.565, "ph": "X", "cat": "fee", "dur": 2698.795, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525101393.965, "ph": "X", "cat": "fee", "dur": 2725.395, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525104133.56, "ph": "X", "cat": "fee", "dur": 1818.796, "name": "builtins.next"}, {"pid": 10440, "tid": 20604, "ts": 24525104124.16, "ph": "X", "cat": "fee", "dur": 1833.396, "name": "Distribution.from_name (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:383)"}, {"pid": 10440, "tid": 20604, "ts": 24525104122.66, "ph": "X", "cat": "fee", "dur": 1835.096, "name": "distribution (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:856)"}, {"pid": 10440, "tid": 20604, "ts": 24525106785.555, "ph": "X", "cat": "fee", "dur": 1048.998, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525106759.155, "ph": "X", "cat": "fee", "dur": 1084.598, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525106755.855, "ph": "X", "cat": "fee", "dur": 1123.897, "name": "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\parser.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525106753.655, "ph": "X", "cat": "fee", "dur": 1126.397, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525106753.355, "ph": "X", "cat": "fee", "dur": 1126.897, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525106326.756, "ph": "X", "cat": "fee", "dur": 1553.697, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525106301.156, "ph": "X", "cat": "fee", "dur": 1580.597, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525106191.356, "ph": "X", "cat": "fee", "dur": 1692.296, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525106169.056, "ph": "X", "cat": "fee", "dur": 1719.696, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525106167.456, "ph": "X", "cat": "fee", "dur": 1965.896, "name": "message_from_string (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\__init__.py:31)"}, {"pid": 10440, "tid": 20604, "ts": 24525105959.257, "ph": "X", "cat": "fee", "dur": 2207.696, "name": "Distribution.metadata (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:436)"}, {"pid": 10440, "tid": 20604, "ts": 24525105958.457, "ph": "X", "cat": "fee", "dur": 2216.796, "name": "Distribution.version (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:464)"}, {"pid": 10440, "tid": 20604, "ts": 24525104122.16, "ph": "X", "cat": "fee", "dur": 4053.792, "name": "version (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:882)"}, {"pid": 10440, "tid": 20604, "ts": 24525058418.741, "ph": "X", "cat": "fee", "dur": 50335.61, "name": "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525058416.341, "ph": "X", "cat": "fee", "dur": 50338.81, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24525058416.041, "ph": "X", "cat": "fee", "dur": 50339.21, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)"}, {"pid": 10440, "tid": 20604, "ts": 24525057891.142, "ph": "X", "cat": "fee", "dur": 50864.309, "name": "_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)"}, {"pid": 10440, "tid": 20604, "ts": 24525057847.142, "ph": "X", "cat": "fee", "dur": 50910.109, "name": "_load_unlocked (<frozen importlib._bootstrap>:911)"}, {"pid": 10440, "tid": 20604, "ts": 24525057318.143, "ph": "X", "cat": "fee", "dur": 51439.508, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)"}, {"pid": 10440, "tid": 20604, "ts": 24525057222.044, "ph": "X", "cat": "fee", "dur": 51545.908, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)"}, {"pid": 10440, "tid": 20604, "ts": 24525391085.148, "ph": "X", "cat": "fee", "dur": 4933.791, "name": "Random._randbelow_with_getrandbits (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:242)"}, {"pid": 10440, "tid": 20604, "ts": 24525391084.448, "ph": "X", "cat": "fee", "dur": 4935.891, "name": "Random.randrange (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:291)"}, {"pid": 10440, "tid": 20604, "ts": 24525391084.248, "ph": "X", "cat": "fee", "dur": 4936.691, "name": "Random.randint (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:332)"}, {"pid": 10440, "tid": 20604, "ts": 24525452460.539, "ph": "X", "cat": "fee", "dur": 1244.397, "name": "set.add"}, {"pid": 10440, "tid": 20604, "ts": 24525793288.931, "ph": "X", "cat": "fee", "dur": 2757.995, "name": "set.add"}, {"pid": 10440, "tid": 20604, "ts": 24526497249.977, "ph": "X", "cat": "fee", "dur": 5685.289, "name": "set.add"}, {"pid": 10440, "tid": 20604, "ts": 24525109318.451, "ph": "X", "cat": "fee", "dur": 2577541.605, "name": "ObstacleStore.from_config (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:28)"}, {"pid": 10440, "tid": 20604, "ts": 24525109145.551, "ph": "X", "cat": "fee", "dur": 2577716.405, "name": "AStarPathfinder._generate_grid (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:64)"}, {"pid": 10440, "tid": 20604, "ts": 24525109123.351, "ph": "X", "cat": "fee", "dur": 2577741.705, "name": "AStarPathfinder.__init__ (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:55)"}, {"pid": 10440, "tid": 20604, "ts": 24525109120.551, "ph": "X", "cat": "fee", "dur": 2577784.305, "name": "AStarPathfinder.__init__ (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py:71)"}, {"pid": 10440, "tid": 20604, "ts": 24525057218.044, "ph": "X", "cat": "fee", "dur": 2629692.912, "name": "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\main.py:1)"}, {"pid": 10440, "tid": 20604, "ts": 24525057214.844, "ph": "X", "cat": "fee", "dur": 2629698.912, "name": "builtins.exec"}, {"pid": 10440, "tid": 20604, "ts": 24527687005.256, "ph": "X", "cat": "fee", "dur": 228.799, "name": "Finalize.__call__ (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:208)"}, {"pid": 10440, "tid": 20604, "ts": 24527686990.656, "ph": "X", "cat": "fee", "dur": 243.499, "name": "_run_finalizers (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:271)"}, {"pid": 10440, "tid": 20604, "ts": 24527686942.456, "ph": "X", "cat": "fee", "dur": 291.799, "name": "_exit_function (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:323)"}, {"pid": 10440, "tid": 20604, "ts": 24527686927.056, "ph": "X", "cat": "fee", "dur": 307.299, "name": "atexit._run_exitfuncs"}], "viztracer_metadata": {"overflow": false, "version": "1.1.1"}, "file_info": {"files": {"<frozen importlib._bootstrap_external>": ["\"\"\"Core implementation of path-based import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module in the early\n# stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\n# Module injected manually by _set_bootstrap_module()\n_bootstrap = None\n\n# Import builtin modules\nimport _imp\nimport _io\nimport sys\nimport _warnings\nimport marshal\n\n\n_MS_WINDOWS = (sys.platform == 'win32')\nif _MS_WINDOWS:\n    import nt as _os\n    import winreg\nelse:\n    import posix as _os\n\n\nif _MS_WINDOWS:\n    path_separators = ['\\\\', '/']\nelse:\n    path_separators = ['/']\n# Assumption made in _path_join()\nassert all(len(sep) == 1 for sep in path_separators)\npath_sep = path_separators[0]\npath_sep_tuple = tuple(path_separators)\npath_separators = ''.join(path_separators)\n_pathseps_with_colon = {f':{s}' for s in path_separators}\n\n\n# Bootstrap-related code ######################################################\n_CASE_INSENSITIVE_PLATFORMS_STR_KEY = 'win',\n_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY = 'cygwin', 'darwin'\n_CASE_INSENSITIVE_PLATFORMS =  (_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY\n                                + _CASE_INSENSITIVE_PLATFORMS_STR_KEY)\n\n\ndef _make_relax_case():\n    if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS_STR_KEY):\n            key = 'PYTHONCASEOK'\n        else:\n            key = b'PYTHONCASEOK'\n\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively and ignore environment flags are not set.\"\"\"\n            return not sys.flags.ignore_environment and key in _os.environ\n    else:\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively.\"\"\"\n            return False\n    return _relax_case\n\n_relax_case = _make_relax_case()\n\n\ndef _pack_uint32(x):\n    \"\"\"Convert a 32-bit integer to little-endian.\"\"\"\n    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')\n\n\ndef _unpack_uint32(data):\n    \"\"\"Convert 4 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 4\n    return int.from_bytes(data, 'little')\n\ndef _unpack_uint16(data):\n    \"\"\"Convert 2 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 2\n    return int.from_bytes(data, 'little')\n\n\nif _MS_WINDOWS:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        if not path_parts:\n            return \"\"\n        if len(path_parts) == 1:\n            return path_parts[0]\n        root = \"\"\n        path = []\n        for new_root, tail in map(_os._path_splitroot, path_parts):\n            if new_root.startswith(path_sep_tuple) or new_root.endswith(path_sep_tuple):\n                root = new_root.rstrip(path_separators) or root\n                path = [path_sep + tail]\n            elif new_root.endswith(':'):\n                if root.casefold() != new_root.casefold():\n                    # Drive relative paths have to be resolved by the OS, so we reset the\n                    # tail but do not add a path_sep prefix.\n                    root = new_root\n                    path = [tail]\n                else:\n                    path.append(tail)\n            else:\n                root = new_root or root\n                path.append(tail)\n        path = [p.rstrip(path_separators) for p in path if p]\n        if len(path) == 1 and not path[0]:\n            # Avoid losing the root's trailing separator when joining with nothing\n            return root + path_sep\n        return root + path_sep.join(path)\n\nelse:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        return path_sep.join([part.rstrip(path_separators)\n                              for part in path_parts if part])\n\n\ndef _path_split(path):\n    \"\"\"Replacement for os.path.split().\"\"\"\n    i = max(path.rfind(p) for p in path_separators)\n    if i < 0:\n        return '', path\n    return path[:i], path[i + 1:]\n\n\ndef _path_stat(path):\n    \"\"\"Stat the path.\n\n    Made a separate function to make it easier to override in experiments\n    (e.g. cache stat results).\n\n    \"\"\"\n    return _os.stat(path)\n\n\ndef _path_is_mode_type(path, mode):\n    \"\"\"Test whether the path is the specified mode type.\"\"\"\n    try:\n        stat_info = _path_stat(path)\n    except OSError:\n        return False\n    return (stat_info.st_mode & 0o170000) == mode\n\n\ndef _path_isfile(path):\n    \"\"\"Replacement for os.path.isfile.\"\"\"\n    return _path_is_mode_type(path, 0o100000)\n\n\ndef _path_isdir(path):\n    \"\"\"Replacement for os.path.isdir.\"\"\"\n    if not path:\n        path = _os.getcwd()\n    return _path_is_mode_type(path, 0o040000)\n\n\nif _MS_WINDOWS:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        if not path:\n            return False\n        root = _os._path_splitroot(path)[0].replace('/', '\\\\')\n        return len(root) > 1 and (root.startswith('\\\\\\\\') or root.endswith('\\\\'))\n\nelse:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        return path.startswith(path_separators)\n\n\ndef _path_abspath(path):\n    \"\"\"Replacement for os.path.abspath.\"\"\"\n    if not _path_isabs(path):\n        for sep in path_separators:\n            path = path.removeprefix(f\".{sep}\")\n        return _path_join(_os.getcwd(), path)\n    else:\n        return path\n\n\ndef _write_atomic(path, data, mode=0o666):\n    \"\"\"Best-effort function to write data to a path atomically.\n    Be prepared to handle a FileExistsError if concurrent writing of the\n    temporary file is attempted.\"\"\"\n    # id() is used to generate a pseudo-random filename.\n    path_tmp = f'{path}.{id(path)}'\n    fd = _os.open(path_tmp,\n                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)\n    try:\n        # We first write data to a temporary file, and then use os.replace() to\n        # perform an atomic rename.\n        with _io.FileIO(fd, 'wb') as file:\n            bytes_written = file.write(data)\n        if bytes_written != len(data):\n            # Raise an OSError so the 'except' below cleans up the partially\n            # written file.\n            raise OSError(\"os.write() didn't write the full pyc file\")\n        _os.replace(path_tmp, path)\n    except OSError:\n        try:\n            _os.unlink(path_tmp)\n        except OSError:\n            pass\n        raise\n\n\n_code_type = type(_write_atomic.__code__)\n\n\n# Finder/loader utility code ###############################################\n\n# Magic word to reject .pyc files generated by other Python versions.\n# It should change for each incompatible change to the bytecode.\n#\n# The value of CR and LF is incorporated so if you ever read or write\n# a .pyc file in text mode the magic number will be wrong; also, the\n# Apple MPW compiler swaps their values, botching string constants.\n#\n# There were a variety of old schemes for setting the magic number.\n# The current working scheme is to increment the previous value by\n# 10.\n#\n# Starting with the adoption of PEP 3147 in Python 3.2, every bump in magic\n# number also includes a new \"magic tag\", i.e. a human readable string used\n# to represent the magic number in __pycache__ directories.  When you change\n# the magic number, you must also set a new unique magic tag.  Generally this\n# can be named after the Python major version of the magic number bump, but\n# it can really be anything, as long as it's different than anything else\n# that's come before.  The tags are included in the following table, starting\n# with Python 3.2a0.\n#\n# Known values:\n#  Python 1.5:   20121\n#  Python 1.5.1: 20121\n#     Python 1.5.2: 20121\n#     Python 1.6:   50428\n#     Python 2.0:   50823\n#     Python 2.0.1: 50823\n#     Python 2.1:   60202\n#     Python 2.1.1: 60202\n#     Python 2.1.2: 60202\n#     Python 2.2:   60717\n#     Python 2.3a0: 62011\n#     Python 2.3a0: 62021\n#     Python 2.3a0: 62011 (!)\n#     Python 2.4a0: 62041\n#     Python 2.4a3: 62051\n#     Python 2.4b1: 62061\n#     Python 2.5a0: 62071\n#     Python 2.5a0: 62081 (ast-branch)\n#     Python 2.5a0: 62091 (with)\n#     Python 2.5a0: 62092 (changed WITH_CLEANUP opcode)\n#     Python 2.5b3: 62101 (fix wrong code: for x, in ...)\n#     Python 2.5b3: 62111 (fix wrong code: x += yield)\n#     Python 2.5c1: 62121 (fix wrong lnotab with for loops and\n#                          storing constants that should have been removed)\n#     Python 2.5c2: 62131 (fix wrong code: for x, in ... in listcomp/genexp)\n#     Python 2.6a0: 62151 (peephole optimizations and STORE_MAP opcode)\n#     Python 2.6a1: 62161 (WITH_CLEANUP optimization)\n#     Python 2.7a0: 62171 (optimize list comprehensions/change LIST_APPEND)\n#     Python 2.7a0: 62181 (optimize conditional branches:\n#                          introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE)\n#     Python 2.7a0  62191 (introduce SETUP_WITH)\n#     Python 2.7a0  62201 (introduce BUILD_SET)\n#     Python 2.7a0  62211 (introduce MAP_ADD and SET_ADD)\n#     Python 3000:   3000\n#                    3010 (removed UNARY_CONVERT)\n#                    3020 (added BUILD_SET)\n#                    3030 (added keyword-only parameters)\n#                    3040 (added signature annotations)\n#                    3050 (print becomes a function)\n#                    3060 (PEP 3115 metaclass syntax)\n#                    3061 (string literals become unicode)\n#                    3071 (PEP 3109 raise changes)\n#                    3081 (PEP 3137 make __file__ and __name__ unicode)\n#                    3091 (kill str8 interning)\n#                    3101 (merge from 2.6a0, see 62151)\n#                    3103 (__file__ points to source file)\n#     Python 3.0a4: 3111 (WITH_CLEANUP optimization).\n#     Python 3.0b1: 3131 (lexical exception stacking, including POP_EXCEPT\n                          #3021)\n#     Python 3.1a1: 3141 (optimize list, set and dict comprehensions:\n#                         change LIST_APPEND and SET_ADD, add MAP_ADD #2183)\n#     Python 3.1a1: 3151 (optimize conditional branches:\n#                         introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE\n                          #4715)\n#     Python 3.2a1: 3160 (add SETUP_WITH #6101)\n#                   tag: cpython-32\n#     Python 3.2a2: 3170 (add DUP_TOP_TWO, remove DUP_TOPX and ROT_FOUR #9225)\n#                   tag: cpython-32\n#     Python 3.2a3  3180 (add DELETE_DEREF #4617)\n#     Python 3.3a1  3190 (__class__ super closure changed)\n#     Python 3.3a1  3200 (PEP 3155 __qualname__ added #13448)\n#     Python 3.3a1  3210 (added size modulo 2**32 to the pyc header #13645)\n#     Python 3.3a2  3220 (changed PEP 380 implementation #14230)\n#     Python 3.3a4  3230 (revert changes to implicit __class__ closure #14857)\n#     Python 3.4a1  3250 (evaluate positional default arguments before\n#                        keyword-only defaults #16967)\n#     Python 3.4a1  3260 (add LOAD_CLASSDEREF; allow locals of class to override\n#                        free vars #17853)\n#     Python 3.4a1  3270 (various tweaks to the __class__ closure #12370)\n#     Python 3.4a1  3280 (remove implicit class argument)\n#     Python 3.4a4  3290 (changes to __qualname__ computation #19301)\n#     Python 3.4a4  3300 (more changes to __qualname__ computation #19301)\n#     Python 3.4rc2 3310 (alter __qualname__ computation #20625)\n#     Python 3.5a1  3320 (PEP 465: Matrix multiplication operator #21176)\n#     Python 3.5b1  3330 (PEP 448: Additional Unpacking Generalizations #2292)\n#     Python 3.5b2  3340 (fix dictionary display evaluation order #11205)\n#     Python 3.5b3  3350 (add GET_YIELD_FROM_ITER opcode #24400)\n#     Python 3.5.2  3351 (fix BUILD_MAP_UNPACK_WITH_CALL opcode #27286)\n#     Python 3.6a0  3360 (add FORMAT_VALUE opcode #25483)\n#     Python 3.6a1  3361 (lineno delta of code.co_lnotab becomes signed #26107)\n#     Python 3.6a2  3370 (16 bit wordcode #26647)\n#     Python 3.6a2  3371 (add BUILD_CONST_KEY_MAP opcode #27140)\n#     Python 3.6a2  3372 (MAKE_FUNCTION simplification, remove MAKE_CLOSURE\n#                         #27095)\n#     Python 3.6b1  3373 (add BUILD_STRING opcode #27078)\n#     Python 3.6b1  3375 (add SETUP_ANNOTATIONS and STORE_ANNOTATION opcodes\n#                         #27985)\n#     Python 3.6b1  3376 (simplify CALL_FUNCTIONs & BUILD_MAP_UNPACK_WITH_CALL\n                          #27213)\n#     Python 3.6b1  3377 (set __class__ cell from type.__new__ #23722)\n#     Python 3.6b2  3378 (add BUILD_TUPLE_UNPACK_WITH_CALL #28257)\n#     Python 3.6rc1 3379 (more thorough __class__ validation #23722)\n#     Python 3.7a1  3390 (add LOAD_METHOD and CALL_METHOD opcodes #26110)\n#     Python 3.7a2  3391 (update GET_AITER #31709)\n#     Python 3.7a4  3392 (PEP 552: Deterministic pycs #31650)\n#     Python 3.7b1  3393 (remove STORE_ANNOTATION opcode #32550)\n#     Python 3.7b5  3394 (restored docstring as the first stmt in the body;\n#                         this might affected the first line number #32911)\n#     Python 3.8a1  3400 (move frame block handling to compiler #17611)\n#     Python 3.8a1  3401 (add END_ASYNC_FOR #33041)\n#     Python 3.8a1  3410 (PEP570 Python Positional-Only Parameters #36540)\n#     Python 3.8b2  3411 (Reverse evaluation order of key: value in dict\n#                         comprehensions #35224)\n#     Python 3.8b2  3412 (Swap the position of positional args and positional\n#                         only args in ast.arguments #37593)\n#     Python 3.8b4  3413 (Fix \"break\" and \"continue\" in \"finally\" #37830)\n#     Python 3.9a0  3420 (add LOAD_ASSERTION_ERROR #34880)\n#     Python 3.9a0  3421 (simplified bytecode for with blocks #32949)\n#     Python 3.9a0  3422 (remove BEGIN_FINALLY, END_FINALLY, CALL_FINALLY, POP_FINALLY bytecodes #33387)\n#     Python 3.9a2  3423 (add IS_OP, CONTAINS_OP and JUMP_IF_NOT_EXC_MATCH bytecodes #39156)\n#     Python 3.9a2  3424 (simplify bytecodes for *value unpacking)\n#     Python 3.9a2  3425 (simplify bytecodes for **value unpacking)\n#     Python 3.10a1 3430 (Make 'annotations' future by default)\n#     Python 3.10a1 3431 (New line number table format -- PEP 626)\n#     Python 3.10a2 3432 (Function annotation for MAKE_FUNCTION is changed from dict to tuple bpo-42202)\n#     Python 3.10a2 3433 (RERAISE restores f_lasti if oparg != 0)\n#     Python 3.10a6 3434 (PEP 634: Structural Pattern Matching)\n#     Python 3.10a7 3435 Use instruction offsets (as opposed to byte offsets).\n#     Python 3.10b1 3436 (Add GEN_START bytecode #43683)\n#     Python 3.10b1 3437 (Undo making 'annotations' future by default - We like to dance among core devs!)\n#     Python 3.10b1 3438 Safer line number table handling.\n#     Python 3.10b1 3439 (Add ROT_N)\n#     Python 3.11a1 3450 Use exception table for unwinding (\"zero cost\" exception handling)\n#     Python 3.11a1 3451 (Add CALL_METHOD_KW)\n#     Python 3.11a1 3452 (drop nlocals from marshaled code objects)\n#     Python 3.11a1 3453 (add co_fastlocalnames and co_fastlocalkinds)\n#     Python 3.11a1 3454 (compute cell offsets relative to locals bpo-43693)\n#     Python 3.11a1 3455 (add MAKE_CELL bpo-43693)\n#     Python 3.11a1 3456 (interleave cell args bpo-43693)\n#     Python 3.11a1 3457 (Change localsplus to a bytes object bpo-43693)\n#     Python 3.11a1 3458 (imported objects now don't use LOAD_METHOD/CALL_METHOD)\n#     Python 3.11a1 3459 (PEP 657: add end line numbers and column offsets for instructions)\n#     Python 3.11a1 3460 (Add co_qualname field to PyCodeObject bpo-44530)\n#     Python 3.11a1 3461 (JUMP_ABSOLUTE must jump backwards)\n#     Python 3.11a2 3462 (bpo-44511: remove COPY_DICT_WITHOUT_KEYS, change\n#                         MATCH_CLASS and MATCH_KEYS, and add COPY)\n#     Python 3.11a3 3463 (bpo-45711: JUMP_IF_NOT_EXC_MATCH no longer pops the\n#                         active exception)\n#     Python 3.11a3 3464 (bpo-45636: Merge numeric BINARY_*/INPLACE_* into\n#                         BINARY_OP)\n#     Python 3.11a3 3465 (Add COPY_FREE_VARS opcode)\n#     Python 3.11a4 3466 (bpo-45292: PEP-654 except*)\n#     Python 3.11a4 3467 (Change CALL_xxx opcodes)\n#     Python 3.11a4 3468 (Add SEND opcode)\n#     Python 3.11a4 3469 (bpo-45711: remove type, traceback from exc_info)\n#     Python 3.11a4 3470 (bpo-46221: PREP_RERAISE_STAR no longer pushes lasti)\n#     Python 3.11a4 3471 (bpo-46202: remove pop POP_EXCEPT_AND_RERAISE)\n#     Python 3.11a4 3472 (bpo-46009: replace GEN_START with POP_TOP)\n#     Python 3.11a4 3473 (Add POP_JUMP_IF_NOT_NONE/POP_JUMP_IF_NONE opcodes)\n#     Python 3.11a4 3474 (Add RESUME opcode)\n#     Python 3.11a5 3475 (Add RETURN_GENERATOR opcode)\n#     Python 3.11a5 3476 (Add ASYNC_GEN_WRAP opcode)\n#     Python 3.11a5 3477 (Replace DUP_TOP/DUP_TOP_TWO with COPY and\n#                         ROT_TWO/ROT_THREE/ROT_FOUR/ROT_N with SWAP)\n#     Python 3.11a5 3478 (New CALL opcodes)\n#     Python 3.11a5 3479 (Add PUSH_NULL opcode)\n#     Python 3.11a5 3480 (New CALL opcodes, second iteration)\n#     Python 3.11a5 3481 (Use inline cache for BINARY_OP)\n#     Python 3.11a5 3482 (Use inline caching for UNPACK_SEQUENCE and LOAD_GLOBAL)\n#     Python 3.11a5 3483 (Use inline caching for COMPARE_OP and BINARY_SUBSCR)\n#     Python 3.11a5 3484 (Use inline caching for LOAD_ATTR, LOAD_METHOD, and\n#                         STORE_ATTR)\n#     Python 3.11a5 3485 (Add an oparg to GET_AWAITABLE)\n#     Python 3.11a6 3486 (Use inline caching for PRECALL and CALL)\n#     Python 3.11a6 3487 (Remove the adaptive \"oparg counter\" mechanism)\n#     Python 3.11a6 3488 (LOAD_GLOBAL can push additional NULL)\n#     Python 3.11a6 3489 (Add JUMP_BACKWARD, remove JUMP_ABSOLUTE)\n#     Python 3.11a6 3490 (remove JUMP_IF_NOT_EXC_MATCH, add CHECK_EXC_MATCH)\n#     Python 3.11a6 3491 (remove JUMP_IF_NOT_EG_MATCH, add CHECK_EG_MATCH,\n#                         add JUMP_BACKWARD_NO_INTERRUPT, make JUMP_NO_INTERRUPT virtual)\n#     Python 3.11a7 3492 (make POP_JUMP_IF_NONE/NOT_NONE/TRUE/FALSE relative)\n#     Python 3.11a7 3493 (Make JUMP_IF_TRUE_OR_POP/JUMP_IF_FALSE_OR_POP relative)\n#     Python 3.11a7 3494 (New location info table)\n#     Python 3.11b4 3495 (Set line number of module's RESUME instr to 0 per PEP 626)\n#     Python 3.12a1 3500 (Remove PRECALL opcode)\n#     Python 3.12a1 3501 (YIELD_VALUE oparg == stack_depth)\n#     Python 3.12a1 3502 (LOAD_FAST_CHECK, no NULL-check in LOAD_FAST)\n#     Python 3.12a1 3503 (Shrink LOAD_METHOD cache)\n#     Python 3.12a1 3504 (Merge LOAD_METHOD back into LOAD_ATTR)\n#     Python 3.12a1 3505 (Specialization/Cache for FOR_ITER)\n#     Python 3.12a1 3506 (Add BINARY_SLICE and STORE_SLICE instructions)\n#     Python 3.12a1 3507 (Set lineno of module's RESUME to 0)\n#     Python 3.12a1 3508 (Add CLEANUP_THROW)\n#     Python 3.12a1 3509 (Conditional jumps only jump forward)\n#     Python 3.12a2 3510 (FOR_ITER leaves iterator on the stack)\n#     Python 3.12a2 3511 (Add STOPITERATION_ERROR instruction)\n#     Python 3.12a2 3512 (Remove all unused consts from code objects)\n#     Python 3.12a4 3513 (Add CALL_INTRINSIC_1 instruction, removed STOPITERATION_ERROR, PRINT_EXPR, IMPORT_STAR)\n#     Python 3.12a4 3514 (Remove ASYNC_GEN_WRAP, LIST_TO_TUPLE, and UNARY_POSITIVE)\n#     Python 3.12a5 3515 (Embed jump mask in COMPARE_OP oparg)\n#     Python 3.12a5 3516 (Add COMPARE_AND_BRANCH instruction)\n#     Python 3.12a5 3517 (Change YIELD_VALUE oparg to exception block depth)\n#     Python 3.12a6 3518 (Add RETURN_CONST instruction)\n#     Python 3.12a6 3519 (Modify SEND instruction)\n#     Python 3.12a6 3520 (Remove PREP_RERAISE_STAR, add CALL_INTRINSIC_2)\n#     Python 3.12a7 3521 (Shrink the LOAD_GLOBAL caches)\n#     Python 3.12a7 3522 (Removed JUMP_IF_FALSE_OR_POP/JUMP_IF_TRUE_OR_POP)\n#     Python 3.12a7 3523 (Convert COMPARE_AND_BRANCH back to COMPARE_OP)\n#     Python 3.12a7 3524 (Shrink the BINARY_SUBSCR caches)\n#     Python 3.12b1 3525 (Shrink the CALL caches)\n#     Python 3.12b1 3526 (Add instrumentation support)\n#     Python 3.12b1 3527 (Add LOAD_SUPER_ATTR)\n#     Python 3.12b1 3528 (Add LOAD_SUPER_ATTR_METHOD specialization)\n#     Python 3.12b1 3529 (Inline list/dict/set comprehensions)\n#     Python 3.12b1 3530 (Shrink the LOAD_SUPER_ATTR caches)\n#     Python 3.12b1 3531 (Add PEP 695 changes)\n\n#     Python 3.13 will start with 3550\n\n#     Please don't copy-paste the same pre-release tag for new entries above!!!\n#     You should always use the *upcoming* tag. For example, if 3.12a6 came out\n#     a week ago, I should put \"Python 3.12a7\" next to my new magic number.\n\n# MAGIC must change whenever the bytecode emitted by the compiler may no\n# longer be understood by older implementations of the eval loop (usually\n# due to the addition of new opcodes).\n#\n# Starting with Python 3.11, Python 3.n starts with magic number 2900+50n.\n#\n# Whenever MAGIC_NUMBER is changed, the ranges in the magic_values array\n# in PC/launcher.c must also be updated.\n\nMAGIC_NUMBER = (3531).to_bytes(2, 'little') + b'\\r\\n'\n\n_RAW_MAGIC_NUMBER = int.from_bytes(MAGIC_NUMBER, 'little')  # For import.c\n\n_PYCACHE = '__pycache__'\n_OPT = 'opt-'\n\nSOURCE_SUFFIXES = ['.py']\nif _MS_WINDOWS:\n    SOURCE_SUFFIXES.append('.pyw')\n\nEXTENSION_SUFFIXES = _imp.extension_suffixes()\n\nBYTECODE_SUFFIXES = ['.pyc']\n# Deprecated.\nDEBUG_BYTECODE_SUFFIXES = OPTIMIZED_BYTECODE_SUFFIXES = BYTECODE_SUFFIXES\n\ndef cache_from_source(path, debug_override=None, *, optimization=None):\n    \"\"\"Given the path to a .py file, return the path to its .pyc file.\n\n    The .py file does not need to exist; this simply returns the path to the\n    .pyc file calculated as if the .py file were imported.\n\n    The 'optimization' parameter controls the presumed optimization level of\n    the bytecode file. If 'optimization' is not None, the string representation\n    of the argument is taken and verified to be alphanumeric (else ValueError\n    is raised).\n\n    The debug_override parameter is deprecated. If debug_override is not None,\n    a True value is the same as setting 'optimization' to the empty string\n    while a False value is equivalent to setting 'optimization' to '1'.\n\n    If sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if debug_override is not None:\n        _warnings.warn('the debug_override parameter is deprecated; use '\n                       \"'optimization' instead\", DeprecationWarning)\n        if optimization is not None:\n            message = 'debug_override or optimization must be set to None'\n            raise TypeError(message)\n        optimization = '' if debug_override else 1\n    path = _os.fspath(path)\n    head, tail = _path_split(path)\n    base, sep, rest = tail.rpartition('.')\n    tag = sys.implementation.cache_tag\n    if tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    almost_filename = ''.join([(base if base else rest), sep, tag])\n    if optimization is None:\n        if sys.flags.optimize == 0:\n            optimization = ''\n        else:\n            optimization = sys.flags.optimize\n    optimization = str(optimization)\n    if optimization != '':\n        if not optimization.isalnum():\n            raise ValueError(f'{optimization!r} is not alphanumeric')\n        almost_filename = f'{almost_filename}.{_OPT}{optimization}'\n    filename = almost_filename + BYTECODE_SUFFIXES[0]\n    if sys.pycache_prefix is not None:\n        # We need an absolute path to the py file to avoid the possibility of\n        # collisions within sys.pycache_prefix, if someone has two different\n        # `foo/bar.py` on their system and they import both of them using the\n        # same sys.pycache_prefix. Let's say sys.pycache_prefix is\n        # `C:\\Bytecode`; the idea here is that if we get `Foo\\Bar`, we first\n        # make it absolute (`C:\\Somewhere\\Foo\\Bar`), then make it root-relative\n        # (`Somewhere\\Foo\\Bar`), so we end up placing the bytecode file in an\n        # unambiguous `C:\\Bytecode\\Somewhere\\Foo\\Bar\\`.\n        head = _path_abspath(head)\n\n        # Strip initial drive from a Windows path. We know we have an absolute\n        # path here, so the second part of the check rules out a POSIX path that\n        # happens to contain a colon at the second character.\n        if head[1] == ':' and head[0] not in path_separators:\n            head = head[2:]\n\n        # Strip initial path separator from `head` to complete the conversion\n        # back to a root-relative path before joining.\n        return _path_join(\n            sys.pycache_prefix,\n            head.lstrip(path_separators),\n            filename,\n        )\n    return _path_join(head, _PYCACHE, filename)\n\n\ndef source_from_cache(path):\n    \"\"\"Given the path to a .pyc. file, return the path to its .py file.\n\n    The .pyc file does not need to exist; this simply returns the path to\n    the .py file calculated to correspond to the .pyc file.  If path does\n    not conform to PEP 3147/488 format, ValueError will be raised. If\n    sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if sys.implementation.cache_tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    path = _os.fspath(path)\n    head, pycache_filename = _path_split(path)\n    found_in_pycache_prefix = False\n    if sys.pycache_prefix is not None:\n        stripped_path = sys.pycache_prefix.rstrip(path_separators)\n        if head.startswith(stripped_path + path_sep):\n            head = head[len(stripped_path):]\n            found_in_pycache_prefix = True\n    if not found_in_pycache_prefix:\n        head, pycache = _path_split(head)\n        if pycache != _PYCACHE:\n            raise ValueError(f'{_PYCACHE} not bottom-level directory in '\n                             f'{path!r}')\n    dot_count = pycache_filename.count('.')\n    if dot_count not in {2, 3}:\n        raise ValueError(f'expected only 2 or 3 dots in {pycache_filename!r}')\n    elif dot_count == 3:\n        optimization = pycache_filename.rsplit('.', 2)[-2]\n        if not optimization.startswith(_OPT):\n            raise ValueError(\"optimization portion of filename does not start \"\n                             f\"with {_OPT!r}\")\n        opt_level = optimization[len(_OPT):]\n        if not opt_level.isalnum():\n            raise ValueError(f\"optimization level {optimization!r} is not an \"\n                             \"alphanumeric value\")\n    base_filename = pycache_filename.partition('.')[0]\n    return _path_join(head, base_filename + SOURCE_SUFFIXES[0])\n\n\ndef _get_sourcefile(bytecode_path):\n    \"\"\"Convert a bytecode file path to a source path (if possible).\n\n    This function exists purely for backwards-compatibility for\n    PyImport_ExecCodeModuleWithFilenames() in the C API.\n\n    \"\"\"\n    if len(bytecode_path) == 0:\n        return None\n    rest, _, extension = bytecode_path.rpartition('.')\n    if not rest or extension.lower()[-3:-1] != 'py':\n        return bytecode_path\n    try:\n        source_path = source_from_cache(bytecode_path)\n    except (NotImplementedError, ValueError):\n        source_path = bytecode_path[:-1]\n    return source_path if _path_isfile(source_path) else bytecode_path\n\n\ndef _get_cached(filename):\n    if filename.endswith(tuple(SOURCE_SUFFIXES)):\n        try:\n            return cache_from_source(filename)\n        except NotImplementedError:\n            pass\n    elif filename.endswith(tuple(BYTECODE_SUFFIXES)):\n        return filename\n    else:\n        return None\n\n\ndef _calc_mode(path):\n    \"\"\"Calculate the mode permissions for a bytecode file.\"\"\"\n    try:\n        mode = _path_stat(path).st_mode\n    except OSError:\n        mode = 0o666\n    # We always ensure write access so we can update cached files\n    # later even when the source files are read-only on Windows (#6074)\n    mode |= 0o200\n    return mode\n\n\ndef _check_name(method):\n    \"\"\"Decorator to verify that the module being requested matches the one the\n    loader can handle.\n\n    The first argument (self) must define _name which the second argument is\n    compared against. If the comparison fails then ImportError is raised.\n\n    \"\"\"\n    def _check_name_wrapper(self, name=None, *args, **kwargs):\n        if name is None:\n            name = self.name\n        elif self.name != name:\n            raise ImportError('loader for %s cannot handle %s' %\n                                (self.name, name), name=name)\n        return method(self, name, *args, **kwargs)\n\n    # FIXME: @_check_name is used to define class methods before the\n    # _bootstrap module is set by _set_bootstrap_module().\n    if _bootstrap is not None:\n        _wrap = _bootstrap._wrap\n    else:\n        def _wrap(new, old):\n            for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n                if hasattr(old, replace):\n                    setattr(new, replace, getattr(old, replace))\n            new.__dict__.update(old.__dict__)\n\n    _wrap(_check_name_wrapper, method)\n    return _check_name_wrapper\n\n\ndef _classify_pyc(data, name, exc_details):\n    \"\"\"Perform basic validity checking of a pyc header and return the flags field,\n    which determines how the pyc should be further validated against the source.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required, though.)\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    ImportError is raised when the magic number is incorrect or when the flags\n    field is invalid. EOFError is raised when the data is found to be truncated.\n\n    \"\"\"\n    magic = data[:4]\n    if magic != MAGIC_NUMBER:\n        message = f'bad magic number in {name!r}: {magic!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if len(data) < 16:\n        message = f'reached EOF while reading pyc header of {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise EOFError(message)\n    flags = _unpack_uint32(data[4:8])\n    # Only the first two flags are defined.\n    if flags & ~0b11:\n        message = f'invalid flags {flags!r} in {name!r}'\n        raise ImportError(message, **exc_details)\n    return flags\n\n\ndef _validate_timestamp_pyc(data, source_mtime, source_size, name,\n                            exc_details):\n    \"\"\"Validate a pyc against the source last-modified time.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_mtime* is the last modified timestamp of the source file.\n\n    *source_size* is None or the size of the source file in bytes.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if _unpack_uint32(data[8:12]) != (source_mtime & 0xFFFFFFFF):\n        message = f'bytecode is stale for {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if (source_size is not None and\n        _unpack_uint32(data[12:16]) != (source_size & 0xFFFFFFFF)):\n        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)\n\n\ndef _validate_hash_pyc(data, source_hash, name, exc_details):\n    \"\"\"Validate a hash-based pyc by checking the real source hash against the one in\n    the pyc header.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_hash* is the importlib.util.source_hash() of the source file.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if data[8:16] != source_hash:\n        raise ImportError(\n            f'hash in bytecode doesn\\'t match hash of source {name!r}',\n            **exc_details,\n        )\n\n\ndef _compile_bytecode(data, name=None, bytecode_path=None, source_path=None):\n    \"\"\"Compile bytecode as found in a pyc.\"\"\"\n    code = marshal.loads(data)\n    if isinstance(code, _code_type):\n        _bootstrap._verbose_message('code object from {!r}', bytecode_path)\n        if source_path is not None:\n            _imp._fix_co_filename(code, source_path)\n        return code\n    else:\n        raise ImportError(f'Non-code object in {bytecode_path!r}',\n                          name=name, path=bytecode_path)\n\n\ndef _code_to_timestamp_pyc(code, mtime=0, source_size=0):\n    \"Produce the data for a timestamp-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    data.extend(_pack_uint32(0))\n    data.extend(_pack_uint32(mtime))\n    data.extend(_pack_uint32(source_size))\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef _code_to_hash_pyc(code, source_hash, checked=True):\n    \"Produce the data for a hash-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    flags = 0b1 | checked << 1\n    data.extend(_pack_uint32(flags))\n    assert len(source_hash) == 8\n    data.extend(source_hash)\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef decode_source(source_bytes):\n    \"\"\"Decode bytes representing source code and return the string.\n\n    Universal newline support is used in the decoding.\n    \"\"\"\n    import tokenize  # To avoid bootstrap issues.\n    source_bytes_readline = _io.BytesIO(source_bytes).readline\n    encoding = tokenize.detect_encoding(source_bytes_readline)\n    newline_decoder = _io.IncrementalNewlineDecoder(None, True)\n    return newline_decoder.decode(source_bytes.decode(encoding[0]))\n\n\n# Module specifications #######################################################\n\n_POPULATE = object()\n\n\ndef spec_from_file_location(name, location=None, *, loader=None,\n                            submodule_search_locations=_POPULATE):\n    \"\"\"Return a module spec based on a file location.\n\n    To indicate that the module is a package, set\n    submodule_search_locations to a list of directory paths.  An\n    empty list is sufficient, though its not otherwise useful to the\n    import system.\n\n    The loader must take a spec as its only __init__() arg.\n\n    \"\"\"\n    if location is None:\n        # The caller may simply want a partially populated location-\n        # oriented spec.  So we set the location to a bogus value and\n        # fill in as much as we can.\n        location = '<unknown>'\n        if hasattr(loader, 'get_filename'):\n            # ExecutionLoader\n            try:\n                location = loader.get_filename(name)\n            except ImportError:\n                pass\n    else:\n        location = _os.fspath(location)\n        try:\n            location = _path_abspath(location)\n        except OSError:\n            pass\n\n    # If the location is on the filesystem, but doesn't actually exist,\n    # we could return None here, indicating that the location is not\n    # valid.  However, we don't have a good way of testing since an\n    # indirect location (e.g. a zip file or URL) will look like a\n    # non-existent file relative to the filesystem.\n\n    spec = _bootstrap.ModuleSpec(name, loader, origin=location)\n    spec._set_fileattr = True\n\n    # Pick a loader if one wasn't provided.\n    if loader is None:\n        for loader_class, suffixes in _get_supported_file_loaders():\n            if location.endswith(tuple(suffixes)):\n                loader = loader_class(name, location)\n                spec.loader = loader\n                break\n        else:\n            return None\n\n    # Set submodule_search_paths appropriately.\n    if submodule_search_locations is _POPULATE:\n        # Check the loader.\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                pass\n            else:\n                if is_package:\n                    spec.submodule_search_locations = []\n    else:\n        spec.submodule_search_locations = submodule_search_locations\n    if spec.submodule_search_locations == []:\n        if location:\n            dirname = _path_split(location)[0]\n            spec.submodule_search_locations.append(dirname)\n\n    return spec\n\n\ndef _bless_my_loader(module_globals):\n    \"\"\"Helper function for _warnings.c\n\n    See GH#97850 for details.\n    \"\"\"\n    # 2022-10-06(warsaw): For now, this helper is only used in _warnings.c and\n    # that use case only has the module globals.  This function could be\n    # extended to accept either that or a module object.  However, in the\n    # latter case, it would be better to raise certain exceptions when looking\n    # at a module, which should have either a __loader__ or __spec__.loader.\n    # For backward compatibility, it is possible that we'll get an empty\n    # dictionary for the module globals, and that cannot raise an exception.\n    if not isinstance(module_globals, dict):\n        return None\n\n    missing = object()\n    loader = module_globals.get('__loader__', None)\n    spec = module_globals.get('__spec__', missing)\n\n    if loader is None:\n        if spec is missing:\n            # If working with a module:\n            # raise AttributeError('Module globals is missing a __spec__')\n            return None\n        elif spec is None:\n            raise ValueError('Module globals is missing a __spec__.loader')\n\n    spec_loader = getattr(spec, 'loader', missing)\n\n    if spec_loader in (missing, None):\n        if loader is None:\n            exc = AttributeError if spec_loader is missing else ValueError\n            raise exc('Module globals is missing a __spec__.loader')\n        _warnings.warn(\n            'Module globals is missing a __spec__.loader',\n            DeprecationWarning)\n        spec_loader = loader\n\n    assert spec_loader is not None\n    if loader is not None and loader != spec_loader:\n        _warnings.warn(\n            'Module globals; __loader__ != __spec__.loader',\n            DeprecationWarning)\n        return loader\n\n    return spec_loader\n\n\n# Loaders #####################################################################\n\nclass WindowsRegistryFinder:\n\n    \"\"\"Meta path finder for modules declared in the Windows registry.\"\"\"\n\n    REGISTRY_KEY = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}')\n    REGISTRY_KEY_DEBUG = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}\\\\Debug')\n    DEBUG_BUILD = (_MS_WINDOWS and '_d.pyd' in EXTENSION_SUFFIXES)\n\n    @staticmethod\n    def _open_registry(key):\n        try:\n            return winreg.OpenKey(winreg.HKEY_CURRENT_USER, key)\n        except OSError:\n            return winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key)\n\n    @classmethod\n    def _search_registry(cls, fullname):\n        if cls.DEBUG_BUILD:\n            registry_key = cls.REGISTRY_KEY_DEBUG\n        else:\n            registry_key = cls.REGISTRY_KEY\n        key = registry_key.format(fullname=fullname,\n                                  sys_version='%d.%d' % sys.version_info[:2])\n        try:\n            with cls._open_registry(key) as hkey:\n                filepath = winreg.QueryValue(hkey, '')\n        except OSError:\n            return None\n        return filepath\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        filepath = cls._search_registry(fullname)\n        if filepath is None:\n            return None\n        try:\n            _path_stat(filepath)\n        except OSError:\n            return None\n        for loader, suffixes in _get_supported_file_loaders():\n            if filepath.endswith(tuple(suffixes)):\n                spec = _bootstrap.spec_from_loader(fullname,\n                                                   loader(fullname, filepath),\n                                                   origin=filepath)\n                return spec\n\n\nclass _LoaderBasics:\n\n    \"\"\"Base class of common code needed by both SourceLoader and\n    SourcelessFileLoader.\"\"\"\n\n    def is_package(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.is_package by checking if\n        the path returned by get_filename has a filename of '__init__.py'.\"\"\"\n        filename = _path_split(self.get_filename(fullname))[1]\n        filename_base = filename.rsplit('.', 1)[0]\n        tail_name = fullname.rpartition('.')[2]\n        return filename_base == '__init__' and tail_name != '__init__'\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        \"\"\"Execute the module.\"\"\"\n        code = self.get_code(module.__name__)\n        if code is None:\n            raise ImportError(f'cannot load module {module.__name__!r} when '\n                              'get_code() returns None')\n        _bootstrap._call_with_frames_removed(exec, code, module.__dict__)\n\n    def load_module(self, fullname):\n        \"\"\"This method is deprecated.\"\"\"\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n\nclass SourceLoader(_LoaderBasics):\n\n    def path_mtime(self, path):\n        \"\"\"Optional method that returns the modification time (an int) for the\n        specified path (a str).\n\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        raise OSError\n\n    def path_stats(self, path):\n        \"\"\"Optional method returning a metadata dict for the specified\n        path (a str).\n\n        Possible keys:\n        - 'mtime' (mandatory) is the numeric timestamp of last source\n          code modification;\n        - 'size' (optional) is the size in bytes of the source code.\n\n        Implementing this method allows the loader to read bytecode files.\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        return {'mtime': self.path_mtime(path)}\n\n    def _cache_bytecode(self, source_path, cache_path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n\n        The source path is needed in order to correctly transfer permissions\n        \"\"\"\n        # For backwards compatibility, we delegate to set_data()\n        return self.set_data(cache_path, data)\n\n    def set_data(self, path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n        \"\"\"\n\n\n    def get_source(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_source.\"\"\"\n        path = self.get_filename(fullname)\n        try:\n            source_bytes = self.get_data(path)\n        except OSError as exc:\n            raise ImportError('source not available through get_data()',\n                              name=fullname) from exc\n        return decode_source(source_bytes)\n\n    def source_to_code(self, data, path, *, _optimize=-1):\n        \"\"\"Return the code object compiled from source.\n\n        The 'data' argument can be any object type that compile() supports.\n        \"\"\"\n        return _bootstrap._call_with_frames_removed(compile, data, path, 'exec',\n                                        dont_inherit=True, optimize=_optimize)\n\n    def get_code(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_code.\n\n        Reading of bytecode requires path_stats to be implemented. To write\n        bytecode, set_data must also be implemented.\n\n        \"\"\"\n        source_path = self.get_filename(fullname)\n        source_mtime = None\n        source_bytes = None\n        source_hash = None\n        hash_based = False\n        check_source = True\n        try:\n            bytecode_path = cache_from_source(source_path)\n        except NotImplementedError:\n            bytecode_path = None\n        else:\n            try:\n                st = self.path_stats(source_path)\n            except OSError:\n                pass\n            else:\n                source_mtime = int(st['mtime'])\n                try:\n                    data = self.get_data(bytecode_path)\n                except OSError:\n                    pass\n                else:\n                    exc_details = {\n                        'name': fullname,\n                        'path': bytecode_path,\n                    }\n                    try:\n                        flags = _classify_pyc(data, fullname, exc_details)\n                        bytes_data = memoryview(data)[16:]\n                        hash_based = flags & 0b1 != 0\n                        if hash_based:\n                            check_source = flags & 0b10 != 0\n                            if (_imp.check_hash_based_pycs != 'never' and\n                                (check_source or\n                                 _imp.check_hash_based_pycs == 'always')):\n                                source_bytes = self.get_data(source_path)\n                                source_hash = _imp.source_hash(\n                                    _RAW_MAGIC_NUMBER,\n                                    source_bytes,\n                                )\n                                _validate_hash_pyc(data, source_hash, fullname,\n                                                   exc_details)\n                        else:\n                            _validate_timestamp_pyc(\n                                data,\n                                source_mtime,\n                                st['size'],\n                                fullname,\n                                exc_details,\n                            )\n                    except (ImportError, EOFError):\n                        pass\n                    else:\n                        _bootstrap._verbose_message('{} matches {}', bytecode_path,\n                                                    source_path)\n                        return _compile_bytecode(bytes_data, name=fullname,\n                                                 bytecode_path=bytecode_path,\n                                                 source_path=source_path)\n        if source_bytes is None:\n            source_bytes = self.get_data(source_path)\n        code_object = self.source_to_code(source_bytes, source_path)\n        _bootstrap._verbose_message('code object from {}', source_path)\n        if (not sys.dont_write_bytecode and bytecode_path is not None and\n                source_mtime is not None):\n            if hash_based:\n                if source_hash is None:\n                    source_hash = _imp.source_hash(_RAW_MAGIC_NUMBER,\n                                                   source_bytes)\n                data = _code_to_hash_pyc(code_object, source_hash, check_source)\n            else:\n                data = _code_to_timestamp_pyc(code_object, source_mtime,\n                                              len(source_bytes))\n            try:\n                self._cache_bytecode(source_path, bytecode_path, data)\n            except NotImplementedError:\n                pass\n        return code_object\n\n\nclass FileLoader:\n\n    \"\"\"Base file loader class which implements the loader protocol methods that\n    require file system usage.\"\"\"\n\n    def __init__(self, fullname, path):\n        \"\"\"Cache the module name and the path to the file found by the\n        finder.\"\"\"\n        self.name = fullname\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    @_check_name\n    def load_module(self, fullname):\n        \"\"\"Load a module from a file.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The only reason for this method is for the name check.\n        # Issue #14857: Avoid the zero-argument form of super so the implementation\n        # of that form can be updated without breaking the frozen module.\n        return super(FileLoader, self).load_module(fullname)\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n    def get_data(self, path):\n        \"\"\"Return the data from path as raw bytes.\"\"\"\n        if isinstance(self, (SourceLoader, ExtensionFileLoader)):\n            with _io.open_code(str(path)) as file:\n                return file.read()\n        else:\n            with _io.FileIO(path, 'r') as file:\n                return file.read()\n\n    @_check_name\n    def get_resource_reader(self, module):\n        from importlib.readers import FileReader\n        return FileReader(self)\n\n\nclass SourceFileLoader(FileLoader, SourceLoader):\n\n    \"\"\"Concrete implementation of SourceLoader using the file system.\"\"\"\n\n    def path_stats(self, path):\n        \"\"\"Return the metadata for the path.\"\"\"\n        st = _path_stat(path)\n        return {'mtime': st.st_mtime, 'size': st.st_size}\n\n    def _cache_bytecode(self, source_path, bytecode_path, data):\n        # Adapt between the two APIs\n        mode = _calc_mode(source_path)\n        return self.set_data(bytecode_path, data, _mode=mode)\n\n    def set_data(self, path, data, *, _mode=0o666):\n        \"\"\"Write bytes data to a file.\"\"\"\n        parent, filename = _path_split(path)\n        path_parts = []\n        # Figure out what directories are missing.\n        while parent and not _path_isdir(parent):\n            parent, part = _path_split(parent)\n            path_parts.append(part)\n        # Create needed directories.\n        for part in reversed(path_parts):\n            parent = _path_join(parent, part)\n            try:\n                _os.mkdir(parent)\n            except FileExistsError:\n                # Probably another Python process already created the dir.\n                continue\n            except OSError as exc:\n                # Could be a permission error, read-only filesystem: just forget\n                # about writing the data.\n                _bootstrap._verbose_message('could not create {!r}: {!r}',\n                                            parent, exc)\n                return\n        try:\n            _write_atomic(path, data, _mode)\n            _bootstrap._verbose_message('created {!r}', path)\n        except OSError as exc:\n            # Same as above: just don't write the bytecode.\n            _bootstrap._verbose_message('could not create {!r}: {!r}', path,\n                                        exc)\n\n\nclass SourcelessFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader which handles sourceless file imports.\"\"\"\n\n    def get_code(self, fullname):\n        path = self.get_filename(fullname)\n        data = self.get_data(path)\n        # Call _classify_pyc to do basic validation of the pyc but ignore the\n        # result. There's no source to check against.\n        exc_details = {\n            'name': fullname,\n            'path': path,\n        }\n        _classify_pyc(data, fullname, exc_details)\n        return _compile_bytecode(\n            memoryview(data)[16:],\n            name=fullname,\n            bytecode_path=path,\n        )\n\n    def get_source(self, fullname):\n        \"\"\"Return None as there is no source code.\"\"\"\n        return None\n\n\nclass ExtensionFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader for extension modules.\n\n    The constructor is designed to work with FileFinder.\n\n    \"\"\"\n\n    def __init__(self, name, path):\n        self.name = name\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    def create_module(self, spec):\n        \"\"\"Create an uninitialized extension module\"\"\"\n        module = _bootstrap._call_with_frames_removed(\n            _imp.create_dynamic, spec)\n        _bootstrap._verbose_message('extension module {!r} loaded from {!r}',\n                         spec.name, self.path)\n        return module\n\n    def exec_module(self, module):\n        \"\"\"Initialize an extension module\"\"\"\n        _bootstrap._call_with_frames_removed(_imp.exec_dynamic, module)\n        _bootstrap._verbose_message('extension module {!r} executed from {!r}',\n                         self.name, self.path)\n\n    def is_package(self, fullname):\n        \"\"\"Return True if the extension module is a package.\"\"\"\n        file_name = _path_split(self.path)[1]\n        return any(file_name == '__init__' + suffix\n                   for suffix in EXTENSION_SUFFIXES)\n\n    def get_code(self, fullname):\n        \"\"\"Return None as an extension module cannot create a code object.\"\"\"\n        return None\n\n    def get_source(self, fullname):\n        \"\"\"Return None as extension modules have no source code.\"\"\"\n        return None\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n\nclass _NamespacePath:\n    \"\"\"Represents a namespace package's path.  It uses the module name\n    to find its parent module, and from there it looks up the parent's\n    __path__.  When this changes, the module's own path is recomputed,\n    using path_finder.  For top-level modules, the parent module's path\n    is sys.path.\"\"\"\n\n    # When invalidate_caches() is called, this epoch is incremented\n    # https://bugs.python.org/issue45703\n    _epoch = 0\n\n    def __init__(self, name, path, path_finder):\n        self._name = name\n        self._path = path\n        self._last_parent_path = tuple(self._get_parent_path())\n        self._last_epoch = self._epoch\n        self._path_finder = path_finder\n\n    def _find_parent_path_names(self):\n        \"\"\"Returns a tuple of (parent-module-name, parent-path-attr-name)\"\"\"\n        parent, dot, me = self._name.rpartition('.')\n        if dot == '':\n            # This is a top-level module. sys.path contains the parent path.\n            return 'sys', 'path'\n        # Not a top-level module. parent-module.__path__ contains the\n        #  parent path.\n        return parent, '__path__'\n\n    def _get_parent_path(self):\n        parent_module_name, path_attr_name = self._find_parent_path_names()\n        return getattr(sys.modules[parent_module_name], path_attr_name)\n\n    def _recalculate(self):\n        # If the parent's path has changed, recalculate _path\n        parent_path = tuple(self._get_parent_path()) # Make a copy\n        if parent_path != self._last_parent_path or self._epoch != self._last_epoch:\n            spec = self._path_finder(self._name, parent_path)\n            # Note that no changes are made if a loader is returned, but we\n            #  do remember the new parent path\n            if spec is not None and spec.loader is None:\n                if spec.submodule_search_locations:\n                    self._path = spec.submodule_search_locations\n            self._last_parent_path = parent_path     # Save the copy\n            self._last_epoch = self._epoch\n        return self._path\n\n    def __iter__(self):\n        return iter(self._recalculate())\n\n    def __getitem__(self, index):\n        return self._recalculate()[index]\n\n    def __setitem__(self, index, path):\n        self._path[index] = path\n\n    def __len__(self):\n        return len(self._recalculate())\n\n    def __repr__(self):\n        return f'_NamespacePath({self._path!r})'\n\n    def __contains__(self, item):\n        return item in self._recalculate()\n\n    def append(self, item):\n        self._path.append(item)\n\n\n# This class is actually exposed publicly in a namespace package's __loader__\n# attribute, so it should be available through a non-private name.\n# https://github.com/python/cpython/issues/92054\nclass NamespaceLoader:\n    def __init__(self, name, path, path_finder):\n        self._path = _NamespacePath(name, path, path_finder)\n\n    def is_package(self, fullname):\n        return True\n\n    def get_source(self, fullname):\n        return ''\n\n    def get_code(self, fullname):\n        return compile('', '<string>', 'exec', dont_inherit=True)\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        pass\n\n    def load_module(self, fullname):\n        \"\"\"Load a namespace module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The import system never calls this method.\n        _bootstrap._verbose_message('namespace module loaded with path {!r}',\n                                    self._path)\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n    def get_resource_reader(self, module):\n        from importlib.readers import NamespaceReader\n        return NamespaceReader(self._path)\n\n\n# We use this exclusively in module_from_spec() for backward-compatibility.\n_NamespaceLoader = NamespaceLoader\n\n\n# Finders #####################################################################\n\nclass PathFinder:\n\n    \"\"\"Meta path finder for sys.path and package __path__ attributes.\"\"\"\n\n    @staticmethod\n    def invalidate_caches():\n        \"\"\"Call the invalidate_caches() method on all path entry finders\n        stored in sys.path_importer_caches (where implemented).\"\"\"\n        for name, finder in list(sys.path_importer_cache.items()):\n            # Drop entry if finder name is a relative path. The current\n            # working directory may have changed.\n            if finder is None or not _path_isabs(name):\n                del sys.path_importer_cache[name]\n            elif hasattr(finder, 'invalidate_caches'):\n                finder.invalidate_caches()\n        # Also invalidate the caches of _NamespacePaths\n        # https://bugs.python.org/issue45703\n        _NamespacePath._epoch += 1\n\n        from importlib.metadata import MetadataPathFinder\n        MetadataPathFinder.invalidate_caches()\n\n    @staticmethod\n    def _path_hooks(path):\n        \"\"\"Search sys.path_hooks for a finder for 'path'.\"\"\"\n        if sys.path_hooks is not None and not sys.path_hooks:\n            _warnings.warn('sys.path_hooks is empty', ImportWarning)\n        for hook in sys.path_hooks:\n            try:\n                return hook(path)\n            except ImportError:\n                continue\n        else:\n            return None\n\n    @classmethod\n    def _path_importer_cache(cls, path):\n        \"\"\"Get the finder for the path entry from sys.path_importer_cache.\n\n        If the path entry is not in the cache, find the appropriate finder\n        and cache it. If no finder is available, store None.\n\n        \"\"\"\n        if path == '':\n            try:\n                path = _os.getcwd()\n            except FileNotFoundError:\n                # Don't cache the failure as the cwd can easily change to\n                # a valid directory later on.\n                return None\n        try:\n            finder = sys.path_importer_cache[path]\n        except KeyError:\n            finder = cls._path_hooks(path)\n            sys.path_importer_cache[path] = finder\n        return finder\n\n    @classmethod\n    def _get_spec(cls, fullname, path, target=None):\n        \"\"\"Find the loader or namespace_path for this module/package name.\"\"\"\n        # If this ends up being a namespace package, namespace_path is\n        #  the list of paths that will become its __path__\n        namespace_path = []\n        for entry in path:\n            if not isinstance(entry, str):\n                continue\n            finder = cls._path_importer_cache(entry)\n            if finder is not None:\n                spec = finder.find_spec(fullname, target)\n                if spec is None:\n                    continue\n                if spec.loader is not None:\n                    return spec\n                portions = spec.submodule_search_locations\n                if portions is None:\n                    raise ImportError('spec missing loader')\n                # This is possibly part of a namespace package.\n                #  Remember these path entries (if any) for when we\n                #  create a namespace package, and continue iterating\n                #  on path.\n                namespace_path.extend(portions)\n        else:\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = namespace_path\n            return spec\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        \"\"\"Try to find a spec for 'fullname' on sys.path or 'path'.\n\n        The search is based on sys.path_hooks and sys.path_importer_cache.\n        \"\"\"\n        if path is None:\n            path = sys.path\n        spec = cls._get_spec(fullname, path, target)\n        if spec is None:\n            return None\n        elif spec.loader is None:\n            namespace_path = spec.submodule_search_locations\n            if namespace_path:\n                # We found at least one namespace path.  Return a spec which\n                # can create the namespace package.\n                spec.origin = None\n                spec.submodule_search_locations = _NamespacePath(fullname, namespace_path, cls._get_spec)\n                return spec\n            else:\n                return None\n        else:\n            return spec\n\n    @staticmethod\n    def find_distributions(*args, **kwargs):\n        \"\"\"\n        Find distributions.\n\n        Return an iterable of all Distribution instances capable of\n        loading the metadata for packages matching ``context.name``\n        (or all names if ``None`` indicated) along the paths in the list\n        of directories ``context.path``.\n        \"\"\"\n        from importlib.metadata import MetadataPathFinder\n        return MetadataPathFinder.find_distributions(*args, **kwargs)\n\n\nclass FileFinder:\n\n    \"\"\"File-based finder.\n\n    Interactions with the file system are cached for performance, being\n    refreshed when the directory the finder is handling has been modified.\n\n    \"\"\"\n\n    def __init__(self, path, *loader_details):\n        \"\"\"Initialize with the path to search on and a variable number of\n        2-tuples containing the loader and the file suffixes the loader\n        recognizes.\"\"\"\n        loaders = []\n        for loader, suffixes in loader_details:\n            loaders.extend((suffix, loader) for suffix in suffixes)\n        self._loaders = loaders\n        # Base (directory) path\n        if not path or path == '.':\n            self.path = _os.getcwd()\n        else:\n            self.path = _path_abspath(path)\n        self._path_mtime = -1\n        self._path_cache = set()\n        self._relaxed_path_cache = set()\n\n    def invalidate_caches(self):\n        \"\"\"Invalidate the directory mtime.\"\"\"\n        self._path_mtime = -1\n\n    def _get_spec(self, loader_class, fullname, path, smsl, target):\n        loader = loader_class(fullname, path)\n        return spec_from_file_location(fullname, path, loader=loader,\n                                       submodule_search_locations=smsl)\n\n    def find_spec(self, fullname, target=None):\n        \"\"\"Try to find a spec for the specified module.\n\n        Returns the matching spec, or None if not found.\n        \"\"\"\n        is_namespace = False\n        tail_module = fullname.rpartition('.')[2]\n        try:\n            mtime = _path_stat(self.path or _os.getcwd()).st_mtime\n        except OSError:\n            mtime = -1\n        if mtime != self._path_mtime:\n            self._fill_cache()\n            self._path_mtime = mtime\n        # tail_module keeps the original casing, for __file__ and friends\n        if _relax_case():\n            cache = self._relaxed_path_cache\n            cache_module = tail_module.lower()\n        else:\n            cache = self._path_cache\n            cache_module = tail_module\n        # Check if the module is the name of a directory (and thus a package).\n        if cache_module in cache:\n            base_path = _path_join(self.path, tail_module)\n            for suffix, loader_class in self._loaders:\n                init_filename = '__init__' + suffix\n                full_path = _path_join(base_path, init_filename)\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path, [base_path], target)\n            else:\n                # If a namespace package, return the path if we don't\n                #  find a module in the next section.\n                is_namespace = _path_isdir(base_path)\n        # Check for a file w/ a proper suffix exists.\n        for suffix, loader_class in self._loaders:\n            try:\n                full_path = _path_join(self.path, tail_module + suffix)\n            except ValueError:\n                return None\n            _bootstrap._verbose_message('trying {}', full_path, verbosity=2)\n            if cache_module + suffix in cache:\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path,\n                                          None, target)\n        if is_namespace:\n            _bootstrap._verbose_message('possible namespace for {}', base_path)\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = [base_path]\n            return spec\n        return None\n\n    def _fill_cache(self):\n        \"\"\"Fill the cache of potential modules and packages for this directory.\"\"\"\n        path = self.path\n        try:\n            contents = _os.listdir(path or _os.getcwd())\n        except (FileNotFoundError, PermissionError, NotADirectoryError):\n            # Directory has either been removed, turned into a file, or made\n            # unreadable.\n            contents = []\n        # We store two cached versions, to handle runtime changes of the\n        # PYTHONCASEOK environment variable.\n        if not sys.platform.startswith('win'):\n            self._path_cache = set(contents)\n        else:\n            # Windows users can import modules with case-insensitive file\n            # suffixes (for legacy reasons). Make the suffix lowercase here\n            # so it's done once instead of for every import. This is safe as\n            # the specified suffixes to check against are always specified in a\n            # case-sensitive manner.\n            lower_suffix_contents = set()\n            for item in contents:\n                name, dot, suffix = item.partition('.')\n                if dot:\n                    new_name = f'{name}.{suffix.lower()}'\n                else:\n                    new_name = name\n                lower_suffix_contents.add(new_name)\n            self._path_cache = lower_suffix_contents\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n            self._relaxed_path_cache = {fn.lower() for fn in contents}\n\n    @classmethod\n    def path_hook(cls, *loader_details):\n        \"\"\"A class method which returns a closure to use on sys.path_hook\n        which will return an instance using the specified loaders and the path\n        called on the closure.\n\n        If the path called on the closure is not a directory, ImportError is\n        raised.\n\n        \"\"\"\n        def path_hook_for_FileFinder(path):\n            \"\"\"Path hook for importlib.machinery.FileFinder.\"\"\"\n            if not _path_isdir(path):\n                raise ImportError('only directories are supported', path=path)\n            return cls(path, *loader_details)\n\n        return path_hook_for_FileFinder\n\n    def __repr__(self):\n        return f'FileFinder({self.path!r})'\n\n\n# Import setup ###############################################################\n\ndef _fix_up_module(ns, name, pathname, cpathname=None):\n    # This function is used by PyImport_ExecCodeModuleObject().\n    loader = ns.get('__loader__')\n    spec = ns.get('__spec__')\n    if not loader:\n        if spec:\n            loader = spec.loader\n        elif pathname == cpathname:\n            loader = SourcelessFileLoader(name, pathname)\n        else:\n            loader = SourceFileLoader(name, pathname)\n    if not spec:\n        spec = spec_from_file_location(name, pathname, loader=loader)\n        if cpathname:\n            spec.cached = _path_abspath(cpathname)\n    try:\n        ns['__spec__'] = spec\n        ns['__loader__'] = loader\n        ns['__file__'] = pathname\n        ns['__cached__'] = cpathname\n    except Exception:\n        # Not important enough to report.\n        pass\n\n\ndef _get_supported_file_loaders():\n    \"\"\"Returns a list of file-based module loaders.\n\n    Each item is a tuple (loader, suffixes).\n    \"\"\"\n    extensions = ExtensionFileLoader, _imp.extension_suffixes()\n    source = SourceFileLoader, SOURCE_SUFFIXES\n    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES\n    return [extensions, source, bytecode]\n\n\ndef _set_bootstrap_module(_bootstrap_module):\n    global _bootstrap\n    _bootstrap = _bootstrap_module\n\n\ndef _install(_bootstrap_module):\n    \"\"\"Install the path-based import components.\"\"\"\n    _set_bootstrap_module(_bootstrap_module)\n    supported_loaders = _get_supported_file_loaders()\n    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])\n    sys.meta_path.append(PathFinder)\n", 1749], "<frozen importlib._bootstrap>": ["\"\"\"Core implementation of import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n#\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module\n# in the early stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\ndef _object_name(obj):\n    try:\n        return obj.__qualname__\n    except AttributeError:\n        return type(obj).__qualname__\n\n# Bootstrap-related code ######################################################\n\n# Modules injected manually by _setup()\n_thread = None\n_warnings = None\n_weakref = None\n\n# Import done by _install_external_importers()\n_bootstrap_external = None\n\n\ndef _wrap(new, old):\n    \"\"\"Simple substitute for functools.update_wrapper.\"\"\"\n    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n        if hasattr(old, replace):\n            setattr(new, replace, getattr(old, replace))\n    new.__dict__.update(old.__dict__)\n\n\ndef _new_module(name):\n    return type(sys)(name)\n\n\n# Module-level locking ########################################################\n\n# For a list that can have a weakref to it.\nclass _List(list):\n    pass\n\n\n# Copied from weakref.py with some simplifications and modifications unique to\n# bootstrapping importlib. Many methods were simply deleting for simplicity, so if they\n# are needed in the future they may work if simply copied back in.\nclass _WeakValueDictionary:\n\n    def __init__(self):\n        self_weakref = _weakref.ref(self)\n\n        # Inlined to avoid issues with inheriting from _weakref.ref before _weakref is\n        # set by _setup(). Since there's only one instance of this class, this is\n        # not expensive.\n        class KeyedRef(_weakref.ref):\n\n            __slots__ = \"key\",\n\n            def __new__(type, ob, key):\n                self = super().__new__(type, ob, type.remove)\n                self.key = key\n                return self\n\n            def __init__(self, ob, key):\n                super().__init__(ob, self.remove)\n\n            @staticmethod\n            def remove(wr):\n                nonlocal self_weakref\n\n                self = self_weakref()\n                if self is not None:\n                    if self._iterating:\n                        self._pending_removals.append(wr.key)\n                    else:\n                        _weakref._remove_dead_weakref(self.data, wr.key)\n\n        self._KeyedRef = KeyedRef\n        self.clear()\n\n    def clear(self):\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _weakref._remove_dead_weakref(d, key)\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            if (o := wr()) is None:\n                return default\n            else:\n                return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = self._KeyedRef(default, key)\n            return default\n        else:\n            return o\n\n\n# A dict mapping module names to weakrefs of _ModuleLock instances.\n# Dictionary protected by the global import lock.\n_module_locks = {}\n\n# A dict mapping thread IDs to weakref'ed lists of _ModuleLock instances.\n# This maps a thread to the module locks it is blocking on acquiring.  The\n# values are lists because a single thread could perform a re-entrant import\n# and be \"in the process\" of blocking on locks for more than one module.  A\n# thread can be \"in the process\" because a thread cannot actually block on\n# acquiring more than one lock but it can have set up bookkeeping that reflects\n# that it intends to block on acquiring more than one lock.\n#\n# The dictionary uses a WeakValueDictionary to avoid keeping unnecessary\n# lists around, regardless of GC runs. This way there's no memory leak if\n# the list is no longer needed (GH-106176).\n_blocking_on = None\n\n\nclass _BlockingOnManager:\n    \"\"\"A context manager responsible to updating ``_blocking_on``.\"\"\"\n    def __init__(self, thread_id, lock):\n        self.thread_id = thread_id\n        self.lock = lock\n\n    def __enter__(self):\n        \"\"\"Mark the running thread as waiting for self.lock. via _blocking_on.\"\"\"\n        # Interactions with _blocking_on are *not* protected by the global\n        # import lock here because each thread only touches the state that it\n        # owns (state keyed on its thread id).  The global import lock is\n        # re-entrant (i.e., a single thread may take it more than once) so it\n        # wouldn't help us be correct in the face of re-entrancy either.\n\n        self.blocked_on = _blocking_on.setdefault(self.thread_id, _List())\n        self.blocked_on.append(self.lock)\n\n    def __exit__(self, *args, **kwargs):\n        \"\"\"Remove self.lock from this thread's _blocking_on list.\"\"\"\n        self.blocked_on.remove(self.lock)\n\n\nclass _DeadlockError(RuntimeError):\n    pass\n\n\n\ndef _has_deadlocked(target_id, *, seen_ids, candidate_ids, blocking_on):\n    \"\"\"Check if 'target_id' is holding the same lock as another thread(s).\n\n    The search within 'blocking_on' starts with the threads listed in\n    'candidate_ids'.  'seen_ids' contains any threads that are considered\n    already traversed in the search.\n\n    Keyword arguments:\n    target_id     -- The thread id to try to reach.\n    seen_ids      -- A set of threads that have already been visited.\n    candidate_ids -- The thread ids from which to begin.\n    blocking_on   -- A dict representing the thread/blocking-on graph.  This may\n                     be the same object as the global '_blocking_on' but it is\n                     a parameter to reduce the impact that global mutable\n                     state has on the result of this function.\n    \"\"\"\n    if target_id in candidate_ids:\n        # If we have already reached the target_id, we're done - signal that it\n        # is reachable.\n        return True\n\n    # Otherwise, try to reach the target_id from each of the given candidate_ids.\n    for tid in candidate_ids:\n        if not (candidate_blocking_on := blocking_on.get(tid)):\n            # There are no edges out from this node, skip it.\n            continue\n        elif tid in seen_ids:\n            # bpo 38091: the chain of tid's we encounter here eventually leads\n            # to a fixed point or a cycle, but does not reach target_id.\n            # This means we would not actually deadlock.  This can happen if\n            # other threads are at the beginning of acquire() below.\n            return False\n        seen_ids.add(tid)\n\n        # Follow the edges out from this thread.\n        edges = [lock.owner for lock in candidate_blocking_on]\n        if _has_deadlocked(target_id, seen_ids=seen_ids, candidate_ids=edges,\n                blocking_on=blocking_on):\n            return True\n\n    return False\n\n\nclass _ModuleLock:\n    \"\"\"A recursive lock implementation which is able to detect deadlocks\n    (e.g. thread 1 trying to take locks A then B, and thread 2 trying to\n    take locks B then A).\n    \"\"\"\n\n    def __init__(self, name):\n        # Create an RLock for protecting the import process for the\n        # corresponding module.  Since it is an RLock, a single thread will be\n        # able to take it more than once.  This is necessary to support\n        # re-entrancy in the import system that arises from (at least) signal\n        # handlers and the garbage collector.  Consider the case of:\n        #\n        #  import foo\n        #  -> ...\n        #     -> importlib._bootstrap._ModuleLock.acquire\n        #        -> ...\n        #           -> <garbage collector>\n        #              -> __del__\n        #                 -> import foo\n        #                    -> ...\n        #                       -> importlib._bootstrap._ModuleLock.acquire\n        #                          -> _BlockingOnManager.__enter__\n        #\n        # If a different thread than the running one holds the lock then the\n        # thread will have to block on taking the lock, which is what we want\n        # for thread safety.\n        self.lock = _thread.RLock()\n        self.wakeup = _thread.allocate_lock()\n\n        # The name of the module for which this is a lock.\n        self.name = name\n\n        # Can end up being set to None if this lock is not owned by any thread\n        # or the thread identifier for the owning thread.\n        self.owner = None\n\n        # Represent the number of times the owning thread has acquired this lock\n        # via a list of True.  This supports RLock-like (\"re-entrant lock\")\n        # behavior, necessary in case a single thread is following a circular\n        # import dependency and needs to take the lock for a single module\n        # more than once.\n        #\n        # Counts are represented as a list of True because list.append(True)\n        # and list.pop() are both atomic and thread-safe in CPython and it's hard\n        # to find another primitive with the same properties.\n        self.count = []\n\n        # This is a count of the number of threads that are blocking on\n        # self.wakeup.acquire() awaiting to get their turn holding this module\n        # lock.  When the module lock is released, if this is greater than\n        # zero, it is decremented and `self.wakeup` is released one time.  The\n        # intent is that this will let one other thread make more progress on\n        # acquiring this module lock.  This repeats until all the threads have\n        # gotten a turn.\n        #\n        # This is incremented in self.acquire() when a thread notices it is\n        # going to have to wait for another thread to finish.\n        #\n        # See the comment above count for explanation of the representation.\n        self.waiters = []\n\n    def has_deadlock(self):\n        # To avoid deadlocks for concurrent or re-entrant circular imports,\n        # look at _blocking_on to see if any threads are blocking\n        # on getting the import lock for any module for which the import lock\n        # is held by this thread.\n        return _has_deadlocked(\n            # Try to find this thread.\n            target_id=_thread.get_ident(),\n            seen_ids=set(),\n            # Start from the thread that holds the import lock for this\n            # module.\n            candidate_ids=[self.owner],\n            # Use the global \"blocking on\" state.\n            blocking_on=_blocking_on,\n        )\n\n    def acquire(self):\n        \"\"\"\n        Acquire the module lock.  If a potential deadlock is detected,\n        a _DeadlockError is raised.\n        Otherwise, the lock is always acquired and True is returned.\n        \"\"\"\n        tid = _thread.get_ident()\n        with _BlockingOnManager(tid, self):\n            while True:\n                # Protect interaction with state on self with a per-module\n                # lock.  This makes it safe for more than one thread to try to\n                # acquire the lock for a single module at the same time.\n                with self.lock:\n                    if self.count == [] or self.owner == tid:\n                        # If the lock for this module is unowned then we can\n                        # take the lock immediately and succeed.  If the lock\n                        # for this module is owned by the running thread then\n                        # we can also allow the acquire to succeed.  This\n                        # supports circular imports (thread T imports module A\n                        # which imports module B which imports module A).\n                        self.owner = tid\n                        self.count.append(True)\n                        return True\n\n                    # At this point we know the lock is held (because count !=\n                    # 0) by another thread (because owner != tid).  We'll have\n                    # to get in line to take the module lock.\n\n                    # But first, check to see if this thread would create a\n                    # deadlock by acquiring this module lock.  If it would\n                    # then just stop with an error.\n                    #\n                    # It's not clear who is expected to handle this error.\n                    # There is one handler in _lock_unlock_module but many\n                    # times this method is called when entering the context\n                    # manager _ModuleLockManager instead - so _DeadlockError\n                    # will just propagate up to application code.\n                    #\n                    # This seems to be more than just a hypothetical -\n                    # https://stackoverflow.com/questions/59509154\n                    # https://github.com/encode/django-rest-framework/issues/7078\n                    if self.has_deadlock():\n                        raise _DeadlockError(f'deadlock detected by {self!r}')\n\n                    # Check to see if we're going to be able to acquire the\n                    # lock.  If we are going to have to wait then increment\n                    # the waiters so `self.release` will know to unblock us\n                    # later on.  We do this part non-blockingly so we don't\n                    # get stuck here before we increment waiters.  We have\n                    # this extra acquire call (in addition to the one below,\n                    # outside the self.lock context manager) to make sure\n                    # self.wakeup is held when the next acquire is called (so\n                    # we block).  This is probably needlessly complex and we\n                    # should just take self.wakeup in the return codepath\n                    # above.\n                    if self.wakeup.acquire(False):\n                        self.waiters.append(None)\n\n                # Now take the lock in a blocking fashion.  This won't\n                # complete until the thread holding this lock\n                # (self.owner) calls self.release.\n                self.wakeup.acquire()\n\n                # Taking the lock has served its purpose (making us wait), so we can\n                # give it up now.  We'll take it w/o blocking again on the\n                # next iteration around this 'while' loop.\n                self.wakeup.release()\n\n    def release(self):\n        tid = _thread.get_ident()\n        with self.lock:\n            if self.owner != tid:\n                raise RuntimeError('cannot release un-acquired lock')\n            assert len(self.count) > 0\n            self.count.pop()\n            if not len(self.count):\n                self.owner = None\n                if len(self.waiters) > 0:\n                    self.waiters.pop()\n                    self.wakeup.release()\n\n    def __repr__(self):\n        return f'_ModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _DummyModuleLock:\n    \"\"\"A simple _ModuleLock equivalent for Python builds without\n    multi-threading support.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        self.count = 0\n\n    def acquire(self):\n        self.count += 1\n        return True\n\n    def release(self):\n        if self.count == 0:\n            raise RuntimeError('cannot release un-acquired lock')\n        self.count -= 1\n\n    def __repr__(self):\n        return f'_DummyModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _ModuleLockManager:\n\n    def __init__(self, name):\n        self._name = name\n        self._lock = None\n\n    def __enter__(self):\n        self._lock = _get_module_lock(self._name)\n        self._lock.acquire()\n\n    def __exit__(self, *args, **kwargs):\n        self._lock.release()\n\n\n# The following two functions are for consumption by Python/import.c.\n\ndef _get_module_lock(name):\n    \"\"\"Get or create the module lock for a given module name.\n\n    Acquire/release internally the global import lock to protect\n    _module_locks.\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        try:\n            lock = _module_locks[name]()\n        except KeyError:\n            lock = None\n\n        if lock is None:\n            if _thread is None:\n                lock = _DummyModuleLock(name)\n            else:\n                lock = _ModuleLock(name)\n\n            def cb(ref, name=name):\n                _imp.acquire_lock()\n                try:\n                    # bpo-31070: Check if another thread created a new lock\n                    # after the previous lock was destroyed\n                    # but before the weakref callback was called.\n                    if _module_locks.get(name) is ref:\n                        del _module_locks[name]\n                finally:\n                    _imp.release_lock()\n\n            _module_locks[name] = _weakref.ref(lock, cb)\n    finally:\n        _imp.release_lock()\n\n    return lock\n\n\ndef _lock_unlock_module(name):\n    \"\"\"Acquires then releases the module lock for a given module name.\n\n    This is used to ensure a module is completely initialized, in the\n    event it is being imported by another thread.\n    \"\"\"\n    lock = _get_module_lock(name)\n    try:\n        lock.acquire()\n    except _DeadlockError:\n        # Concurrent circular import, we'll accept a partially initialized\n        # module object.\n        pass\n    else:\n        lock.release()\n\n# Frame stripping magic ###############################################\ndef _call_with_frames_removed(f, *args, **kwds):\n    \"\"\"remove_importlib_frames in import.c will always remove sequences\n    of importlib frames that end with a call to this function\n\n    Use it instead of a normal call in places where including the importlib\n    frames introduces unwanted noise into the traceback (e.g. when executing\n    module code)\n    \"\"\"\n    return f(*args, **kwds)\n\n\ndef _verbose_message(message, *args, verbosity=1):\n    \"\"\"Print the message to stderr if -v/PYTHONVERBOSE is turned on.\"\"\"\n    if sys.flags.verbose >= verbosity:\n        if not message.startswith(('#', 'import ')):\n            message = '# ' + message\n        print(message.format(*args), file=sys.stderr)\n\n\ndef _requires_builtin(fxn):\n    \"\"\"Decorator to verify the named module is built-in.\"\"\"\n    def _requires_builtin_wrapper(self, fullname):\n        if fullname not in sys.builtin_module_names:\n            raise ImportError(f'{fullname!r} is not a built-in module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_builtin_wrapper, fxn)\n    return _requires_builtin_wrapper\n\n\ndef _requires_frozen(fxn):\n    \"\"\"Decorator to verify the named module is frozen.\"\"\"\n    def _requires_frozen_wrapper(self, fullname):\n        if not _imp.is_frozen(fullname):\n            raise ImportError(f'{fullname!r} is not a frozen module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_frozen_wrapper, fxn)\n    return _requires_frozen_wrapper\n\n\n# Typically used by loader classes as a method replacement.\ndef _load_module_shim(self, fullname):\n    \"\"\"Load the specified module into sys.modules and return it.\n\n    This method is deprecated.  Use loader.exec_module() instead.\n\n    \"\"\"\n    msg = (\"the load_module() method is deprecated and slated for removal in \"\n           \"Python 3.15; use exec_module() instead\")\n    _warnings.warn(msg, DeprecationWarning)\n    spec = spec_from_loader(fullname, self)\n    if fullname in sys.modules:\n        module = sys.modules[fullname]\n        _exec(spec, module)\n        return sys.modules[fullname]\n    else:\n        return _load(spec)\n\n# Module specifications #######################################################\n\ndef _module_repr(module):\n    \"\"\"The implementation of ModuleType.__repr__().\"\"\"\n    loader = getattr(module, '__loader__', None)\n    if spec := getattr(module, \"__spec__\", None):\n        return _module_repr_from_spec(spec)\n    # Fall through to a catch-all which always succeeds.\n    try:\n        name = module.__name__\n    except AttributeError:\n        name = '?'\n    try:\n        filename = module.__file__\n    except AttributeError:\n        if loader is None:\n            return f'<module {name!r}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        return f'<module {name!r} from {filename!r}>'\n\n\nclass ModuleSpec:\n    \"\"\"The specification for a module, used for loading.\n\n    A module's spec is the source for information about the module.  For\n    data associated with the module, including source, use the spec's\n    loader.\n\n    `name` is the absolute name of the module.  `loader` is the loader\n    to use when loading the module.  `parent` is the name of the\n    package the module is in.  The parent is derived from the name.\n\n    `is_package` determines if the module is considered a package or\n    not.  On modules this is reflected by the `__path__` attribute.\n\n    `origin` is the specific location used by the loader from which to\n    load the module, if that information is available.  When filename is\n    set, origin will match.\n\n    `has_location` indicates that a spec's \"origin\" reflects a location.\n    When this is True, `__file__` attribute of the module is set.\n\n    `cached` is the location of the cached bytecode file, if any.  It\n    corresponds to the `__cached__` attribute.\n\n    `submodule_search_locations` is the sequence of path entries to\n    search when importing submodules.  If set, is_package should be\n    True--and False otherwise.\n\n    Packages are simply modules that (may) have submodules.  If a spec\n    has a non-None value in `submodule_search_locations`, the import\n    system will consider modules loaded from the spec as packages.\n\n    Only finders (see importlib.abc.MetaPathFinder and\n    importlib.abc.PathEntryFinder) should modify ModuleSpec instances.\n\n    \"\"\"\n\n    def __init__(self, name, loader, *, origin=None, loader_state=None,\n                 is_package=None):\n        self.name = name\n        self.loader = loader\n        self.origin = origin\n        self.loader_state = loader_state\n        self.submodule_search_locations = [] if is_package else None\n        self._uninitialized_submodules = []\n\n        # file-location attributes\n        self._set_fileattr = False\n        self._cached = None\n\n    def __repr__(self):\n        args = [f'name={self.name!r}', f'loader={self.loader!r}']\n        if self.origin is not None:\n            args.append(f'origin={self.origin!r}')\n        if self.submodule_search_locations is not None:\n            args.append(f'submodule_search_locations={self.submodule_search_locations}')\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __eq__(self, other):\n        smsl = self.submodule_search_locations\n        try:\n            return (self.name == other.name and\n                    self.loader == other.loader and\n                    self.origin == other.origin and\n                    smsl == other.submodule_search_locations and\n                    self.cached == other.cached and\n                    self.has_location == other.has_location)\n        except AttributeError:\n            return NotImplemented\n\n    @property\n    def cached(self):\n        if self._cached is None:\n            if self.origin is not None and self._set_fileattr:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                self._cached = _bootstrap_external._get_cached(self.origin)\n        return self._cached\n\n    @cached.setter\n    def cached(self, cached):\n        self._cached = cached\n\n    @property\n    def parent(self):\n        \"\"\"The name of the module's parent.\"\"\"\n        if self.submodule_search_locations is None:\n            return self.name.rpartition('.')[0]\n        else:\n            return self.name\n\n    @property\n    def has_location(self):\n        return self._set_fileattr\n\n    @has_location.setter\n    def has_location(self, value):\n        self._set_fileattr = bool(value)\n\n\ndef spec_from_loader(name, loader, *, origin=None, is_package=None):\n    \"\"\"Return a module spec based on various loader methods.\"\"\"\n    if origin is None:\n        origin = getattr(loader, '_ORIGIN', None)\n\n    if not origin and hasattr(loader, 'get_filename'):\n        if _bootstrap_external is None:\n            raise NotImplementedError\n        spec_from_file_location = _bootstrap_external.spec_from_file_location\n\n        if is_package is None:\n            return spec_from_file_location(name, loader=loader)\n        search = [] if is_package else None\n        return spec_from_file_location(name, loader=loader,\n                                       submodule_search_locations=search)\n\n    if is_package is None:\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                is_package = None  # aka, undefined\n        else:\n            # the default\n            is_package = False\n\n    return ModuleSpec(name, loader, origin=origin, is_package=is_package)\n\n\ndef _spec_from_module(module, loader=None, origin=None):\n    # This function is meant for use in _setup().\n    try:\n        spec = module.__spec__\n    except AttributeError:\n        pass\n    else:\n        if spec is not None:\n            return spec\n\n    name = module.__name__\n    if loader is None:\n        try:\n            loader = module.__loader__\n        except AttributeError:\n            # loader will stay None.\n            pass\n    try:\n        location = module.__file__\n    except AttributeError:\n        location = None\n    if origin is None:\n        if loader is not None:\n            origin = getattr(loader, '_ORIGIN', None)\n        if not origin and location is not None:\n            origin = location\n    try:\n        cached = module.__cached__\n    except AttributeError:\n        cached = None\n    try:\n        submodule_search_locations = list(module.__path__)\n    except AttributeError:\n        submodule_search_locations = None\n\n    spec = ModuleSpec(name, loader, origin=origin)\n    spec._set_fileattr = False if location is None else (origin == location)\n    spec.cached = cached\n    spec.submodule_search_locations = submodule_search_locations\n    return spec\n\n\ndef _init_module_attrs(spec, module, *, override=False):\n    # The passed-in module may be not support attribute assignment,\n    # in which case we simply don't set the attributes.\n    # __name__\n    if (override or getattr(module, '__name__', None) is None):\n        try:\n            module.__name__ = spec.name\n        except AttributeError:\n            pass\n    # __loader__\n    if override or getattr(module, '__loader__', None) is None:\n        loader = spec.loader\n        if loader is None:\n            # A backward compatibility hack.\n            if spec.submodule_search_locations is not None:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                NamespaceLoader = _bootstrap_external.NamespaceLoader\n\n                loader = NamespaceLoader.__new__(NamespaceLoader)\n                loader._path = spec.submodule_search_locations\n                spec.loader = loader\n                # While the docs say that module.__file__ is not set for\n                # built-in modules, and the code below will avoid setting it if\n                # spec.has_location is false, this is incorrect for namespace\n                # packages.  Namespace packages have no location, but their\n                # __spec__.origin is None, and thus their module.__file__\n                # should also be None for consistency.  While a bit of a hack,\n                # this is the best place to ensure this consistency.\n                #\n                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module\n                # and bpo-32305\n                module.__file__ = None\n        try:\n            module.__loader__ = loader\n        except AttributeError:\n            pass\n    # __package__\n    if override or getattr(module, '__package__', None) is None:\n        try:\n            module.__package__ = spec.parent\n        except AttributeError:\n            pass\n    # __spec__\n    try:\n        module.__spec__ = spec\n    except AttributeError:\n        pass\n    # __path__\n    if override or getattr(module, '__path__', None) is None:\n        if spec.submodule_search_locations is not None:\n            # XXX We should extend __path__ if it's already a list.\n            try:\n                module.__path__ = spec.submodule_search_locations\n            except AttributeError:\n                pass\n    # __file__/__cached__\n    if spec.has_location:\n        if override or getattr(module, '__file__', None) is None:\n            try:\n                module.__file__ = spec.origin\n            except AttributeError:\n                pass\n\n        if override or getattr(module, '__cached__', None) is None:\n            if spec.cached is not None:\n                try:\n                    module.__cached__ = spec.cached\n                except AttributeError:\n                    pass\n    return module\n\n\ndef module_from_spec(spec):\n    \"\"\"Create a module based on the provided spec.\"\"\"\n    # Typically loaders will not implement create_module().\n    module = None\n    if hasattr(spec.loader, 'create_module'):\n        # If create_module() returns `None` then it means default\n        # module creation should be used.\n        module = spec.loader.create_module(spec)\n    elif hasattr(spec.loader, 'exec_module'):\n        raise ImportError('loaders that define exec_module() '\n                          'must also define create_module()')\n    if module is None:\n        module = _new_module(spec.name)\n    _init_module_attrs(spec, module)\n    return module\n\n\ndef _module_repr_from_spec(spec):\n    \"\"\"Return the repr to use for the module.\"\"\"\n    name = '?' if spec.name is None else spec.name\n    if spec.origin is None:\n        loader = spec.loader\n        if loader is None:\n            return f'<module {name!r}>'\n        elif (\n            _bootstrap_external is not None\n            and isinstance(loader, _bootstrap_external.NamespaceLoader)\n        ):\n            return f'<module {name!r} (namespace) from {list(loader._path)}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        if spec.has_location:\n            return f'<module {name!r} from {spec.origin!r}>'\n        else:\n            return f'<module {spec.name!r} ({spec.origin})>'\n\n\n# Used by importlib.reload() and _load_module_shim().\ndef _exec(spec, module):\n    \"\"\"Execute the spec's specified module in an existing module's namespace.\"\"\"\n    name = spec.name\n    with _ModuleLockManager(name):\n        if sys.modules.get(name) is not module:\n            msg = f'module {name!r} not in sys.modules'\n            raise ImportError(msg, name=name)\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # Namespace package.\n                _init_module_attrs(spec, module, override=True)\n            else:\n                _init_module_attrs(spec, module, override=True)\n                if not hasattr(spec.loader, 'exec_module'):\n                    msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                           \"falling back to load_module()\")\n                    _warnings.warn(msg, ImportWarning)\n                    spec.loader.load_module(name)\n                else:\n                    spec.loader.exec_module(module)\n        finally:\n            # Update the order of insertion into sys.modules for module\n            # clean-up at shutdown.\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n    return module\n\n\ndef _load_backward_compatible(spec):\n    # It is assumed that all callers have been warned about using load_module()\n    # appropriately before calling this function.\n    try:\n        spec.loader.load_module(spec.name)\n    except:\n        if spec.name in sys.modules:\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n        raise\n    # The module must be in sys.modules at this point!\n    # Move it to the end of sys.modules.\n    module = sys.modules.pop(spec.name)\n    sys.modules[spec.name] = module\n    if getattr(module, '__loader__', None) is None:\n        try:\n            module.__loader__ = spec.loader\n        except AttributeError:\n            pass\n    if getattr(module, '__package__', None) is None:\n        try:\n            # Since module.__path__ may not line up with\n            # spec.submodule_search_paths, we can't necessarily rely\n            # on spec.parent here.\n            module.__package__ = module.__name__\n            if not hasattr(module, '__path__'):\n                module.__package__ = spec.name.rpartition('.')[0]\n        except AttributeError:\n            pass\n    if getattr(module, '__spec__', None) is None:\n        try:\n            module.__spec__ = spec\n        except AttributeError:\n            pass\n    return module\n\ndef _load_unlocked(spec):\n    # A helper for direct use by the import system.\n    if spec.loader is not None:\n        # Not a namespace package.\n        if not hasattr(spec.loader, 'exec_module'):\n            msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                    \"falling back to load_module()\")\n            _warnings.warn(msg, ImportWarning)\n            return _load_backward_compatible(spec)\n\n    module = module_from_spec(spec)\n\n    # This must be done before putting the module in sys.modules\n    # (otherwise an optimization shortcut in import.c becomes\n    # wrong).\n    spec._initializing = True\n    try:\n        sys.modules[spec.name] = module\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # A namespace package so do nothing.\n            else:\n                spec.loader.exec_module(module)\n        except:\n            try:\n                del sys.modules[spec.name]\n            except KeyError:\n                pass\n            raise\n        # Move the module to the end of sys.modules.\n        # We don't ensure that the import-related module attributes get\n        # set in the sys.modules replacement case.  Such modules are on\n        # their own.\n        module = sys.modules.pop(spec.name)\n        sys.modules[spec.name] = module\n        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)\n    finally:\n        spec._initializing = False\n\n    return module\n\n# A method used during testing of _load_unlocked() and by\n# _load_module_shim().\ndef _load(spec):\n    \"\"\"Return a new module object, loaded by the spec's loader.\n\n    The module is not added to its parent.\n\n    If a module is already in sys.modules, that existing module gets\n    clobbered.\n\n    \"\"\"\n    with _ModuleLockManager(spec.name):\n        return _load_unlocked(spec)\n\n\n# Loaders #####################################################################\n\nclass BuiltinImporter:\n\n    \"\"\"Meta path import for built-in modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"built-in\"\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        if _imp.is_builtin(fullname):\n            return spec_from_loader(fullname, cls, origin=cls._ORIGIN)\n        else:\n            return None\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Create a built-in module\"\"\"\n        if spec.name not in sys.builtin_module_names:\n            raise ImportError(f'{spec.name!r} is not a built-in module',\n                              name=spec.name)\n        return _call_with_frames_removed(_imp.create_builtin, spec)\n\n    @staticmethod\n    def exec_module(module):\n        \"\"\"Exec a built-in module\"\"\"\n        _call_with_frames_removed(_imp.exec_builtin, module)\n\n    @classmethod\n    @_requires_builtin\n    def get_code(cls, fullname):\n        \"\"\"Return None as built-in modules do not have code objects.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def get_source(cls, fullname):\n        \"\"\"Return None as built-in modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def is_package(cls, fullname):\n        \"\"\"Return False as built-in modules are never packages.\"\"\"\n        return False\n\n    load_module = classmethod(_load_module_shim)\n\n\nclass FrozenImporter:\n\n    \"\"\"Meta path import for frozen modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"frozen\"\n\n    @classmethod\n    def _fix_up_module(cls, module):\n        spec = module.__spec__\n        state = spec.loader_state\n        if state is None:\n            # The module is missing FrozenImporter-specific values.\n\n            # Fix up the spec attrs.\n            origname = vars(module).pop('__origname__', None)\n            assert origname, 'see PyImport_ImportFrozenModuleObject()'\n            ispkg = hasattr(module, '__path__')\n            assert _imp.is_frozen_package(module.__name__) == ispkg, ispkg\n            filename, pkgdir = cls._resolve_filename(origname, spec.name, ispkg)\n            spec.loader_state = type(sys.implementation)(\n                filename=filename,\n                origname=origname,\n            )\n            __path__ = spec.submodule_search_locations\n            if ispkg:\n                assert __path__ == [], __path__\n                if pkgdir:\n                    spec.submodule_search_locations.insert(0, pkgdir)\n            else:\n                assert __path__ is None, __path__\n\n            # Fix up the module attrs (the bare minimum).\n            assert not hasattr(module, '__file__'), module.__file__\n            if filename:\n                try:\n                    module.__file__ = filename\n                except AttributeError:\n                    pass\n            if ispkg:\n                if module.__path__ != __path__:\n                    assert module.__path__ == [], module.__path__\n                    module.__path__.extend(__path__)\n        else:\n            # These checks ensure that _fix_up_module() is only called\n            # in the right places.\n            __path__ = spec.submodule_search_locations\n            ispkg = __path__ is not None\n            # Check the loader state.\n            assert sorted(vars(state)) == ['filename', 'origname'], state\n            if state.origname:\n                # The only frozen modules with \"origname\" set are stdlib modules.\n                (__file__, pkgdir,\n                 ) = cls._resolve_filename(state.origname, spec.name, ispkg)\n                assert state.filename == __file__, (state.filename, __file__)\n                if pkgdir:\n                    assert __path__ == [pkgdir], (__path__, pkgdir)\n                else:\n                    assert __path__ == ([] if ispkg else None), __path__\n            else:\n                __file__ = None\n                assert state.filename is None, state.filename\n                assert __path__ == ([] if ispkg else None), __path__\n            # Check the file attrs.\n            if __file__:\n                assert hasattr(module, '__file__')\n                assert module.__file__ == __file__, (module.__file__, __file__)\n            else:\n                assert not hasattr(module, '__file__'), module.__file__\n            if ispkg:\n                assert hasattr(module, '__path__')\n                assert module.__path__ == __path__, (module.__path__, __path__)\n            else:\n                assert not hasattr(module, '__path__'), module.__path__\n        assert not spec.has_location\n\n    @classmethod\n    def _resolve_filename(cls, fullname, alias=None, ispkg=False):\n        if not fullname or not getattr(sys, '_stdlib_dir', None):\n            return None, None\n        try:\n            sep = cls._SEP\n        except AttributeError:\n            sep = cls._SEP = '\\\\' if sys.platform == 'win32' else '/'\n\n        if fullname != alias:\n            if fullname.startswith('<'):\n                fullname = fullname[1:]\n                if not ispkg:\n                    fullname = f'{fullname}.__init__'\n            else:\n                ispkg = False\n        relfile = fullname.replace('.', sep)\n        if ispkg:\n            pkgdir = f'{sys._stdlib_dir}{sep}{relfile}'\n            filename = f'{pkgdir}{sep}__init__.py'\n        else:\n            pkgdir = None\n            filename = f'{sys._stdlib_dir}{sep}{relfile}.py'\n        return filename, pkgdir\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        info = _call_with_frames_removed(_imp.find_frozen, fullname)\n        if info is None:\n            return None\n        # We get the marshaled data in exec_module() (the loader\n        # part of the importer), instead of here (the finder part).\n        # The loader is the usual place to get the data that will\n        # be loaded into the module.  (For example, see _LoaderBasics\n        # in _bootstra_external.py.)  Most importantly, this importer\n        # is simpler if we wait to get the data.\n        # However, getting as much data in the finder as possible\n        # to later load the module is okay, and sometimes important.\n        # (That's why ModuleSpec.loader_state exists.)  This is\n        # especially true if it avoids throwing away expensive data\n        # the loader would otherwise duplicate later and can be done\n        # efficiently.  In this case it isn't worth it.\n        _, ispkg, origname = info\n        spec = spec_from_loader(fullname, cls,\n                                origin=cls._ORIGIN,\n                                is_package=ispkg)\n        filename, pkgdir = cls._resolve_filename(origname, fullname, ispkg)\n        spec.loader_state = type(sys.implementation)(\n            filename=filename,\n            origname=origname,\n        )\n        if pkgdir:\n            spec.submodule_search_locations.insert(0, pkgdir)\n        return spec\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Set __file__, if able.\"\"\"\n        module = _new_module(spec.name)\n        try:\n            filename = spec.loader_state.filename\n        except AttributeError:\n            pass\n        else:\n            if filename:\n                module.__file__ = filename\n        return module\n\n    @staticmethod\n    def exec_module(module):\n        spec = module.__spec__\n        name = spec.name\n        code = _call_with_frames_removed(_imp.get_frozen_object, name)\n        exec(code, module.__dict__)\n\n    @classmethod\n    def load_module(cls, fullname):\n        \"\"\"Load a frozen module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # Warning about deprecation implemented in _load_module_shim().\n        module = _load_module_shim(cls, fullname)\n        info = _imp.find_frozen(fullname)\n        assert info is not None\n        _, ispkg, origname = info\n        module.__origname__ = origname\n        vars(module).pop('__file__', None)\n        if ispkg:\n            module.__path__ = []\n        cls._fix_up_module(module)\n        return module\n\n    @classmethod\n    @_requires_frozen\n    def get_code(cls, fullname):\n        \"\"\"Return the code object for the frozen module.\"\"\"\n        return _imp.get_frozen_object(fullname)\n\n    @classmethod\n    @_requires_frozen\n    def get_source(cls, fullname):\n        \"\"\"Return None as frozen modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_frozen\n    def is_package(cls, fullname):\n        \"\"\"Return True if the frozen module is a package.\"\"\"\n        return _imp.is_frozen_package(fullname)\n\n\n# Import itself ###############################################################\n\nclass _ImportLockContext:\n\n    \"\"\"Context manager for the import lock.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Acquire the import lock.\"\"\"\n        _imp.acquire_lock()\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Release the import lock regardless of any raised exceptions.\"\"\"\n        _imp.release_lock()\n\n\ndef _resolve_name(name, package, level):\n    \"\"\"Resolve a relative module name to an absolute one.\"\"\"\n    bits = package.rsplit('.', level - 1)\n    if len(bits) < level:\n        raise ImportError('attempted relative import beyond top-level package')\n    base = bits[0]\n    return f'{base}.{name}' if name else base\n\n\ndef _find_spec(name, path, target=None):\n    \"\"\"Find a module's spec.\"\"\"\n    meta_path = sys.meta_path\n    if meta_path is None:\n        # PyImport_Cleanup() is running or has been called.\n        raise ImportError(\"sys.meta_path is None, Python is likely \"\n                          \"shutting down\")\n\n    if not meta_path:\n        _warnings.warn('sys.meta_path is empty', ImportWarning)\n\n    # We check sys.modules here for the reload case.  While a passed-in\n    # target will usually indicate a reload there is no guarantee, whereas\n    # sys.modules provides one.\n    is_reload = name in sys.modules\n    for finder in meta_path:\n        with _ImportLockContext():\n            try:\n                find_spec = finder.find_spec\n            except AttributeError:\n                continue\n            else:\n                spec = find_spec(name, path, target)\n        if spec is not None:\n            # The parent import may have already imported this module.\n            if not is_reload and name in sys.modules:\n                module = sys.modules[name]\n                try:\n                    __spec__ = module.__spec__\n                except AttributeError:\n                    # We use the found spec since that is the one that\n                    # we would have used if the parent module hadn't\n                    # beaten us to the punch.\n                    return spec\n                else:\n                    if __spec__ is None:\n                        return spec\n                    else:\n                        return __spec__\n            else:\n                return spec\n    else:\n        return None\n\n\ndef _sanity_check(name, package, level):\n    \"\"\"Verify arguments are \"sane\".\"\"\"\n    if not isinstance(name, str):\n        raise TypeError(f'module name must be str, not {type(name)}')\n    if level < 0:\n        raise ValueError('level must be >= 0')\n    if level > 0:\n        if not isinstance(package, str):\n            raise TypeError('__package__ not set to a string')\n        elif not package:\n            raise ImportError('attempted relative import with no known parent '\n                              'package')\n    if not name and level == 0:\n        raise ValueError('Empty module name')\n\n\n_ERR_MSG_PREFIX = 'No module named '\n_ERR_MSG = _ERR_MSG_PREFIX + '{!r}'\n\ndef _find_and_load_unlocked(name, import_):\n    path = None\n    parent = name.rpartition('.')[0]\n    parent_spec = None\n    if parent:\n        if parent not in sys.modules:\n            _call_with_frames_removed(import_, parent)\n        # Crazy side-effects!\n        if name in sys.modules:\n            return sys.modules[name]\n        parent_module = sys.modules[parent]\n        try:\n            path = parent_module.__path__\n        except AttributeError:\n            msg = f'{_ERR_MSG_PREFIX}{name!r}; {parent!r} is not a package'\n            raise ModuleNotFoundError(msg, name=name) from None\n        parent_spec = parent_module.__spec__\n        child = name.rpartition('.')[2]\n    spec = _find_spec(name, path)\n    if spec is None:\n        raise ModuleNotFoundError(f'{_ERR_MSG_PREFIX}{name!r}', name=name)\n    else:\n        if parent_spec:\n            # Temporarily add child we are currently importing to parent's\n            # _uninitialized_submodules for circular import tracking.\n            parent_spec._uninitialized_submodules.append(child)\n        try:\n            module = _load_unlocked(spec)\n        finally:\n            if parent_spec:\n                parent_spec._uninitialized_submodules.pop()\n    if parent:\n        # Set the module as an attribute on its parent.\n        parent_module = sys.modules[parent]\n        try:\n            setattr(parent_module, child, module)\n        except AttributeError:\n            msg = f\"Cannot set an attribute on {parent!r} for child module {child!r}\"\n            _warnings.warn(msg, ImportWarning)\n    return module\n\n\n_NEEDS_LOADING = object()\n\n\ndef _find_and_load(name, import_):\n    \"\"\"Find and load the module.\"\"\"\n\n    # Optimization: we avoid unneeded module locking if the module\n    # already exists in sys.modules and is fully initialized.\n    module = sys.modules.get(name, _NEEDS_LOADING)\n    if (module is _NEEDS_LOADING or\n        getattr(getattr(module, \"__spec__\", None), \"_initializing\", False)):\n        with _ModuleLockManager(name):\n            module = sys.modules.get(name, _NEEDS_LOADING)\n            if module is _NEEDS_LOADING:\n                return _find_and_load_unlocked(name, import_)\n\n        # Optimization: only call _bootstrap._lock_unlock_module() if\n        # module.__spec__._initializing is True.\n        # NOTE: because of this, initializing must be set *before*\n        # putting the new module in sys.modules.\n        _lock_unlock_module(name)\n\n    if module is None:\n        message = f'import of {name} halted; None in sys.modules'\n        raise ModuleNotFoundError(message, name=name)\n\n    return module\n\n\ndef _gcd_import(name, package=None, level=0):\n    \"\"\"Import and return the module based on its name, the package the call is\n    being made from, and the level adjustment.\n\n    This function represents the greatest common denominator of functionality\n    between import_module and __import__. This includes setting __package__ if\n    the loader did not.\n\n    \"\"\"\n    _sanity_check(name, package, level)\n    if level > 0:\n        name = _resolve_name(name, package, level)\n    return _find_and_load(name, _gcd_import)\n\n\ndef _handle_fromlist(module, fromlist, import_, *, recursive=False):\n    \"\"\"Figure out what __import__ should return.\n\n    The import_ parameter is a callable which takes the name of module to\n    import. It is required to decouple the function from assuming importlib's\n    import implementation is desired.\n\n    \"\"\"\n    # The hell that is fromlist ...\n    # If a package was imported, try to import stuff from fromlist.\n    for x in fromlist:\n        if not isinstance(x, str):\n            if recursive:\n                where = module.__name__ + '.__all__'\n            else:\n                where = \"``from list''\"\n            raise TypeError(f\"Item in {where} must be str, \"\n                            f\"not {type(x).__name__}\")\n        elif x == '*':\n            if not recursive and hasattr(module, '__all__'):\n                _handle_fromlist(module, module.__all__, import_,\n                                 recursive=True)\n        elif not hasattr(module, x):\n            from_name = f'{module.__name__}.{x}'\n            try:\n                _call_with_frames_removed(import_, from_name)\n            except ModuleNotFoundError as exc:\n                # Backwards-compatibility dictates we ignore failed\n                # imports triggered by fromlist for modules that don't\n                # exist.\n                if (exc.name == from_name and\n                    sys.modules.get(from_name, _NEEDS_LOADING) is not None):\n                    continue\n                raise\n    return module\n\n\ndef _calc___package__(globals):\n    \"\"\"Calculate what __package__ should be.\n\n    __package__ is not guaranteed to be defined or could be set to None\n    to represent that its proper value is unknown.\n\n    \"\"\"\n    package = globals.get('__package__')\n    spec = globals.get('__spec__')\n    if package is not None:\n        if spec is not None and package != spec.parent:\n            _warnings.warn(\"__package__ != __spec__.parent \"\n                           f\"({package!r} != {spec.parent!r})\",\n                           DeprecationWarning, stacklevel=3)\n        return package\n    elif spec is not None:\n        return spec.parent\n    else:\n        _warnings.warn(\"can't resolve package from __spec__ or __package__, \"\n                       \"falling back on __name__ and __path__\",\n                       ImportWarning, stacklevel=3)\n        package = globals['__name__']\n        if '__path__' not in globals:\n            package = package.rpartition('.')[0]\n    return package\n\n\ndef __import__(name, globals=None, locals=None, fromlist=(), level=0):\n    \"\"\"Import a module.\n\n    The 'globals' argument is used to infer where the import is occurring from\n    to handle relative imports. The 'locals' argument is ignored. The\n    'fromlist' argument specifies what should exist as attributes on the module\n    being imported (e.g. ``from module import <fromlist>``).  The 'level'\n    argument represents the package location to import from in a relative\n    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).\n\n    \"\"\"\n    if level == 0:\n        module = _gcd_import(name)\n    else:\n        globals_ = globals if globals is not None else {}\n        package = _calc___package__(globals_)\n        module = _gcd_import(name, package, level)\n    if not fromlist:\n        # Return up to the first dot in 'name'. This is complicated by the fact\n        # that 'name' may be relative.\n        if level == 0:\n            return _gcd_import(name.partition('.')[0])\n        elif not name:\n            return module\n        else:\n            # Figure out where to slice the module's name up to the first dot\n            # in 'name'.\n            cut_off = len(name) - len(name.partition('.')[0])\n            # Slice end needs to be positive to alleviate need to special-case\n            # when ``'.' not in name``.\n            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]\n    elif hasattr(module, '__path__'):\n        return _handle_fromlist(module, fromlist, _gcd_import)\n    else:\n        return module\n\n\ndef _builtin_from_name(name):\n    spec = BuiltinImporter.find_spec(name)\n    if spec is None:\n        raise ImportError('no built-in module named ' + name)\n    return _load_unlocked(spec)\n\n\ndef _setup(sys_module, _imp_module):\n    \"\"\"Setup importlib by importing needed built-in modules and injecting them\n    into the global namespace.\n\n    As sys is needed for sys.modules access and _imp is needed to load built-in\n    modules, those two modules must be explicitly passed in.\n\n    \"\"\"\n    global _imp, sys, _blocking_on\n    _imp = _imp_module\n    sys = sys_module\n\n    # Set up the spec for existing builtin/frozen modules.\n    module_type = type(sys)\n    for name, module in sys.modules.items():\n        if isinstance(module, module_type):\n            if name in sys.builtin_module_names:\n                loader = BuiltinImporter\n            elif _imp.is_frozen(name):\n                loader = FrozenImporter\n            else:\n                continue\n            spec = _spec_from_module(module, loader)\n            _init_module_attrs(spec, module)\n            if loader is FrozenImporter:\n                loader._fix_up_module(module)\n\n    # Directly load built-in modules needed during bootstrap.\n    self_module = sys.modules[__name__]\n    for builtin_name in ('_thread', '_warnings', '_weakref'):\n        if builtin_name not in sys.modules:\n            builtin_module = _builtin_from_name(builtin_name)\n        else:\n            builtin_module = sys.modules[builtin_name]\n        setattr(self_module, builtin_name, builtin_module)\n\n    # Instantiation requires _weakref to have been set.\n    _blocking_on = _WeakValueDictionary()\n\n\ndef _install(sys_module, _imp_module):\n    \"\"\"Install importers for builtin and frozen modules\"\"\"\n    _setup(sys_module, _imp_module)\n\n    sys.meta_path.append(BuiltinImporter)\n    sys.meta_path.append(FrozenImporter)\n\n\ndef _install_external_importers():\n    \"\"\"Install importers that require external filesystem access\"\"\"\n    global _bootstrap_external\n    import _frozen_importlib_external\n    _bootstrap_external = _frozen_importlib_external\n    _frozen_importlib_external._install(sys.modules[__name__])\n", 1551], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dis.py": ["\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\nimport collections\nimport io\n\nfrom opcode import *\nfrom opcode import (\n    __all__ as _opcodes_all,\n    _cache_format,\n    _inline_cache_entries,\n    _nb_ops,\n    _intrinsic_1_descs,\n    _intrinsic_2_descs,\n    _specializations,\n    _specialized_instructions,\n)\n\n__all__ = [\"code_info\", \"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\", \"show_code\",\n           \"get_instructions\", \"Instruction\", \"Bytecode\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              classmethod, staticmethod, type)\n\nFORMAT_VALUE = opmap['FORMAT_VALUE']\nFORMAT_VALUE_CONVERTERS = (\n    (None, ''),\n    (str, 'str'),\n    (repr, 'repr'),\n    (ascii, 'ascii'),\n)\nMAKE_FUNCTION = opmap['MAKE_FUNCTION']\nMAKE_FUNCTION_FLAGS = ('defaults', 'kwdefaults', 'annotations', 'closure')\n\nLOAD_CONST = opmap['LOAD_CONST']\nRETURN_CONST = opmap['RETURN_CONST']\nLOAD_GLOBAL = opmap['LOAD_GLOBAL']\nBINARY_OP = opmap['BINARY_OP']\nJUMP_BACKWARD = opmap['JUMP_BACKWARD']\nFOR_ITER = opmap['FOR_ITER']\nSEND = opmap['SEND']\nLOAD_ATTR = opmap['LOAD_ATTR']\nLOAD_SUPER_ATTR = opmap['LOAD_SUPER_ATTR']\nCALL_INTRINSIC_1 = opmap['CALL_INTRINSIC_1']\nCALL_INTRINSIC_2 = opmap['CALL_INTRINSIC_2']\n\nCACHE = opmap[\"CACHE\"]\n\n_all_opname = list(opname)\n_all_opmap = dict(opmap)\n_empty_slot = [slot for slot, name in enumerate(_all_opname) if name.startswith(\"<\")]\nfor spec_op, specialized in zip(_empty_slot, _specialized_instructions):\n    # fill opname and opmap\n    _all_opname[spec_op] = specialized\n    _all_opmap[specialized] = spec_op\n\ndeoptmap = {\n    specialized: base for base, family in _specializations.items() for specialized in family\n}\n\ndef _try_compile(source, name):\n    \"\"\"Attempts to compile the given source, first as an expression and\n       then as a statement if the first approach fails.\n\n       Utility function to accept strings in functions that otherwise\n       expect code objects\n    \"\"\"\n    try:\n        return compile(source, name, 'eval')\n    except SyntaxError:\n        pass\n    return compile(source, name, 'exec')\n\ndef dis(x=None, *, file=None, depth=None, show_caches=False, adaptive=False):\n    \"\"\"Disassemble classes, methods, functions, and other compiled objects.\n\n    With no argument, disassemble the last traceback.\n\n    Compiled objects currently include generator objects, async generator\n    objects, and coroutine objects, all of which store their code object\n    in a special attribute.\n    \"\"\"\n    if x is None:\n        distb(file=file, show_caches=show_caches, adaptive=adaptive)\n        return\n    # Extract functions from methods.\n    if hasattr(x, '__func__'):\n        x = x.__func__\n    # Extract compiled code objects from...\n    if hasattr(x, '__code__'):  # ...a function, or\n        x = x.__code__\n    elif hasattr(x, 'gi_code'):  #...a generator object, or\n        x = x.gi_code\n    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or\n        x = x.ag_code\n    elif hasattr(x, 'cr_code'):  #...a coroutine.\n        x = x.cr_code\n    # Perform the disassembly.\n    if hasattr(x, '__dict__'):  # Class or module\n        items = sorted(x.__dict__.items())\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print(\"Disassembly of %s:\" % name, file=file)\n                try:\n                    dis(x1, file=file, depth=depth, show_caches=show_caches, adaptive=adaptive)\n                except TypeError as msg:\n                    print(\"Sorry:\", msg, file=file)\n                print(file=file)\n    elif hasattr(x, 'co_code'): # Code object\n        _disassemble_recursive(x, file=file, depth=depth, show_caches=show_caches, adaptive=adaptive)\n    elif isinstance(x, (bytes, bytearray)): # Raw bytecode\n        _disassemble_bytes(x, file=file, show_caches=show_caches)\n    elif isinstance(x, str):    # Source code\n        _disassemble_str(x, file=file, depth=depth, show_caches=show_caches, adaptive=adaptive)\n    else:\n        raise TypeError(\"don't know how to disassemble %s objects\" %\n                        type(x).__name__)\n\ndef distb(tb=None, *, file=None, show_caches=False, adaptive=False):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            if hasattr(sys, 'last_exc'):\n                tb = sys.last_exc.__traceback__\n            else:\n                tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError(\"no last traceback to disassemble\") from None\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti, file=file, show_caches=show_caches, adaptive=adaptive)\n\n# The inspect module interrogates this dictionary to build its\n# list of CO_* constants. It is also used by pretty_flags to\n# turn the co_flags field into a human readable list.\nCOMPILER_FLAG_NAMES = {\n     1: \"OPTIMIZED\",\n     2: \"NEWLOCALS\",\n     4: \"VARARGS\",\n     8: \"VARKEYWORDS\",\n    16: \"NESTED\",\n    32: \"GENERATOR\",\n    64: \"NOFREE\",\n   128: \"COROUTINE\",\n   256: \"ITERABLE_COROUTINE\",\n   512: \"ASYNC_GENERATOR\",\n}\n\ndef pretty_flags(flags):\n    \"\"\"Return pretty representation of code flags.\"\"\"\n    names = []\n    for i in range(32):\n        flag = 1<<i\n        if flags & flag:\n            names.append(COMPILER_FLAG_NAMES.get(flag, hex(flag)))\n            flags ^= flag\n            if not flags:\n                break\n    else:\n        names.append(hex(flags))\n    return \", \".join(names)\n\nclass _Unknown:\n    def __repr__(self):\n        return \"<unknown>\"\n\n# Sentinel to represent values that cannot be calculated\nUNKNOWN = _Unknown()\n\ndef _get_code_object(x):\n    \"\"\"Helper to handle methods, compiled or raw code objects, and strings.\"\"\"\n    # Extract functions from methods.\n    if hasattr(x, '__func__'):\n        x = x.__func__\n    # Extract compiled code objects from...\n    if hasattr(x, '__code__'):  # ...a function, or\n        x = x.__code__\n    elif hasattr(x, 'gi_code'):  #...a generator object, or\n        x = x.gi_code\n    elif hasattr(x, 'ag_code'):  #...an asynchronous generator object, or\n        x = x.ag_code\n    elif hasattr(x, 'cr_code'):  #...a coroutine.\n        x = x.cr_code\n    # Handle source code.\n    if isinstance(x, str):\n        x = _try_compile(x, \"<disassembly>\")\n    # By now, if we don't have a code object, we can't disassemble x.\n    if hasattr(x, 'co_code'):\n        return x\n    raise TypeError(\"don't know how to disassemble %s objects\" %\n                    type(x).__name__)\n\ndef _deoptop(op):\n    name = _all_opname[op]\n    return _all_opmap[deoptmap[name]] if name in deoptmap else op\n\ndef _get_code_array(co, adaptive):\n    return co._co_code_adaptive if adaptive else co.co_code\n\ndef code_info(x):\n    \"\"\"Formatted details of methods, functions, or code.\"\"\"\n    return _format_code_info(_get_code_object(x))\n\ndef _format_code_info(co):\n    lines = []\n    lines.append(\"Name:              %s\" % co.co_name)\n    lines.append(\"Filename:          %s\" % co.co_filename)\n    lines.append(\"Argument count:    %s\" % co.co_argcount)\n    lines.append(\"Positional-only arguments: %s\" % co.co_posonlyargcount)\n    lines.append(\"Kw-only arguments: %s\" % co.co_kwonlyargcount)\n    lines.append(\"Number of locals:  %s\" % co.co_nlocals)\n    lines.append(\"Stack size:        %s\" % co.co_stacksize)\n    lines.append(\"Flags:             %s\" % pretty_flags(co.co_flags))\n    if co.co_consts:\n        lines.append(\"Constants:\")\n        for i_c in enumerate(co.co_consts):\n            lines.append(\"%4d: %r\" % i_c)\n    if co.co_names:\n        lines.append(\"Names:\")\n        for i_n in enumerate(co.co_names):\n            lines.append(\"%4d: %s\" % i_n)\n    if co.co_varnames:\n        lines.append(\"Variable names:\")\n        for i_n in enumerate(co.co_varnames):\n            lines.append(\"%4d: %s\" % i_n)\n    if co.co_freevars:\n        lines.append(\"Free variables:\")\n        for i_n in enumerate(co.co_freevars):\n            lines.append(\"%4d: %s\" % i_n)\n    if co.co_cellvars:\n        lines.append(\"Cell variables:\")\n        for i_n in enumerate(co.co_cellvars):\n            lines.append(\"%4d: %s\" % i_n)\n    return \"\\n\".join(lines)\n\ndef show_code(co, *, file=None):\n    \"\"\"Print details of methods, functions, or code to *file*.\n\n    If *file* is not provided, the output is printed on stdout.\n    \"\"\"\n    print(code_info(co), file=file)\n\nPositions = collections.namedtuple(\n    'Positions',\n    [\n        'lineno',\n        'end_lineno',\n        'col_offset',\n        'end_col_offset',\n    ],\n    defaults=[None] * 4\n)\n\n_Instruction = collections.namedtuple(\n    \"_Instruction\",\n    [\n        'opname',\n        'opcode',\n        'arg',\n        'argval',\n        'argrepr',\n        'offset',\n        'starts_line',\n        'is_jump_target',\n        'positions'\n    ],\n    defaults=[None]\n)\n\n_Instruction.opname.__doc__ = \"Human readable name for operation\"\n_Instruction.opcode.__doc__ = \"Numeric code for operation\"\n_Instruction.arg.__doc__ = \"Numeric argument to operation (if any), otherwise None\"\n_Instruction.argval.__doc__ = \"Resolved arg value (if known), otherwise same as arg\"\n_Instruction.argrepr.__doc__ = \"Human readable description of operation argument\"\n_Instruction.offset.__doc__ = \"Start index of operation within bytecode sequence\"\n_Instruction.starts_line.__doc__ = \"Line started by this opcode (if any), otherwise None\"\n_Instruction.is_jump_target.__doc__ = \"True if other code jumps to here, otherwise False\"\n_Instruction.positions.__doc__ = \"dis.Positions object holding the span of source code covered by this instruction\"\n\n_ExceptionTableEntry = collections.namedtuple(\"_ExceptionTableEntry\",\n    \"start end target depth lasti\")\n\n_OPNAME_WIDTH = 20\n_OPARG_WIDTH = 5\n\nclass Instruction(_Instruction):\n    \"\"\"Details for a bytecode operation\n\n       Defined fields:\n         opname - human readable name for operation\n         opcode - numeric code for operation\n         arg - numeric argument to operation (if any), otherwise None\n         argval - resolved arg value (if known), otherwise same as arg\n         argrepr - human readable description of operation argument\n         offset - start index of operation within bytecode sequence\n         starts_line - line started by this opcode (if any), otherwise None\n         is_jump_target - True if other code jumps to here, otherwise False\n         positions - Optional dis.Positions object holding the span of source code\n                     covered by this instruction\n    \"\"\"\n\n    def _disassemble(self, lineno_width=3, mark_as_current=False, offset_width=4):\n        \"\"\"Format instruction details for inclusion in disassembly output\n\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *mark_as_current* inserts a '-->' marker arrow as part of the line\n        *offset_width* sets the width of the instruction offset field\n        \"\"\"\n        fields = []\n        # Column: Source code line number\n        if lineno_width:\n            if self.starts_line is not None:\n                lineno_fmt = \"%%%dd\" % lineno_width\n                fields.append(lineno_fmt % self.starts_line)\n            else:\n                fields.append(' ' * lineno_width)\n        # Column: Current instruction indicator\n        if mark_as_current:\n            fields.append('-->')\n        else:\n            fields.append('   ')\n        # Column: Jump target marker\n        if self.is_jump_target:\n            fields.append('>>')\n        else:\n            fields.append('  ')\n        # Column: Instruction offset from start of code sequence\n        fields.append(repr(self.offset).rjust(offset_width))\n        # Column: Opcode name\n        fields.append(self.opname.ljust(_OPNAME_WIDTH))\n        # Column: Opcode argument\n        if self.arg is not None:\n            fields.append(repr(self.arg).rjust(_OPARG_WIDTH))\n            # Column: Opcode argument details\n            if self.argrepr:\n                fields.append('(' + self.argrepr + ')')\n        return ' '.join(fields).rstrip()\n\n\ndef get_instructions(x, *, first_line=None, show_caches=False, adaptive=False):\n    \"\"\"Iterator for the opcodes in methods, functions or code\n\n    Generates a series of Instruction named tuples giving the details of\n    each operations in the supplied code.\n\n    If *first_line* is not None, it indicates the line number that should\n    be reported for the first source line in the disassembled code.\n    Otherwise, the source line information (if any) is taken directly from\n    the disassembled code object.\n    \"\"\"\n    co = _get_code_object(x)\n    linestarts = dict(findlinestarts(co))\n    if first_line is not None:\n        line_offset = first_line - co.co_firstlineno\n    else:\n        line_offset = 0\n    return _get_instructions_bytes(_get_code_array(co, adaptive),\n                                   co._varname_from_oparg,\n                                   co.co_names, co.co_consts,\n                                   linestarts, line_offset,\n                                   co_positions=co.co_positions(),\n                                   show_caches=show_caches)\n\ndef _get_const_value(op, arg, co_consts):\n    \"\"\"Helper to get the value of the const in a hasconst op.\n\n       Returns the dereferenced constant if this is possible.\n       Otherwise (if it is a LOAD_CONST and co_consts is not\n       provided) returns the dis.UNKNOWN sentinel.\n    \"\"\"\n    assert op in hasconst\n\n    argval = UNKNOWN\n    if co_consts is not None:\n        argval = co_consts[arg]\n    return argval\n\ndef _get_const_info(op, arg, co_consts):\n    \"\"\"Helper to get optional details about const references\n\n       Returns the dereferenced constant and its repr if the value\n       can be calculated.\n       Otherwise returns the sentinel value dis.UNKNOWN for the value\n       and an empty string for its repr.\n    \"\"\"\n    argval = _get_const_value(op, arg, co_consts)\n    argrepr = repr(argval) if argval is not UNKNOWN else ''\n    return argval, argrepr\n\ndef _get_name_info(name_index, get_name, **extrainfo):\n    \"\"\"Helper to get optional details about named references\n\n       Returns the dereferenced name as both value and repr if the name\n       list is defined.\n       Otherwise returns the sentinel value dis.UNKNOWN for the value\n       and an empty string for its repr.\n    \"\"\"\n    if get_name is not None:\n        argval = get_name(name_index, **extrainfo)\n        return argval, argval\n    else:\n        return UNKNOWN, ''\n\ndef _parse_varint(iterator):\n    b = next(iterator)\n    val = b & 63\n    while b&64:\n        val <<= 6\n        b = next(iterator)\n        val |= b&63\n    return val\n\ndef _parse_exception_table(code):\n    iterator = iter(code.co_exceptiontable)\n    entries = []\n    try:\n        while True:\n            start = _parse_varint(iterator)*2\n            length = _parse_varint(iterator)*2\n            end = start + length\n            target = _parse_varint(iterator)*2\n            dl = _parse_varint(iterator)\n            depth = dl >> 1\n            lasti = bool(dl&1)\n            entries.append(_ExceptionTableEntry(start, end, target, depth, lasti))\n    except StopIteration:\n        return entries\n\ndef _is_backward_jump(op):\n    return 'JUMP_BACKWARD' in opname[op]\n\ndef _get_instructions_bytes(code, varname_from_oparg=None,\n                            names=None, co_consts=None,\n                            linestarts=None, line_offset=0,\n                            exception_entries=(), co_positions=None,\n                            show_caches=False):\n    \"\"\"Iterate over the instructions in a bytecode string.\n\n    Generates a sequence of Instruction namedtuples giving the details of each\n    opcode.  Additional information about the code's runtime environment\n    (e.g. variable names, co_consts) can be specified using optional\n    arguments.\n\n    \"\"\"\n    co_positions = co_positions or iter(())\n    get_name = None if names is None else names.__getitem__\n    labels = set(findlabels(code))\n    for start, end, target, _, _ in exception_entries:\n        for i in range(start, end):\n            labels.add(target)\n    starts_line = None\n    for offset, op, arg in _unpack_opargs(code):\n        if linestarts is not None:\n            starts_line = linestarts.get(offset, None)\n            if starts_line is not None:\n                starts_line += line_offset\n        is_jump_target = offset in labels\n        argval = None\n        argrepr = ''\n        positions = Positions(*next(co_positions, ()))\n        deop = _deoptop(op)\n        caches = _inline_cache_entries[deop]\n        if arg is not None:\n            #  Set argval to the dereferenced value of the argument when\n            #  available, and argrepr to the string representation of argval.\n            #    _disassemble_bytes needs the string repr of the\n            #    raw name index for LOAD_GLOBAL, LOAD_CONST, etc.\n            argval = arg\n            if deop in hasconst:\n                argval, argrepr = _get_const_info(deop, arg, co_consts)\n            elif deop in hasname:\n                if deop == LOAD_GLOBAL:\n                    argval, argrepr = _get_name_info(arg//2, get_name)\n                    if (arg & 1) and argrepr:\n                        argrepr = \"NULL + \" + argrepr\n                elif deop == LOAD_ATTR:\n                    argval, argrepr = _get_name_info(arg//2, get_name)\n                    if (arg & 1) and argrepr:\n                        argrepr = \"NULL|self + \" + argrepr\n                elif deop == LOAD_SUPER_ATTR:\n                    argval, argrepr = _get_name_info(arg//4, get_name)\n                    if (arg & 1) and argrepr:\n                        argrepr = \"NULL|self + \" + argrepr\n                else:\n                    argval, argrepr = _get_name_info(arg, get_name)\n            elif deop in hasjabs:\n                argval = arg*2\n                argrepr = \"to \" + repr(argval)\n            elif deop in hasjrel:\n                signed_arg = -arg if _is_backward_jump(deop) else arg\n                argval = offset + 2 + signed_arg*2\n                argval += 2 * caches\n                argrepr = \"to \" + repr(argval)\n            elif deop in haslocal or deop in hasfree:\n                argval, argrepr = _get_name_info(arg, varname_from_oparg)\n            elif deop in hascompare:\n                argval = cmp_op[arg>>4]\n                argrepr = argval\n            elif deop == FORMAT_VALUE:\n                argval, argrepr = FORMAT_VALUE_CONVERTERS[arg & 0x3]\n                argval = (argval, bool(arg & 0x4))\n                if argval[1]:\n                    if argrepr:\n                        argrepr += ', '\n                    argrepr += 'with format'\n            elif deop == MAKE_FUNCTION:\n                argrepr = ', '.join(s for i, s in enumerate(MAKE_FUNCTION_FLAGS)\n                                    if arg & (1<<i))\n            elif deop == BINARY_OP:\n                _, argrepr = _nb_ops[arg]\n            elif deop == CALL_INTRINSIC_1:\n                argrepr = _intrinsic_1_descs[arg]\n            elif deop == CALL_INTRINSIC_2:\n                argrepr = _intrinsic_2_descs[arg]\n        yield Instruction(_all_opname[op], op,\n                          arg, argval, argrepr,\n                          offset, starts_line, is_jump_target, positions)\n        caches = _inline_cache_entries[deop]\n        if not caches:\n            continue\n        if not show_caches:\n            # We still need to advance the co_positions iterator:\n            for _ in range(caches):\n                next(co_positions, ())\n            continue\n        for name, size in _cache_format[opname[deop]].items():\n            for i in range(size):\n                offset += 2\n                # Only show the fancy argrepr for a CACHE instruction when it's\n                # the first entry for a particular cache value:\n                if i == 0:\n                    data = code[offset: offset + 2 * size]\n                    argrepr = f\"{name}: {int.from_bytes(data, sys.byteorder)}\"\n                else:\n                    argrepr = \"\"\n                yield Instruction(\n                    \"CACHE\", CACHE, 0, None, argrepr, offset, None, False,\n                    Positions(*next(co_positions, ()))\n                )\n\ndef disassemble(co, lasti=-1, *, file=None, show_caches=False, adaptive=False):\n    \"\"\"Disassemble a code object.\"\"\"\n    linestarts = dict(findlinestarts(co))\n    exception_entries = _parse_exception_table(co)\n    _disassemble_bytes(_get_code_array(co, adaptive),\n                       lasti, co._varname_from_oparg,\n                       co.co_names, co.co_consts, linestarts, file=file,\n                       exception_entries=exception_entries,\n                       co_positions=co.co_positions(), show_caches=show_caches)\n\ndef _disassemble_recursive(co, *, file=None, depth=None, show_caches=False, adaptive=False):\n    disassemble(co, file=file, show_caches=show_caches, adaptive=adaptive)\n    if depth is None or depth > 0:\n        if depth is not None:\n            depth = depth - 1\n        for x in co.co_consts:\n            if hasattr(x, 'co_code'):\n                print(file=file)\n                print(\"Disassembly of %r:\" % (x,), file=file)\n                _disassemble_recursive(\n                    x, file=file, depth=depth, show_caches=show_caches, adaptive=adaptive\n                )\n\ndef _disassemble_bytes(code, lasti=-1, varname_from_oparg=None,\n                       names=None, co_consts=None, linestarts=None,\n                       *, file=None, line_offset=0, exception_entries=(),\n                       co_positions=None, show_caches=False):\n    # Omit the line number column entirely if we have no line number info\n    show_lineno = bool(linestarts)\n    if show_lineno:\n        maxlineno = max(linestarts.values()) + line_offset\n        if maxlineno >= 1000:\n            lineno_width = len(str(maxlineno))\n        else:\n            lineno_width = 3\n    else:\n        lineno_width = 0\n    maxoffset = len(code) - 2\n    if maxoffset >= 10000:\n        offset_width = len(str(maxoffset))\n    else:\n        offset_width = 4\n    for instr in _get_instructions_bytes(code, varname_from_oparg, names,\n                                         co_consts, linestarts,\n                                         line_offset=line_offset,\n                                         exception_entries=exception_entries,\n                                         co_positions=co_positions,\n                                         show_caches=show_caches):\n        new_source_line = (show_lineno and\n                           instr.starts_line is not None and\n                           instr.offset > 0)\n        if new_source_line:\n            print(file=file)\n        if show_caches:\n            is_current_instr = instr.offset == lasti\n        else:\n            # Each CACHE takes 2 bytes\n            is_current_instr = instr.offset <= lasti \\\n                <= instr.offset + 2 * _inline_cache_entries[_deoptop(instr.opcode)]\n        print(instr._disassemble(lineno_width, is_current_instr, offset_width),\n              file=file)\n    if exception_entries:\n        print(\"ExceptionTable:\", file=file)\n        for entry in exception_entries:\n            lasti = \" lasti\" if entry.lasti else \"\"\n            end = entry.end-2\n            print(f\"  {entry.start} to {end} -> {entry.target} [{entry.depth}]{lasti}\", file=file)\n\ndef _disassemble_str(source, **kwargs):\n    \"\"\"Compile the source string, then disassemble the code object.\"\"\"\n    _disassemble_recursive(_try_compile(source, '<dis>'), **kwargs)\n\ndisco = disassemble                     # XXX For backwards compatibility\n\n\n# Rely on C `int` being 32 bits for oparg\n_INT_BITS = 32\n# Value for c int when it overflows\n_INT_OVERFLOW = 2 ** (_INT_BITS - 1)\n\ndef _unpack_opargs(code):\n    extended_arg = 0\n    caches = 0\n    for i in range(0, len(code), 2):\n        # Skip inline CACHE entries:\n        if caches:\n            caches -= 1\n            continue\n        op = code[i]\n        deop = _deoptop(op)\n        caches = _inline_cache_entries[deop]\n        if deop in hasarg:\n            arg = code[i+1] | extended_arg\n            extended_arg = (arg << 8) if deop == EXTENDED_ARG else 0\n            # The oparg is stored as a signed integer\n            # If the value exceeds its upper limit, it will overflow and wrap\n            # to a negative integer\n            if extended_arg >= _INT_OVERFLOW:\n                extended_arg -= 2 * _INT_OVERFLOW\n        else:\n            arg = None\n            extended_arg = 0\n        yield (i, op, arg)\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    for offset, op, arg in _unpack_opargs(code):\n        if arg is not None:\n            deop = _deoptop(op)\n            caches = _inline_cache_entries[deop]\n            if deop in hasjrel:\n                if _is_backward_jump(deop):\n                    arg = -arg\n                label = offset + 2 + arg*2\n                label += 2 * caches\n            elif deop in hasjabs:\n                label = arg*2\n            else:\n                continue\n            if label not in labels:\n                labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno)\n    \"\"\"\n    lastline = None\n    for start, end, line in code.co_lines():\n        if line is not None and line != lastline:\n            lastline = line\n            yield start, line\n    return\n\ndef _find_imports(co):\n    \"\"\"Find import statements in the code\n\n    Generate triplets (name, level, fromlist) where\n    name is the imported module and level, fromlist are\n    the corresponding args to __import__.\n    \"\"\"\n    IMPORT_NAME = opmap['IMPORT_NAME']\n\n    consts = co.co_consts\n    names = co.co_names\n    opargs = [(op, arg) for _, op, arg in _unpack_opargs(co.co_code)\n                  if op != EXTENDED_ARG]\n    for i, (op, oparg) in enumerate(opargs):\n        if op == IMPORT_NAME and i >= 2:\n            from_op = opargs[i-1]\n            level_op = opargs[i-2]\n            if (from_op[0] in hasconst and level_op[0] in hasconst):\n                level = _get_const_value(level_op[0], level_op[1], consts)\n                fromlist = _get_const_value(from_op[0], from_op[1], consts)\n                yield (names[oparg], level, fromlist)\n\ndef _find_store_names(co):\n    \"\"\"Find names of variables which are written in the code\n\n    Generate sequence of strings\n    \"\"\"\n    STORE_OPS = {\n        opmap['STORE_NAME'],\n        opmap['STORE_GLOBAL']\n    }\n\n    names = co.co_names\n    for _, op, arg in _unpack_opargs(co.co_code):\n        if op in STORE_OPS:\n            yield names[arg]\n\n\nclass Bytecode:\n    \"\"\"The bytecode operations of a piece of code\n\n    Instantiate this with a function, method, other compiled object, string of\n    code, or a code object (as returned by compile()).\n\n    Iterating over this yields the bytecode operations as Instruction instances.\n    \"\"\"\n    def __init__(self, x, *, first_line=None, current_offset=None, show_caches=False, adaptive=False):\n        self.codeobj = co = _get_code_object(x)\n        if first_line is None:\n            self.first_line = co.co_firstlineno\n            self._line_offset = 0\n        else:\n            self.first_line = first_line\n            self._line_offset = first_line - co.co_firstlineno\n        self._linestarts = dict(findlinestarts(co))\n        self._original_object = x\n        self.current_offset = current_offset\n        self.exception_entries = _parse_exception_table(co)\n        self.show_caches = show_caches\n        self.adaptive = adaptive\n\n    def __iter__(self):\n        co = self.codeobj\n        return _get_instructions_bytes(_get_code_array(co, self.adaptive),\n                                       co._varname_from_oparg,\n                                       co.co_names, co.co_consts,\n                                       self._linestarts,\n                                       line_offset=self._line_offset,\n                                       exception_entries=self.exception_entries,\n                                       co_positions=co.co_positions(),\n                                       show_caches=self.show_caches)\n\n    def __repr__(self):\n        return \"{}({!r})\".format(self.__class__.__name__,\n                                 self._original_object)\n\n    @classmethod\n    def from_traceback(cls, tb, *, show_caches=False, adaptive=False):\n        \"\"\" Construct a Bytecode from the given traceback \"\"\"\n        while tb.tb_next:\n            tb = tb.tb_next\n        return cls(\n            tb.tb_frame.f_code, current_offset=tb.tb_lasti, show_caches=show_caches, adaptive=adaptive\n        )\n\n    def info(self):\n        \"\"\"Return formatted information about the code object.\"\"\"\n        return _format_code_info(self.codeobj)\n\n    def dis(self):\n        \"\"\"Return a formatted view of the bytecode operations.\"\"\"\n        co = self.codeobj\n        if self.current_offset is not None:\n            offset = self.current_offset\n        else:\n            offset = -1\n        with io.StringIO() as output:\n            _disassemble_bytes(_get_code_array(co, self.adaptive),\n                               varname_from_oparg=co._varname_from_oparg,\n                               names=co.co_names, co_consts=co.co_consts,\n                               linestarts=self._linestarts,\n                               line_offset=self._line_offset,\n                               file=output,\n                               lasti=offset,\n                               exception_entries=self.exception_entries,\n                               co_positions=co.co_positions(),\n                               show_caches=self.show_caches)\n            return output.getvalue()\n\n\ndef main(args=None):\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('infile', type=argparse.FileType('rb'), nargs='?', default='-')\n    args = parser.parse_args(args=args)\n    with args.infile as infile:\n        source = infile.read()\n    code = compile(source, args.infile.name, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    main()\n", 805], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\inspect.py": ["\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargvalues(), getcallargs() - get info about function arguments\n    getfullargspec() - same, with support for Python 3 features\n    formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\n    signature() - get a Signature object for the callable\n\n    get_annotations() - safely compute an object's annotations\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = ('Ka-Ping Yee <ping@lfw.org>',\n              'Yury Selivanov <yselivanov@sprymix.com>')\n\n__all__ = [\n    \"AGEN_CLOSED\",\n    \"AGEN_CREATED\",\n    \"AGEN_RUNNING\",\n    \"AGEN_SUSPENDED\",\n    \"ArgInfo\",\n    \"Arguments\",\n    \"Attribute\",\n    \"BlockFinder\",\n    \"BoundArguments\",\n    \"BufferFlags\",\n    \"CORO_CLOSED\",\n    \"CORO_CREATED\",\n    \"CORO_RUNNING\",\n    \"CORO_SUSPENDED\",\n    \"CO_ASYNC_GENERATOR\",\n    \"CO_COROUTINE\",\n    \"CO_GENERATOR\",\n    \"CO_ITERABLE_COROUTINE\",\n    \"CO_NESTED\",\n    \"CO_NEWLOCALS\",\n    \"CO_NOFREE\",\n    \"CO_OPTIMIZED\",\n    \"CO_VARARGS\",\n    \"CO_VARKEYWORDS\",\n    \"ClassFoundException\",\n    \"ClosureVars\",\n    \"EndOfBlock\",\n    \"FrameInfo\",\n    \"FullArgSpec\",\n    \"GEN_CLOSED\",\n    \"GEN_CREATED\",\n    \"GEN_RUNNING\",\n    \"GEN_SUSPENDED\",\n    \"Parameter\",\n    \"Signature\",\n    \"TPFLAGS_IS_ABSTRACT\",\n    \"Traceback\",\n    \"classify_class_attrs\",\n    \"cleandoc\",\n    \"currentframe\",\n    \"findsource\",\n    \"formatannotation\",\n    \"formatannotationrelativeto\",\n    \"formatargvalues\",\n    \"get_annotations\",\n    \"getabsfile\",\n    \"getargs\",\n    \"getargvalues\",\n    \"getasyncgenlocals\",\n    \"getasyncgenstate\",\n    \"getattr_static\",\n    \"getblock\",\n    \"getcallargs\",\n    \"getclasstree\",\n    \"getclosurevars\",\n    \"getcomments\",\n    \"getcoroutinelocals\",\n    \"getcoroutinestate\",\n    \"getdoc\",\n    \"getfile\",\n    \"getframeinfo\",\n    \"getfullargspec\",\n    \"getgeneratorlocals\",\n    \"getgeneratorstate\",\n    \"getinnerframes\",\n    \"getlineno\",\n    \"getmembers\",\n    \"getmembers_static\",\n    \"getmodule\",\n    \"getmodulename\",\n    \"getmro\",\n    \"getouterframes\",\n    \"getsource\",\n    \"getsourcefile\",\n    \"getsourcelines\",\n    \"indentsize\",\n    \"isabstract\",\n    \"isasyncgen\",\n    \"isasyncgenfunction\",\n    \"isawaitable\",\n    \"isbuiltin\",\n    \"isclass\",\n    \"iscode\",\n    \"iscoroutine\",\n    \"iscoroutinefunction\",\n    \"isdatadescriptor\",\n    \"isframe\",\n    \"isfunction\",\n    \"isgenerator\",\n    \"isgeneratorfunction\",\n    \"isgetsetdescriptor\",\n    \"ismemberdescriptor\",\n    \"ismethod\",\n    \"ismethoddescriptor\",\n    \"ismethodwrapper\",\n    \"ismodule\",\n    \"isroutine\",\n    \"istraceback\",\n    \"markcoroutinefunction\",\n    \"signature\",\n    \"stack\",\n    \"trace\",\n    \"unwrap\",\n    \"walktree\",\n]\n\n\nimport abc\nimport ast\nimport dis\nimport collections.abc\nimport enum\nimport importlib.machinery\nimport itertools\nimport linecache\nimport os\nimport re\nimport sys\nimport tokenize\nimport token\nimport types\nimport functools\nimport builtins\nfrom keyword import iskeyword\nfrom operator import attrgetter\nfrom collections import namedtuple, OrderedDict\nfrom weakref import ref as make_weakref\n\n# Create constants for the compiler flags in Include/code.h\n# We try to get them from dis to avoid duplication\nmod_dict = globals()\nfor k, v in dis.COMPILER_FLAG_NAMES.items():\n    mod_dict[\"CO_\" + v] = k\ndel k, v, mod_dict\n\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n\ndef get_annotations(obj, *, globals=None, locals=None, eval_str=False):\n    \"\"\"Compute the annotations dict for an object.\n\n    obj may be a callable, class, or module.\n    Passing in an object of any other type raises TypeError.\n\n    Returns a dict.  get_annotations() returns a new dict every time\n    it's called; calling it twice on the same object will return two\n    different but equivalent dicts.\n\n    This function handles several details for you:\n\n      * If eval_str is true, values of type str will\n        be un-stringized using eval().  This is intended\n        for use with stringized annotations\n        (\"from __future__ import annotations\").\n      * If obj doesn't have an annotations dict, returns an\n        empty dict.  (Functions and methods always have an\n        annotations dict; classes, modules, and other types of\n        callables may not.)\n      * Ignores inherited annotations on classes.  If a class\n        doesn't have its own annotations dict, returns an empty dict.\n      * All accesses to object members and dict values are done\n        using getattr() and dict.get() for safety.\n      * Always, always, always returns a freshly-created dict.\n\n    eval_str controls whether or not values of type str are replaced\n    with the result of calling eval() on those values:\n\n      * If eval_str is true, eval() is called on values of type str.\n      * If eval_str is false (the default), values of type str are unchanged.\n\n    globals and locals are passed in to eval(); see the documentation\n    for eval() for more information.  If either globals or locals is\n    None, this function may replace that value with a context-specific\n    default, contingent on type(obj):\n\n      * If obj is a module, globals defaults to obj.__dict__.\n      * If obj is a class, globals defaults to\n        sys.modules[obj.__module__].__dict__ and locals\n        defaults to the obj class namespace.\n      * If obj is a callable, globals defaults to obj.__globals__,\n        although if obj is a wrapped function (using\n        functools.update_wrapper()) it is first unwrapped.\n    \"\"\"\n    if isinstance(obj, type):\n        # class\n        obj_dict = getattr(obj, '__dict__', None)\n        if obj_dict and hasattr(obj_dict, 'get'):\n            ann = obj_dict.get('__annotations__', None)\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = None\n        else:\n            ann = None\n\n        obj_globals = None\n        module_name = getattr(obj, '__module__', None)\n        if module_name:\n            module = sys.modules.get(module_name, None)\n            if module:\n                obj_globals = getattr(module, '__dict__', None)\n        obj_locals = dict(vars(obj))\n        unwrap = obj\n    elif isinstance(obj, types.ModuleType):\n        # module\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__dict__')\n        obj_locals = None\n        unwrap = None\n    elif callable(obj):\n        # this includes types.Function, types.BuiltinFunctionType,\n        # types.BuiltinMethodType, functools.partial, functools.singledispatch,\n        # \"class funclike\" from Lib/test/test_inspect... on and on it goes.\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__globals__', None)\n        obj_locals = None\n        unwrap = obj\n    else:\n        raise TypeError(f\"{obj!r} is not a module, class, or callable.\")\n\n    if ann is None:\n        return {}\n\n    if not isinstance(ann, dict):\n        raise ValueError(f\"{obj!r}.__annotations__ is neither a dict nor None\")\n\n    if not ann:\n        return {}\n\n    if not eval_str:\n        return dict(ann)\n\n    if unwrap is not None:\n        while True:\n            if hasattr(unwrap, '__wrapped__'):\n                unwrap = unwrap.__wrapped__\n                continue\n            if isinstance(unwrap, functools.partial):\n                unwrap = unwrap.func\n                continue\n            break\n        if hasattr(unwrap, \"__globals__\"):\n            obj_globals = unwrap.__globals__\n\n    if globals is None:\n        globals = obj_globals\n    if locals is None:\n        locals = obj_locals or {}\n\n    # \"Inject\" type parameters into the local namespace\n    # (unless they are shadowed by assignments *in* the local namespace),\n    # as a way of emulating annotation scopes when calling `eval()`\n    if type_params := getattr(obj, \"__type_params__\", ()):\n        locals = {param.__name__: param for param in type_params} | locals\n\n    return_value = {key:\n        value if not isinstance(value, str) else eval(value, globals, locals)\n        for key, value in ann.items() }\n    return return_value\n\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\"\"\"\n    return isinstance(object, type)\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    __func__ attribute (etc) when an object passes ismethod().\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__get__\") and not hasattr(tp, \"__set__\")\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have a __set__ or a __delete__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__set__\") or hasattr(tp, \"__delete__\")\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        __code__        code object containing compiled function bytecode\n        __defaults__    tuple of any default values for arguments\n        __globals__     global namespace in which this function was defined\n        __annotations__ dict of parameter annotations\n        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef _has_code_flag(f, flag):\n    \"\"\"Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\"\"\"\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n        return False\n    return bool(f.__code__.co_flags & flag)\n\ndef isgeneratorfunction(obj):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provide the same attributes as functions.\n    See help(isfunction) for a list of attributes.\"\"\"\n    return _has_code_flag(obj, CO_GENERATOR)\n\n# A marker for markcoroutinefunction and iscoroutinefunction.\n_is_coroutine_mark = object()\n\ndef _has_coroutine_mark(f):\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    return getattr(f, \"_is_coroutine_marker\", None) is _is_coroutine_mark\n\ndef markcoroutinefunction(func):\n    \"\"\"\n    Decorator to ensure callable is recognised as a coroutine function.\n    \"\"\"\n    if hasattr(func, '__func__'):\n        func = func.__func__\n    func._is_coroutine_marker = _is_coroutine_mark\n    return func\n\ndef iscoroutinefunction(obj):\n    \"\"\"Return true if the object is a coroutine function.\n\n    Coroutine functions are normally defined with \"async def\" syntax, but may\n    be marked via markcoroutinefunction.\n    \"\"\"\n    return _has_code_flag(obj, CO_COROUTINE) or _has_coroutine_mark(obj)\n\ndef isasyncgenfunction(obj):\n    \"\"\"Return true if the object is an asynchronous generator function.\n\n    Asynchronous generator functions are defined with \"async def\"\n    syntax and have \"yield\" expressions in their body.\n    \"\"\"\n    return _has_code_flag(obj, CO_ASYNC_GENERATOR)\n\ndef isasyncgen(object):\n    \"\"\"Return true if the object is an asynchronous generator.\"\"\"\n    return isinstance(object, types.AsyncGeneratorType)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef iscoroutine(object):\n    \"\"\"Return true if the object is a coroutine.\"\"\"\n    return isinstance(object, types.CoroutineType)\n\ndef isawaitable(object):\n    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n    return (isinstance(object, types.CoroutineType) or\n            isinstance(object, types.GeneratorType) and\n                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n            isinstance(object, collections.abc.Awaitable))\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount         number of arguments (not including *, ** args\n                            or keyword only arguments)\n        co_code             string of raw compiled bytecode\n        co_cellvars         tuple of names of cell variables\n        co_consts           tuple of constants used in the bytecode\n        co_filename         name of file in which this code object was created\n        co_firstlineno      number of first line in Python source code\n        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n                            | 16=nested | 32=generator | 64=nofree | 128=coroutine\n                            | 256=iterable_coroutine | 512=async_generator\n        co_freevars         tuple of names of free variables\n        co_posonlyargcount  number of positional only arguments\n        co_kwonlyargcount   number of keyword only arguments (not including ** arg)\n        co_lnotab           encoded mapping of line numbers to bytecode indices\n        co_name             name with which this code object was defined\n        co_names            tuple of names other than arguments and function locals\n        co_nlocals          number of local variables\n        co_stacksize        virtual machine stack space required\n        co_varnames         tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef ismethodwrapper(object):\n    \"\"\"Return true if the object is a method wrapper.\"\"\"\n    return isinstance(object, types.MethodWrapperType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object)\n            or ismethodwrapper(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    if not isinstance(object, type):\n        return False\n    if object.__flags__ & TPFLAGS_IS_ABSTRACT:\n        return True\n    if not issubclass(type(object), abc.ABCMeta):\n        return False\n    if hasattr(object, '__abstractmethods__'):\n        # It looks like ABCMeta.__new__ has finished running;\n        # TPFLAGS_IS_ABSTRACT should have been accurate.\n        return False\n    # It looks like ABCMeta.__new__ has not finished running yet; we're\n    # probably in __init_subclass__. We'll look for abstractmethods manually.\n    for name, value in object.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            return True\n    for base in object.__bases__:\n        for name in getattr(base, \"__abstractmethods__\", ()):\n            value = getattr(object, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                return True\n    return False\n\ndef _getmembers(object, predicate, getter):\n    results = []\n    processed = set()\n    names = dir(object)\n    if isclass(object):\n        mro = getmro(object)\n        # add any DynamicClassAttributes to the list of names if object is a class;\n        # this may result in duplicate entries if, for example, a virtual\n        # attribute with the same name as a DynamicClassAttribute exists\n        try:\n            for base in object.__bases__:\n                for k, v in base.__dict__.items():\n                    if isinstance(v, types.DynamicClassAttribute):\n                        names.append(k)\n        except AttributeError:\n            pass\n    else:\n        mro = ()\n    for key in names:\n        # First try to get the value via getattr.  Some descriptors don't\n        # like calling their __get__ (see bug #1785), so fall back to\n        # looking in the __dict__.\n        try:\n            value = getter(object, key)\n            # handle the duplicate key\n            if key in processed:\n                raise AttributeError\n        except AttributeError:\n            for base in mro:\n                if key in base.__dict__:\n                    value = base.__dict__[key]\n                    break\n            else:\n                # could be a (currently) missing slot member, or a buggy\n                # __dir__; discard and move on\n                continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n        processed.add(key)\n    results.sort(key=lambda pair: pair[0])\n    return results\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    return _getmembers(object, predicate, getattr)\n\ndef getmembers_static(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name\n    without triggering dynamic lookup via the descriptor protocol,\n    __getattr__ or __getattribute__. Optionally, only return members that\n    satisfy a given predicate.\n\n    Note: this function may not be able to retrieve all members\n       that getmembers can fetch (like dynamically created attributes)\n       and may find members that getmembers can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases.\n    \"\"\"\n    return _getmembers(object, predicate, getattr_static)\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method or descriptor\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained by calling getattr; if this fails, or if the\n           resulting object does not live anywhere in the class' mro (including\n           metaclasses) then the object is looked up in the defining class's\n           dict (found by walking the mro).\n\n    If one of the items in dir(cls) is stored in the metaclass it will now\n    be discovered and not have None be listed as the class in which it was\n    defined.  Any items whose home class cannot be discovered are skipped.\n    \"\"\"\n\n    mro = getmro(cls)\n    metamro = getmro(type(cls)) # for attributes stored in the metaclass\n    metamro = tuple(cls for cls in metamro if cls not in (type, object))\n    class_bases = (cls,) + mro\n    all_bases = class_bases + metamro\n    names = dir(cls)\n    # :dd any DynamicClassAttributes to the list of names;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists.\n    for base in mro:\n        for k, v in base.__dict__.items():\n            if isinstance(v, types.DynamicClassAttribute) and v.fget is not None:\n                names.append(k)\n    result = []\n    processed = set()\n\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Normal objects will be looked up with both getattr and directly in\n        # its class' dict (in case getattr fails [bug #1785], and also to look\n        # for a docstring).\n        # For DynamicClassAttributes on the second pass we only look in the\n        # class's dict.\n        #\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        homecls = None\n        get_obj = None\n        dict_obj = None\n        if name not in processed:\n            try:\n                if name == '__dict__':\n                    raise Exception(\"__dict__ is special, don't want the proxy\")\n                get_obj = getattr(cls, name)\n            except Exception:\n                pass\n            else:\n                homecls = getattr(get_obj, \"__objclass__\", homecls)\n                if homecls not in class_bases:\n                    # if the resulting object does not live somewhere in the\n                    # mro, drop it and search the mro manually\n                    homecls = None\n                    last_cls = None\n                    # first look in the classes\n                    for srch_cls in class_bases:\n                        srch_obj = getattr(srch_cls, name, None)\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    # then check the metaclasses\n                    for srch_cls in metamro:\n                        try:\n                            srch_obj = srch_cls.__getattr__(cls, name)\n                        except AttributeError:\n                            continue\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    if last_cls is not None:\n                        homecls = last_cls\n        for base in all_bases:\n            if name in base.__dict__:\n                dict_obj = base.__dict__[name]\n                if homecls not in metamro:\n                    homecls = base\n                break\n        if homecls is None:\n            # unable to locate the attribute anywhere, most likely due to\n            # buggy custom __dir__; discard and move on\n            continue\n        obj = get_obj if get_obj is not None else dict_obj\n        # Classify the object or its descriptor.\n        if isinstance(dict_obj, (staticmethod, types.BuiltinMethodType)):\n            kind = \"static method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, (classmethod, types.ClassMethodDescriptorType)):\n            kind = \"class method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, property):\n            kind = \"property\"\n            obj = dict_obj\n        elif isroutine(obj):\n            kind = \"method\"\n        else:\n            kind = \"data\"\n        result.append(Attribute(name, kind, homecls, obj))\n        processed.add(name)\n    return result\n\n# ----------------------------------------------------------- class helpers\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    return cls.__mro__\n\n# -------------------------------------------------------- function helpers\n\ndef unwrap(func, *, stop=None):\n    \"\"\"Get the object wrapped by *func*.\n\n   Follows the chain of :attr:`__wrapped__` attributes returning the last\n   object in the chain.\n\n   *stop* is an optional callback accepting an object in the wrapper chain\n   as its sole argument that allows the unwrapping to be terminated early if\n   the callback returns a true value. If the callback never returns a true\n   value, the last object in the chain is returned as usual. For example,\n   :func:`signature` uses this to stop unwrapping if any object in the\n   chain has a ``__signature__`` attribute defined.\n\n   :exc:`ValueError` is raised if a cycle is encountered.\n\n    \"\"\"\n    f = func  # remember the original func for error reporting\n    # Memoise by id to tolerate non-hashable objects, but store objects to\n    # ensure they aren't destroyed, which would allow their IDs to be reused.\n    memo = {id(f): f}\n    recursion_limit = sys.getrecursionlimit()\n    while not isinstance(func, type) and hasattr(func, '__wrapped__'):\n        if stop is not None and stop(func):\n            break\n        func = func.__wrapped__\n        id_func = id(func)\n        if (id_func in memo) or (len(memo) >= recursion_limit):\n            raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n        memo[id_func] = func\n    return func\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = line.expandtabs()\n    return len(expline) - len(expline.lstrip())\n\ndef _findclass(func):\n    cls = sys.modules.get(func.__module__)\n    if cls is None:\n        return None\n    for name in func.__qualname__.split('.')[:-1]:\n        cls = getattr(cls, name)\n    if not isclass(cls):\n        return None\n    return cls\n\ndef _finddoc(obj):\n    if isclass(obj):\n        for base in obj.__mro__:\n            if base is not object:\n                try:\n                    doc = base.__doc__\n                except AttributeError:\n                    continue\n                if doc is not None:\n                    return doc\n        return None\n\n    if ismethod(obj):\n        name = obj.__func__.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            getattr(getattr(self, name, None), '__func__') is obj.__func__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    elif isfunction(obj):\n        name = obj.__name__\n        cls = _findclass(obj)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif isbuiltin(obj):\n        name = obj.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            self.__qualname__ + '.' + name == obj.__qualname__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    # Should be tested before isdatadescriptor().\n    elif isinstance(obj, property):\n        func = obj.fget\n        name = func.__name__\n        cls = _findclass(func)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif ismethoddescriptor(obj) or isdatadescriptor(obj):\n        name = obj.__name__\n        cls = obj.__objclass__\n        if getattr(cls, name) is not obj:\n            return None\n        if ismemberdescriptor(obj):\n            slots = getattr(cls, '__slots__', None)\n            if isinstance(slots, dict) and name in slots:\n                return slots[name]\n    else:\n        return None\n    for base in cls.__mro__:\n        try:\n            doc = getattr(base, name).__doc__\n        except AttributeError:\n            continue\n        if doc is not None:\n            return doc\n    return None\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if doc is None:\n        try:\n            doc = _finddoc(object)\n        except (AttributeError, TypeError):\n            return None\n    if not isinstance(doc, str):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = doc.expandtabs().split('\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxsize\n        for line in lines[1:]:\n            content = len(line.lstrip())\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxsize:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return '\\n'.join(lines)\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if getattr(object, '__file__', None):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        if hasattr(object, '__module__'):\n            module = sys.modules.get(object.__module__)\n            if getattr(module, '__file__', None):\n                return module.__file__\n            if object.__module__ == '__main__':\n                raise OSError('source code not available')\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('module, class, method, function, traceback, frame, or '\n                    'code object was expected, got {}'.format(\n                    type(object).__name__))\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    fname = os.path.basename(path)\n    # Check for paths that look like an actual module file\n    suffixes = [(-len(suffix), suffix)\n                    for suffix in importlib.machinery.all_suffixes()]\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix in suffixes:\n        if fname.endswith(suffix):\n            return fname[:neglen]\n    return None\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n        filename = (os.path.splitext(filename)[0] +\n                    importlib.machinery.SOURCE_SUFFIXES[0])\n    elif any(filename.endswith(s) for s in\n                 importlib.machinery.EXTENSION_SUFFIXES):\n        return None\n    # return a filename found in the linecache even if it doesn't exist on disk\n    if filename in linecache.cache:\n        return filename\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    module = getmodule(object, filename)\n    if getattr(module, '__loader__', None) is not None:\n        return filename\n    elif getattr(getattr(module, \"__spec__\", None), \"loader\", None) is not None:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except (TypeError, FileNotFoundError):\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.copy().items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['builtins']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\n\nclass ClassFoundException(Exception):\n    pass\n\n\nclass _ClassFinder(ast.NodeVisitor):\n\n    def __init__(self, qualname):\n        self.stack = []\n        self.qualname = qualname\n\n    def visit_FunctionDef(self, node):\n        self.stack.append(node.name)\n        self.stack.append('<locals>')\n        self.generic_visit(node)\n        self.stack.pop()\n        self.stack.pop()\n\n    visit_AsyncFunctionDef = visit_FunctionDef\n\n    def visit_ClassDef(self, node):\n        self.stack.append(node.name)\n        if self.qualname == '.'.join(self.stack):\n            # Return the decorator for the class if present\n            if node.decorator_list:\n                line_number = node.decorator_list[0].lineno\n            else:\n                line_number = node.lineno\n\n            # decrement by one since lines starts with indexing by zero\n            line_number -= 1\n            raise ClassFoundException(line_number)\n        self.generic_visit(node)\n        self.stack.pop()\n\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An OSError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getsourcefile(object)\n    if file:\n        # Invalidate cache if needed.\n        linecache.checkcache(file)\n    else:\n        file = getfile(object)\n        # Allow filenames in form of \"<something>\" to pass through.\n        # `doctest` monkeypatches `linecache` module to enable\n        # inspection, so let `linecache.getlines` to be called.\n        if not (file.startswith('<') and file.endswith('>')):\n            raise OSError('source code not available')\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise OSError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        qualname = object.__qualname__\n        source = ''.join(lines)\n        tree = ast.parse(source)\n        class_finder = _ClassFinder(qualname)\n        try:\n            class_finder.visit(tree)\n        except ClassFoundException as e:\n            line_number = e.args[0]\n            return lines, line_number\n        else:\n            raise OSError('could not find class definition')\n\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise OSError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            try:\n                line = lines[lnum]\n            except IndexError:\n                raise OSError('lineno is out of bounds')\n            if pat.match(line):\n                break\n            lnum = lnum - 1\n        return lines, lnum\n    raise OSError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (OSError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and lines[start].strip() in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(lines[end].expandtabs())\n                end = end + 1\n            return ''.join(comments)\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and lines[end].lstrip()[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [lines[end].expandtabs().lstrip()]\n            if end > 0:\n                end = end - 1\n                comment = lines[end].expandtabs().lstrip()\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = lines[end].expandtabs().lstrip()\n            while comments and comments[0].strip() == '#':\n                comments[:1] = []\n            while comments and comments[-1].strip() == '#':\n                comments[-1:] = []\n            return ''.join(comments)\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.indecorator = False\n        self.last = 1\n        self.body_col0 = None\n\n    def tokeneater(self, type, token, srowcol, erowcol, line):\n        if not self.started and not self.indecorator:\n            # skip any decorators\n            if token == \"@\":\n                self.indecorator = True\n            # look for the first \"def\", \"class\" or \"lambda\"\n            elif token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srowcol[0]\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n            # hitting a NEWLINE when in a decorator without args\n            # ends the decorator\n            if self.indecorator:\n                self.indecorator = False\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            if self.body_col0 is None and self.started:\n                self.body_col0 = erowcol[1]\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif type == tokenize.COMMENT:\n            if self.body_col0 is not None and srowcol[1] >= self.body_col0:\n                # Include comments if indented at least as much as the block\n                self.last = srowcol[0]\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokens = tokenize.generate_tokens(iter(lines).__next__)\n        for _token in tokens:\n            blockfinder.tokeneater(*_token)\n    except (EndOfBlock, IndentationError):\n        pass\n    except SyntaxError as e:\n        if \"unmatched\" not in e.msg:\n            raise e from None\n        _, *_token_info = _token\n        try:\n            blockfinder.tokeneater(tokenize.NEWLINE, *_token_info)\n        except (EndOfBlock, IndentationError):\n            pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An OSError is\n    raised if the source code cannot be retrieved.\"\"\"\n    object = unwrap(object)\n    lines, lnum = findsource(object)\n\n    if istraceback(object):\n        object = object.tb_frame\n\n    # for module or frame that corresponds to module, return all source lines\n    if (ismodule(object) or\n        (isframe(object) and object.f_code.co_name == \"<module>\")):\n        return lines, 0\n    else:\n        return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    OSError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return ''.join(lines)\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=False):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if parent not in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args, varargs, varkw')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where\n    'args' is the list of argument names. Keyword-only arguments are\n    appended. 'varargs' and 'varkw' are the names of the * and **\n    arguments or None.\"\"\"\n    if not iscode(co):\n        raise TypeError('{!r} is not a code object'.format(co))\n\n    names = co.co_varnames\n    nargs = co.co_argcount\n    nkwargs = co.co_kwonlyargcount\n    args = list(names[:nargs])\n    kwonlyargs = list(names[nargs:nargs+nkwargs])\n\n    nargs += nkwargs\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args + kwonlyargs, varargs, varkw)\n\n\nFullArgSpec = namedtuple('FullArgSpec',\n    'args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations')\n\ndef getfullargspec(func):\n    \"\"\"Get the names and default values of a callable object's parameters.\n\n    A tuple of seven things is returned:\n    (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).\n    'args' is a list of the parameter names.\n    'varargs' and 'varkw' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n    'kwonlyargs' is a list of keyword-only parameter names.\n    'kwonlydefaults' is a dictionary mapping names from kwonlyargs to defaults.\n    'annotations' is a dictionary mapping parameter names to annotations.\n\n    Notable differences from inspect.signature():\n      - the \"self\" parameter is always reported, even for bound methods\n      - wrapper chains defined by __wrapped__ *not* unwrapped automatically\n    \"\"\"\n    try:\n        # Re: `skip_bound_arg=False`\n        #\n        # There is a notable difference in behaviour between getfullargspec\n        # and Signature: the former always returns 'self' parameter for bound\n        # methods, whereas the Signature always shows the actual calling\n        # signature of the passed object.\n        #\n        # To simulate this behaviour, we \"unbind\" bound methods, to trick\n        # inspect.signature to always return their first parameter (\"self\",\n        # usually)\n\n        # Re: `follow_wrapper_chains=False`\n        #\n        # getfullargspec() historically ignored __wrapped__ attributes,\n        # so we ensure that remains the case in 3.3+\n\n        sig = _signature_from_callable(func,\n                                       follow_wrapper_chains=False,\n                                       skip_bound_arg=False,\n                                       sigcls=Signature,\n                                       eval_str=False)\n    except Exception as ex:\n        # Most of the times 'signature' will raise ValueError.\n        # But, it can also raise AttributeError, and, maybe something\n        # else. So to be fully backwards compatible, we catch all\n        # possible exceptions here, and reraise a TypeError.\n        raise TypeError('unsupported callable') from ex\n\n    args = []\n    varargs = None\n    varkw = None\n    posonlyargs = []\n    kwonlyargs = []\n    annotations = {}\n    defaults = ()\n    kwdefaults = {}\n\n    if sig.return_annotation is not sig.empty:\n        annotations['return'] = sig.return_annotation\n\n    for param in sig.parameters.values():\n        kind = param.kind\n        name = param.name\n\n        if kind is _POSITIONAL_ONLY:\n            posonlyargs.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _POSITIONAL_OR_KEYWORD:\n            args.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _VAR_POSITIONAL:\n            varargs = name\n        elif kind is _KEYWORD_ONLY:\n            kwonlyargs.append(name)\n            if param.default is not param.empty:\n                kwdefaults[name] = param.default\n        elif kind is _VAR_KEYWORD:\n            varkw = name\n\n        if param.annotation is not param.empty:\n            annotations[name] = param.annotation\n\n    if not kwdefaults:\n        # compatibility with 'func.__kwdefaults__'\n        kwdefaults = None\n\n    if not defaults:\n        # compatibility with 'func.__defaults__'\n        defaults = None\n\n    return FullArgSpec(posonlyargs + args, varargs, varkw, defaults,\n                       kwonlyargs, kwdefaults, annotations)\n\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names.\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef formatannotation(annotation, base_module=None):\n    if getattr(annotation, '__module__', None) == 'typing':\n        def repl(match):\n            text = match.group()\n            return text.removeprefix('typing.')\n        return re.sub(r'[\\w\\.]+', repl, repr(annotation))\n    if isinstance(annotation, types.GenericAlias):\n        return str(annotation)\n    if isinstance(annotation, type):\n        if annotation.__module__ in ('builtins', base_module):\n            return annotation.__qualname__\n        return annotation.__module__+'.'+annotation.__qualname__\n    return repr(annotation)\n\ndef formatannotationrelativeto(object):\n    module = getattr(object, '__module__', None)\n    def _formatannotation(annotation):\n        return formatannotation(annotation, module)\n    return _formatannotation\n\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value)):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(convert(args[i]))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + ', '.join(specs) + ')'\n\ndef _missing_arguments(f_name, argnames, pos, values):\n    names = [repr(name) for name in argnames if name not in values]\n    missing = len(names)\n    if missing == 1:\n        s = names[0]\n    elif missing == 2:\n        s = \"{} and {}\".format(*names)\n    else:\n        tail = \", {} and {}\".format(*names[-2:])\n        del names[-2:]\n        s = \", \".join(names) + tail\n    raise TypeError(\"%s() missing %i required %s argument%s: %s\" %\n                    (f_name, missing,\n                      \"positional\" if pos else \"keyword-only\",\n                      \"\" if missing == 1 else \"s\", s))\n\ndef _too_many(f_name, args, kwonly, varargs, defcount, given, values):\n    atleast = len(args) - defcount\n    kwonly_given = len([arg for arg in kwonly if arg in values])\n    if varargs:\n        plural = atleast != 1\n        sig = \"at least %d\" % (atleast,)\n    elif defcount:\n        plural = True\n        sig = \"from %d to %d\" % (atleast, len(args))\n    else:\n        plural = len(args) != 1\n        sig = str(len(args))\n    kwonly_sig = \"\"\n    if kwonly_given:\n        msg = \" positional argument%s (and %d keyword-only argument%s)\"\n        kwonly_sig = (msg % (\"s\" if given != 1 else \"\", kwonly_given,\n                             \"s\" if kwonly_given != 1 else \"\"))\n    raise TypeError(\"%s() takes %s positional argument%s but %d%s %s given\" %\n            (f_name, sig, \"s\" if plural else \"\", given, kwonly_sig,\n             \"was\" if given == 1 and not kwonly_given else \"were\"))\n\ndef getcallargs(func, /, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    spec = getfullargspec(func)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = spec\n    f_name = func.__name__\n    arg2value = {}\n\n\n    if ismethod(func) and func.__self__ is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.__self__,) + positional\n    num_pos = len(positional)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n\n    n = min(num_pos, num_args)\n    for i in range(n):\n        arg2value[args[i]] = positional[i]\n    if varargs:\n        arg2value[varargs] = tuple(positional[n:])\n    possible_kwargs = set(args + kwonlyargs)\n    if varkw:\n        arg2value[varkw] = {}\n    for kw, value in named.items():\n        if kw not in possible_kwargs:\n            if not varkw:\n                raise TypeError(\"%s() got an unexpected keyword argument %r\" %\n                                (f_name, kw))\n            arg2value[varkw][kw] = value\n            continue\n        if kw in arg2value:\n            raise TypeError(\"%s() got multiple values for argument %r\" %\n                            (f_name, kw))\n        arg2value[kw] = value\n    if num_pos > num_args and not varargs:\n        _too_many(f_name, args, kwonlyargs, varargs, num_defaults,\n                   num_pos, arg2value)\n    if num_pos < num_args:\n        req = args[:num_args - num_defaults]\n        for arg in req:\n            if arg not in arg2value:\n                _missing_arguments(f_name, req, True, arg2value)\n        for i, arg in enumerate(args[num_args - num_defaults:]):\n            if arg not in arg2value:\n                arg2value[arg] = defaults[i]\n    missing = 0\n    for kwarg in kwonlyargs:\n        if kwarg not in arg2value:\n            if kwonlydefaults and kwarg in kwonlydefaults:\n                arg2value[kwarg] = kwonlydefaults[kwarg]\n            else:\n                missing += 1\n    if missing:\n        _missing_arguments(f_name, kwonlyargs, False, arg2value)\n    return arg2value\n\nClosureVars = namedtuple('ClosureVars', 'nonlocals globals builtins unbound')\n\ndef getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.__func__\n\n    if not isfunction(func):\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n\n    code = func.__code__\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if func.__closure__ is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {\n            var : cell.cell_contents\n            for var, cell in zip(code.co_freevars, func.__closure__)\n       }\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = func.__globals__\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    global_names = set()\n    for instruction in dis.get_instructions(code):\n        opname = instruction.opname\n        name = instruction.argval\n        if opname == \"LOAD_ATTR\":\n            unbound_names.add(name)\n        elif opname == \"LOAD_GLOBAL\":\n            global_names.add(name)\n    for name in global_names:\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return ClosureVars(nonlocal_vars, global_vars,\n                       builtin_vars, unbound_names)\n\n# -------------------------------------------------- stack frame extraction\n\n_Traceback = namedtuple('_Traceback', 'filename lineno function code_context index')\n\nclass Traceback(_Traceback):\n    def __new__(cls, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('Traceback(filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.filename, self.lineno, self.function, self.code_context,\n                self.index, self.positions))\n\ndef _get_code_position_from_tb(tb):\n    code, instruction_index = tb.tb_frame.f_code, tb.tb_lasti\n    return _get_code_position(code, instruction_index)\n\ndef _get_code_position(code, instruction_index):\n    if instruction_index < 0:\n        return (None, None, None, None)\n    positions_gen = code.co_positions()\n    # The nth entry in code.co_positions() corresponds to instruction (2*n)th since Python 3.10+\n    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        positions = _get_code_position_from_tb(frame)\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n        positions = _get_code_position(frame.f_code, frame.f_lasti)\n\n    if positions[0] is None:\n        frame, *positions = (frame, lineno, *positions[1:])\n    else:\n        frame, *positions = (frame, *positions)\n\n    lineno = positions[0]\n\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except OSError:\n            lines = index = None\n        else:\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines,\n                     index, positions=dis.Positions(*positions))\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\n_FrameInfo = namedtuple('_FrameInfo', ('frame',) + Traceback._fields)\nclass FrameInfo(_FrameInfo):\n    def __new__(cls, frame, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, frame, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('FrameInfo(frame={!r}, filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.frame, self.filename, self.lineno, self.function,\n                self.code_context, self.index, self.positions))\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        traceback_info = getframeinfo(frame, context)\n        frameinfo = (frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        traceback_info = getframeinfo(tb, context)\n        frameinfo = (tb.tb_frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        tb = tb.tb_next\n    return framelist\n\ndef currentframe():\n    \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n    return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    exc = sys.exception()\n    tb = None if exc is None else exc.__traceback__\n    return getinnerframes(tb, context)\n\n\n# ------------------------------------------------ static version of getattr\n\n_sentinel = object()\n_static_getmro = type.__dict__['__mro__'].__get__\n_get_dunder_dict_of_class = type.__dict__[\"__dict__\"].__get__\n\n\ndef _check_instance(obj, attr):\n    instance_dict = {}\n    try:\n        instance_dict = object.__getattribute__(obj, \"__dict__\")\n    except AttributeError:\n        pass\n    return dict.get(instance_dict, attr, _sentinel)\n\n\ndef _check_class(klass, attr):\n    for entry in _static_getmro(klass):\n        if _shadowed_dict(type(entry)) is _sentinel and attr in entry.__dict__:\n            return entry.__dict__[attr]\n    return _sentinel\n\n\n@functools.lru_cache()\ndef _shadowed_dict_from_weakref_mro_tuple(*weakref_mro):\n    for weakref_entry in weakref_mro:\n        # Normally we'd have to check whether the result of weakref_entry()\n        # is None here, in case the object the weakref is pointing to has died.\n        # In this specific case, however, we know that the only caller of this\n        # function is `_shadowed_dict()`, and that therefore this weakref is\n        # guaranteed to point to an object that is still alive.\n        entry = weakref_entry()\n        dunder_dict = _get_dunder_dict_of_class(entry)\n        if '__dict__' in dunder_dict:\n            class_dict = dunder_dict['__dict__']\n            if not (type(class_dict) is types.GetSetDescriptorType and\n                    class_dict.__name__ == \"__dict__\" and\n                    class_dict.__objclass__ is entry):\n                return class_dict\n    return _sentinel\n\n\ndef _shadowed_dict(klass):\n    # gh-118013: the inner function here is decorated with lru_cache for\n    # performance reasons, *but* make sure not to pass strong references\n    # to the items in the mro. Doing so can lead to unexpected memory\n    # consumption in cases where classes are dynamically created and\n    # destroyed, and the dynamically created classes happen to be the only\n    # objects that hold strong references to other objects that take up a\n    # significant amount of memory.\n    return _shadowed_dict_from_weakref_mro_tuple(\n        *[make_weakref(entry) for entry in _static_getmro(klass)]\n    )\n\n\ndef getattr_static(obj, attr, default=_sentinel):\n    \"\"\"Retrieve attributes without triggering dynamic lookup via the\n       descriptor protocol,  __getattr__ or __getattribute__.\n\n       Note: this function may not be able to retrieve all attributes\n       that getattr can fetch (like dynamically created attributes)\n       and may find attributes that getattr can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases. See the\n       documentation for details.\n    \"\"\"\n    instance_result = _sentinel\n\n    objtype = type(obj)\n    if type not in _static_getmro(objtype):\n        klass = objtype\n        dict_attr = _shadowed_dict(klass)\n        if (dict_attr is _sentinel or\n            type(dict_attr) is types.MemberDescriptorType):\n            instance_result = _check_instance(obj, attr)\n    else:\n        klass = obj\n\n    klass_result = _check_class(klass, attr)\n\n    if instance_result is not _sentinel and klass_result is not _sentinel:\n        if _check_class(type(klass_result), \"__get__\") is not _sentinel and (\n            _check_class(type(klass_result), \"__set__\") is not _sentinel\n            or _check_class(type(klass_result), \"__delete__\") is not _sentinel\n        ):\n            return klass_result\n\n    if instance_result is not _sentinel:\n        return instance_result\n    if klass_result is not _sentinel:\n        return klass_result\n\n    if obj is klass:\n        # for types we check the metaclass too\n        for entry in _static_getmro(type(klass)):\n            if (\n                _shadowed_dict(type(entry)) is _sentinel\n                and attr in entry.__dict__\n            ):\n                return entry.__dict__[attr]\n    if default is not _sentinel:\n        return default\n    raise AttributeError(attr)\n\n\n# ------------------------------------------------ generator introspection\n\nGEN_CREATED = 'GEN_CREATED'\nGEN_RUNNING = 'GEN_RUNNING'\nGEN_SUSPENDED = 'GEN_SUSPENDED'\nGEN_CLOSED = 'GEN_CLOSED'\n\ndef getgeneratorstate(generator):\n    \"\"\"Get current state of a generator-iterator.\n\n    Possible states are:\n      GEN_CREATED: Waiting to start execution.\n      GEN_RUNNING: Currently being executed by the interpreter.\n      GEN_SUSPENDED: Currently suspended at a yield expression.\n      GEN_CLOSED: Execution has completed.\n    \"\"\"\n    if generator.gi_running:\n        return GEN_RUNNING\n    if generator.gi_suspended:\n        return GEN_SUSPENDED\n    if generator.gi_frame is None:\n        return GEN_CLOSED\n    return GEN_CREATED\n\n\ndef getgeneratorlocals(generator):\n    \"\"\"\n    Get the mapping of generator local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isgenerator(generator):\n        raise TypeError(\"{!r} is not a Python generator\".format(generator))\n\n    frame = getattr(generator, \"gi_frame\", None)\n    if frame is not None:\n        return generator.gi_frame.f_locals\n    else:\n        return {}\n\n\n# ------------------------------------------------ coroutine introspection\n\nCORO_CREATED = 'CORO_CREATED'\nCORO_RUNNING = 'CORO_RUNNING'\nCORO_SUSPENDED = 'CORO_SUSPENDED'\nCORO_CLOSED = 'CORO_CLOSED'\n\ndef getcoroutinestate(coroutine):\n    \"\"\"Get current state of a coroutine object.\n\n    Possible states are:\n      CORO_CREATED: Waiting to start execution.\n      CORO_RUNNING: Currently being executed by the interpreter.\n      CORO_SUSPENDED: Currently suspended at an await expression.\n      CORO_CLOSED: Execution has completed.\n    \"\"\"\n    if coroutine.cr_running:\n        return CORO_RUNNING\n    if coroutine.cr_suspended:\n        return CORO_SUSPENDED\n    if coroutine.cr_frame is None:\n        return CORO_CLOSED\n    return CORO_CREATED\n\n\ndef getcoroutinelocals(coroutine):\n    \"\"\"\n    Get the mapping of coroutine local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n    frame = getattr(coroutine, \"cr_frame\", None)\n    if frame is not None:\n        return frame.f_locals\n    else:\n        return {}\n\n\n# ----------------------------------- asynchronous generator introspection\n\nAGEN_CREATED = 'AGEN_CREATED'\nAGEN_RUNNING = 'AGEN_RUNNING'\nAGEN_SUSPENDED = 'AGEN_SUSPENDED'\nAGEN_CLOSED = 'AGEN_CLOSED'\n\n\ndef getasyncgenstate(agen):\n    \"\"\"Get current state of an asynchronous generator object.\n\n    Possible states are:\n      AGEN_CREATED: Waiting to start execution.\n      AGEN_RUNNING: Currently being executed by the interpreter.\n      AGEN_SUSPENDED: Currently suspended at a yield expression.\n      AGEN_CLOSED: Execution has completed.\n    \"\"\"\n    if agen.ag_running:\n        return AGEN_RUNNING\n    if agen.ag_suspended:\n        return AGEN_SUSPENDED\n    if agen.ag_frame is None:\n        return AGEN_CLOSED\n    return AGEN_CREATED\n\n\ndef getasyncgenlocals(agen):\n    \"\"\"\n    Get the mapping of asynchronous generator local variables to their current\n    values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isasyncgen(agen):\n        raise TypeError(f\"{agen!r} is not a Python async generator\")\n\n    frame = getattr(agen, \"ag_frame\", None)\n    if frame is not None:\n        return agen.ag_frame.f_locals\n    else:\n        return {}\n\n\n###############################################################################\n### Function Signature Object (PEP 362)\n###############################################################################\n\n\n_NonUserDefinedCallables = (types.WrapperDescriptorType,\n                            types.MethodWrapperType,\n                            types.ClassMethodDescriptorType,\n                            types.BuiltinFunctionType)\n\n\ndef _signature_get_user_defined_method(cls, method_name):\n    \"\"\"Private helper. Checks if ``cls`` has an attribute\n    named ``method_name`` and returns it only if it is a\n    pure python function.\n    \"\"\"\n    if method_name == '__new__':\n        meth = getattr(cls, method_name, None)\n    else:\n        meth = getattr_static(cls, method_name, None)\n    if meth is None or isinstance(meth, _NonUserDefinedCallables):\n        # Once '__signature__' will be added to 'C'-level\n        # callables, this check won't be necessary\n        return None\n    if method_name != '__new__':\n        meth = _descriptor_get(meth, cls)\n    return meth\n\n\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n\n    old_params = wrapped_sig.parameters\n    new_params = OrderedDict(old_params.items())\n\n    partial_args = partial.args or ()\n    partial_keywords = partial.keywords or {}\n\n    if extra_args:\n        partial_args = extra_args + partial_args\n\n    try:\n        ba = wrapped_sig.bind_partial(*partial_args, **partial_keywords)\n    except TypeError as ex:\n        msg = 'partial object {!r} has incorrect arguments'.format(partial)\n        raise ValueError(msg) from ex\n\n\n    transform_to_kwonly = False\n    for param_name, param in old_params.items():\n        try:\n            arg_value = ba.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if param.kind is _POSITIONAL_ONLY:\n                # If positional-only parameter is bound by partial,\n                # it effectively disappears from the signature\n                new_params.pop(param_name)\n                continue\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                if param_name in partial_keywords:\n                    # This means that this parameter, and all parameters\n                    # after it should be keyword-only (and var-positional\n                    # should be removed). Here's why. Consider the following\n                    # function:\n                    #     foo(a, b, *args, c):\n                    #         pass\n                    #\n                    # \"partial(foo, a='spam')\" will have the following\n                    # signature: \"(*, a='spam', b, c)\". Because attempting\n                    # to call that partial with \"(10, 20)\" arguments will\n                    # raise a TypeError, saying that \"a\" argument received\n                    # multiple values.\n                    transform_to_kwonly = True\n                    # Set the new default value\n                    new_params[param_name] = param.replace(default=arg_value)\n                else:\n                    # was passed as a positional argument\n                    new_params.pop(param.name)\n                    continue\n\n            if param.kind is _KEYWORD_ONLY:\n                # Set the new default value\n                new_params[param_name] = param.replace(default=arg_value)\n\n        if transform_to_kwonly:\n            assert param.kind is not _POSITIONAL_ONLY\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                new_param = new_params[param_name].replace(kind=_KEYWORD_ONLY)\n                new_params[param_name] = new_param\n                new_params.move_to_end(param_name)\n            elif param.kind in (_KEYWORD_ONLY, _VAR_KEYWORD):\n                new_params.move_to_end(param_name)\n            elif param.kind is _VAR_POSITIONAL:\n                new_params.pop(param.name)\n\n    return wrapped_sig.replace(parameters=new_params.values())\n\n\ndef _signature_bound_method(sig):\n    \"\"\"Private helper to transform signatures for unbound\n    functions to bound methods.\n    \"\"\"\n\n    params = tuple(sig.parameters.values())\n\n    if not params or params[0].kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n        raise ValueError('invalid method signature')\n\n    kind = params[0].kind\n    if kind in (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n        # Drop first parameter:\n        # '(p1, p2[, ...])' -> '(p2[, ...])'\n        params = params[1:]\n    else:\n        if kind is not _VAR_POSITIONAL:\n            # Unless we add a new parameter type we never\n            # get here\n            raise ValueError('invalid argument type')\n        # It's a var-positional parameter.\n        # Do nothing. '(*args[, ...])' -> '(*args[, ...])'\n\n    return sig.replace(parameters=params)\n\n\ndef _signature_is_builtin(obj):\n    \"\"\"Private helper to test if `obj` is a callable that might\n    support Argument Clinic's __text_signature__ protocol.\n    \"\"\"\n    return (isbuiltin(obj) or\n            ismethoddescriptor(obj) or\n            isinstance(obj, _NonUserDefinedCallables) or\n            # Can't test 'isinstance(type)' here, as it would\n            # also be True for regular python classes.\n            # Can't use the `in` operator here, as it would\n            # invoke the custom __eq__ method.\n            obj is type or obj is object)\n\n\ndef _signature_is_functionlike(obj):\n    \"\"\"Private helper to test if `obj` is a duck type of FunctionType.\n    A good example of such objects are functions compiled with\n    Cython, which have all attributes that a pure Python function\n    would have, but have their code statically compiled.\n    \"\"\"\n\n    if not callable(obj) or isclass(obj):\n        # All function-like objects are obviously callables,\n        # and not classes.\n        return False\n\n    name = getattr(obj, '__name__', None)\n    code = getattr(obj, '__code__', None)\n    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...\n    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here\n    annotations = getattr(obj, '__annotations__', None)\n\n    return (isinstance(code, types.CodeType) and\n            isinstance(name, str) and\n            (defaults is None or isinstance(defaults, tuple)) and\n            (kwdefaults is None or isinstance(kwdefaults, dict)) and\n            (isinstance(annotations, (dict)) or annotations is None) )\n\n\ndef _signature_strip_non_python_syntax(signature):\n    \"\"\"\n    Private helper function. Takes a signature in Argument Clinic's\n    extended signature format.\n\n    Returns a tuple of two things:\n      * that signature re-rendered in standard Python syntax, and\n      * the index of the \"self\" parameter (generally 0), or None if\n        the function does not have a \"self\" parameter.\n    \"\"\"\n\n    if not signature:\n        return signature, None\n\n    self_parameter = None\n\n    lines = [l.encode('ascii') for l in signature.split('\\n') if l]\n    generator = iter(lines).__next__\n    token_stream = tokenize.tokenize(generator)\n\n    text = []\n    add = text.append\n\n    current_parameter = 0\n    OP = token.OP\n    ERRORTOKEN = token.ERRORTOKEN\n\n    # token stream always starts with ENCODING token, skip it\n    t = next(token_stream)\n    assert t.type == tokenize.ENCODING\n\n    for t in token_stream:\n        type, string = t.type, t.string\n\n        if type == OP:\n            if string == ',':\n                current_parameter += 1\n\n        if (type == OP) and (string == '$'):\n            assert self_parameter is None\n            self_parameter = current_parameter\n            continue\n\n        add(string)\n        if (string == ','):\n            add(' ')\n    clean_signature = ''.join(text).strip().replace(\"\\n\", \"\")\n    return clean_signature, self_parameter\n\n\ndef _signature_fromstr(cls, obj, s, skip_bound_arg=True):\n    \"\"\"Private helper to parse content of '__text_signature__'\n    and return a Signature based on it.\n    \"\"\"\n    Parameter = cls._parameter_cls\n\n    clean_signature, self_parameter = _signature_strip_non_python_syntax(s)\n\n    program = \"def foo\" + clean_signature + \": pass\"\n\n    try:\n        module = ast.parse(program)\n    except SyntaxError:\n        module = None\n\n    if not isinstance(module, ast.Module):\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\n\n    f = module.body[0]\n\n    parameters = []\n    empty = Parameter.empty\n\n    module = None\n    module_dict = {}\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        module = sys.modules.get(module_name, None)\n        if module:\n            module_dict = module.__dict__\n    sys_module_dict = sys.modules.copy()\n\n    def parse_name(node):\n        assert isinstance(node, ast.arg)\n        if node.annotation is not None:\n            raise ValueError(\"Annotations are not currently supported\")\n        return node.arg\n\n    def wrap_value(s):\n        try:\n            value = eval(s, module_dict)\n        except NameError:\n            try:\n                value = eval(s, sys_module_dict)\n            except NameError:\n                raise ValueError\n\n        if isinstance(value, (str, int, float, bytes, bool, type(None))):\n            return ast.Constant(value)\n        raise ValueError\n\n    class RewriteSymbolics(ast.NodeTransformer):\n        def visit_Attribute(self, node):\n            a = []\n            n = node\n            while isinstance(n, ast.Attribute):\n                a.append(n.attr)\n                n = n.value\n            if not isinstance(n, ast.Name):\n                raise ValueError\n            a.append(n.id)\n            value = \".\".join(reversed(a))\n            return wrap_value(value)\n\n        def visit_Name(self, node):\n            if not isinstance(node.ctx, ast.Load):\n                raise ValueError()\n            return wrap_value(node.id)\n\n        def visit_BinOp(self, node):\n            # Support constant folding of a couple simple binary operations\n            # commonly used to define default values in text signatures\n            left = self.visit(node.left)\n            right = self.visit(node.right)\n            if not isinstance(left, ast.Constant) or not isinstance(right, ast.Constant):\n                raise ValueError\n            if isinstance(node.op, ast.Add):\n                return ast.Constant(left.value + right.value)\n            elif isinstance(node.op, ast.Sub):\n                return ast.Constant(left.value - right.value)\n            elif isinstance(node.op, ast.BitOr):\n                return ast.Constant(left.value | right.value)\n            raise ValueError\n\n    def p(name_node, default_node, default=empty):\n        name = parse_name(name_node)\n        if default_node and default_node is not _empty:\n            try:\n                default_node = RewriteSymbolics().visit(default_node)\n                default = ast.literal_eval(default_node)\n            except ValueError:\n                raise ValueError(\"{!r} builtin has invalid signature\".format(obj)) from None\n        parameters.append(Parameter(name, kind, default=default, annotation=empty))\n\n    # non-keyword-only parameters\n    total_non_kw_args = len(f.args.posonlyargs) + len(f.args.args)\n    required_non_kw_args = total_non_kw_args - len(f.args.defaults)\n    defaults = itertools.chain(itertools.repeat(None, required_non_kw_args), f.args.defaults)\n\n    kind = Parameter.POSITIONAL_ONLY\n    for (name, default) in zip(f.args.posonlyargs, defaults):\n        p(name, default)\n\n    kind = Parameter.POSITIONAL_OR_KEYWORD\n    for (name, default) in zip(f.args.args, defaults):\n        p(name, default)\n\n    # *args\n    if f.args.vararg:\n        kind = Parameter.VAR_POSITIONAL\n        p(f.args.vararg, empty)\n\n    # keyword-only arguments\n    kind = Parameter.KEYWORD_ONLY\n    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):\n        p(name, default)\n\n    # **kwargs\n    if f.args.kwarg:\n        kind = Parameter.VAR_KEYWORD\n        p(f.args.kwarg, empty)\n\n    if self_parameter is not None:\n        # Possibly strip the bound argument:\n        #    - We *always* strip first bound argument if\n        #      it is a module.\n        #    - We don't strip first bound argument if\n        #      skip_bound_arg is False.\n        assert parameters\n        _self = getattr(obj, '__self__', None)\n        self_isbound = _self is not None\n        self_ismodule = ismodule(_self)\n        if self_isbound and (self_ismodule or skip_bound_arg):\n            parameters.pop(0)\n        else:\n            # for builtins, self parameter is always positional-only!\n            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)\n            parameters[0] = p\n\n    return cls(parameters, return_annotation=cls.empty)\n\n\ndef _signature_from_builtin(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper function to get signature for\n    builtin callables.\n    \"\"\"\n\n    if not _signature_is_builtin(func):\n        raise TypeError(\"{!r} is not a Python builtin \"\n                        \"function\".format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if not s:\n        raise ValueError(\"no signature found for builtin {!r}\".format(func))\n\n    return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n\ndef _signature_from_function(cls, func, skip_bound_arg=True,\n                             globals=None, locals=None, eval_str=False):\n    \"\"\"Private helper: constructs Signature for the given python function.\"\"\"\n\n    is_duck_function = False\n    if not isfunction(func):\n        if _signature_is_functionlike(func):\n            is_duck_function = True\n        else:\n            # If it's not a pure Python function, and not a duck type\n            # of pure function:\n            raise TypeError('{!r} is not a Python function'.format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if s:\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n    Parameter = cls._parameter_cls\n\n    # Parameter information.\n    func_code = func.__code__\n    pos_count = func_code.co_argcount\n    arg_names = func_code.co_varnames\n    posonly_count = func_code.co_posonlyargcount\n    positional = arg_names[:pos_count]\n    keyword_only_count = func_code.co_kwonlyargcount\n    keyword_only = arg_names[pos_count:pos_count + keyword_only_count]\n    annotations = get_annotations(func, globals=globals, locals=locals, eval_str=eval_str)\n    defaults = func.__defaults__\n    kwdefaults = func.__kwdefaults__\n\n    if defaults:\n        pos_default_count = len(defaults)\n    else:\n        pos_default_count = 0\n\n    parameters = []\n\n    non_default_count = pos_count - pos_default_count\n    posonly_left = posonly_count\n\n    # Non-keyword-only parameters w/o defaults.\n    for name in positional[:non_default_count]:\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind))\n        if posonly_left:\n            posonly_left -= 1\n\n    # ... w/ defaults.\n    for offset, name in enumerate(positional[non_default_count:]):\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind,\n                                    default=defaults[offset]))\n        if posonly_left:\n            posonly_left -= 1\n\n    # *args\n    if func_code.co_flags & CO_VARARGS:\n        name = arg_names[pos_count + keyword_only_count]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_POSITIONAL))\n\n    # Keyword-only parameters.\n    for name in keyword_only:\n        default = _empty\n        if kwdefaults is not None:\n            default = kwdefaults.get(name, _empty)\n\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_KEYWORD_ONLY,\n                                    default=default))\n    # **kwargs\n    if func_code.co_flags & CO_VARKEYWORDS:\n        index = pos_count + keyword_only_count\n        if func_code.co_flags & CO_VARARGS:\n            index += 1\n\n        name = arg_names[index]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_KEYWORD))\n\n    # Is 'func' is a pure Python function - don't validate the\n    # parameters list (for correct order and defaults), it should be OK.\n    return cls(parameters,\n               return_annotation=annotations.get('return', _empty),\n               __validate_parameters__=is_duck_function)\n\n\ndef _descriptor_get(descriptor, obj):\n    if isclass(descriptor):\n        return descriptor\n    get = getattr(type(descriptor), '__get__', _sentinel)\n    if get is _sentinel:\n        return descriptor\n    return get(descriptor, obj, type(obj))\n\n\ndef _signature_from_callable(obj, *,\n                             follow_wrapper_chains=True,\n                             skip_bound_arg=True,\n                             globals=None,\n                             locals=None,\n                             eval_str=False,\n                             sigcls):\n\n    \"\"\"Private helper function to get signature for arbitrary\n    callable objects.\n    \"\"\"\n\n    _get_signature_of = functools.partial(_signature_from_callable,\n                                follow_wrapper_chains=follow_wrapper_chains,\n                                skip_bound_arg=skip_bound_arg,\n                                globals=globals,\n                                locals=locals,\n                                sigcls=sigcls,\n                                eval_str=eval_str)\n\n    if not callable(obj):\n        raise TypeError('{!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        # In this case we skip the first parameter of the underlying\n        # function (usually `self` or `cls`).\n        sig = _get_signature_of(obj.__func__)\n\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    # Was this function wrapped by a decorator?\n    if follow_wrapper_chains:\n        # Unwrap until we find an explicit signature or a MethodType (which will be\n        # handled explicitly below).\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")\n                                or isinstance(f, types.MethodType)))\n        if isinstance(obj, types.MethodType):\n            # If the unwrapped object is a *method*, we might want to\n            # skip its first parameter (self).\n            # See test_signature_wrapped_bound_method for details.\n            return _get_signature_of(obj)\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            # since __text_signature__ is not writable on classes, __signature__\n            # may contain text (or be a callable that returns text);\n            # if so, convert it\n            o_sig = sig\n            if not isinstance(sig, (Signature, str)) and callable(sig):\n                sig = sig()\n            if isinstance(sig, str):\n                sig = _signature_fromstr(sigcls, obj, sig)\n            if not isinstance(sig, Signature):\n                raise TypeError(\n                    'unexpected object {!r} in __signature__ '\n                    'attribute'.format(o_sig))\n            return sig\n\n    try:\n        partialmethod = obj._partialmethod\n    except AttributeError:\n        pass\n    else:\n        if isinstance(partialmethod, functools.partialmethod):\n            # Unbound partialmethod (see functools.partialmethod)\n            # This means, that we need to calculate the signature\n            # as if it's a regular partial object, but taking into\n            # account that the first positional argument\n            # (usually `self`, or `cls`) will not be passed\n            # automatically (as for boundmethods)\n\n            wrapped_sig = _get_signature_of(partialmethod.func)\n\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\n                # First argument of the wrapped callable is `*args`, as in\n                # `partialmethod(lambda *args)`.\n                return sig\n            else:\n                sig_params = tuple(sig.parameters.values())\n                assert (not sig_params or\n                        first_wrapped_param is not sig_params[0])\n                new_params = (first_wrapped_param,) + sig_params\n                return sig.replace(parameters=new_params)\n\n    if isfunction(obj) or _signature_is_functionlike(obj):\n        # If it's a pure Python function, or an object that is duck type\n        # of a Python function (Cython functions, for instance), then:\n        return _signature_from_function(sigcls, obj,\n                                        skip_bound_arg=skip_bound_arg,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    if _signature_is_builtin(obj):\n        return _signature_from_builtin(sigcls, obj,\n                                       skip_bound_arg=skip_bound_arg)\n\n    if isinstance(obj, functools.partial):\n        wrapped_sig = _get_signature_of(obj.func)\n        return _signature_get_partial(wrapped_sig, obj)\n\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            return _get_signature_of(call)\n\n        new = _signature_get_user_defined_method(obj, '__new__')\n        init = _signature_get_user_defined_method(obj, '__init__')\n\n        # Go through the MRO and see if any class has user-defined\n        # pure Python __new__ or __init__ method\n        for base in obj.__mro__:\n            # Now we check if the 'obj' class has an own '__new__' method\n            if new is not None and '__new__' in base.__dict__:\n                sig = _get_signature_of(new)\n                if skip_bound_arg:\n                    sig = _signature_bound_method(sig)\n                return sig\n            # or an own '__init__' method\n            elif init is not None and '__init__' in base.__dict__:\n                return _get_signature_of(init)\n\n        # At this point we know, that `obj` is a class, with no user-\n        # defined '__init__', '__new__', or class-level '__call__'\n\n        for base in obj.__mro__[:-1]:\n            # Since '__text_signature__' is implemented as a\n            # descriptor that extracts text signature from the\n            # class docstring, if 'obj' is derived from a builtin\n            # class, its own '__text_signature__' may be 'None'.\n            # Therefore, we go through the MRO (except the last\n            # class in there, which is 'object') to find the first\n            # class with non-empty text signature.\n            try:\n                text_sig = base.__text_signature__\n            except AttributeError:\n                pass\n            else:\n                if text_sig:\n                    # If 'base' class has a __text_signature__ attribute:\n                    # return a signature based on it\n                    return _signature_fromstr(sigcls, base, text_sig)\n\n        # No '__text_signature__' was found for the 'obj' class.\n        # Last option is to check if its '__init__' is\n        # object.__init__ or type.__init__.\n        if type not in obj.__mro__:\n            # We have a class (not metaclass), but no user-defined\n            # __init__ or __new__ for it\n            if (obj.__init__ is object.__init__ and\n                obj.__new__ is object.__new__):\n                # Return a signature of 'object' builtin.\n                return sigcls.from_callable(object)\n            else:\n                raise ValueError(\n                    'no signature found for builtin type {!r}'.format(obj))\n\n    else:\n        # An object with __call__\n        call = getattr_static(type(obj), '__call__', None)\n        if call is not None:\n            call = _descriptor_get(call, obj)\n            return _get_signature_of(call)\n\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))\n\n\nclass _void:\n    \"\"\"A private marker - used in Parameter & Signature.\"\"\"\n\n\nclass _empty:\n    \"\"\"Marker object for Signature.empty and Parameter.empty.\"\"\"\n\n\nclass _ParameterKind(enum.IntEnum):\n    POSITIONAL_ONLY = 'positional-only'\n    POSITIONAL_OR_KEYWORD = 'positional or keyword'\n    VAR_POSITIONAL = 'variadic positional'\n    KEYWORD_ONLY = 'keyword-only'\n    VAR_KEYWORD = 'variadic keyword'\n\n    def __new__(cls, description):\n        value = len(cls.__members__)\n        member = int.__new__(cls, value)\n        member._value_ = value\n        member.description = description\n        return member\n\n    def __str__(self):\n        return self.name\n\n_POSITIONAL_ONLY         = _ParameterKind.POSITIONAL_ONLY\n_POSITIONAL_OR_KEYWORD   = _ParameterKind.POSITIONAL_OR_KEYWORD\n_VAR_POSITIONAL          = _ParameterKind.VAR_POSITIONAL\n_KEYWORD_ONLY            = _ParameterKind.KEYWORD_ONLY\n_VAR_KEYWORD             = _ParameterKind.VAR_KEYWORD\n\n\nclass Parameter:\n    \"\"\"Represents a parameter in a function signature.\n\n    Has the following public attributes:\n\n    * name : str\n        The name of the parameter as a string.\n    * default : object\n        The default value for the parameter if specified.  If the\n        parameter has no default value, this attribute is set to\n        `Parameter.empty`.\n    * annotation\n        The annotation for the parameter if specified.  If the\n        parameter has no annotation, this attribute is set to\n        `Parameter.empty`.\n    * kind : str\n        Describes how argument values are bound to the parameter.\n        Possible values: `Parameter.POSITIONAL_ONLY`,\n        `Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n        `Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n    \"\"\"\n\n    __slots__ = ('_name', '_kind', '_default', '_annotation')\n\n    POSITIONAL_ONLY         = _POSITIONAL_ONLY\n    POSITIONAL_OR_KEYWORD   = _POSITIONAL_OR_KEYWORD\n    VAR_POSITIONAL          = _VAR_POSITIONAL\n    KEYWORD_ONLY            = _KEYWORD_ONLY\n    VAR_KEYWORD             = _VAR_KEYWORD\n\n    empty = _empty\n\n    def __init__(self, name, kind, *, default=_empty, annotation=_empty):\n        try:\n            self._kind = _ParameterKind(kind)\n        except ValueError:\n            raise ValueError(f'value {kind!r} is not a valid Parameter.kind')\n        if default is not _empty:\n            if self._kind in (_VAR_POSITIONAL, _VAR_KEYWORD):\n                msg = '{} parameters cannot have default values'\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n        self._default = default\n        self._annotation = annotation\n\n        if name is _empty:\n            raise ValueError('name is a required attribute for Parameter')\n\n        if not isinstance(name, str):\n            msg = 'name must be a str, not a {}'.format(type(name).__name__)\n            raise TypeError(msg)\n\n        if name[0] == '.' and name[1:].isdigit():\n            # These are implicit arguments generated by comprehensions. In\n            # order to provide a friendlier interface to users, we recast\n            # their name as \"implicitN\" and treat them as positional-only.\n            # See issue 19611.\n            if self._kind != _POSITIONAL_OR_KEYWORD:\n                msg = (\n                    'implicit arguments must be passed as '\n                    'positional or keyword arguments, not {}'\n                )\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n            self._kind = _POSITIONAL_ONLY\n            name = 'implicit{}'.format(name[1:])\n\n        # It's possible for C functions to have a positional-only parameter\n        # where the name is a keyword, so for compatibility we'll allow it.\n        is_keyword = iskeyword(name) and self._kind is not _POSITIONAL_ONLY\n        if is_keyword or not name.isidentifier():\n            raise ValueError('{!r} is not a valid parameter name'.format(name))\n\n        self._name = name\n\n    def __reduce__(self):\n        return (type(self),\n                (self._name, self._kind),\n                {'_default': self._default,\n                 '_annotation': self._annotation})\n\n    def __setstate__(self, state):\n        self._default = state['_default']\n        self._annotation = state['_annotation']\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def annotation(self):\n        return self._annotation\n\n    @property\n    def kind(self):\n        return self._kind\n\n    def replace(self, *, name=_void, kind=_void,\n                annotation=_void, default=_void):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        return type(self)(name, kind, default=default, annotation=annotation)\n\n    def __str__(self):\n        kind = self.kind\n        formatted = self._name\n\n        # Add annotation and default value\n        if self._annotation is not _empty:\n            formatted = '{}: {}'.format(formatted,\n                                       formatannotation(self._annotation))\n\n        if self._default is not _empty:\n            if self._annotation is not _empty:\n                formatted = '{} = {}'.format(formatted, repr(self._default))\n            else:\n                formatted = '{}={}'.format(formatted, repr(self._default))\n\n        if kind == _VAR_POSITIONAL:\n            formatted = '*' + formatted\n        elif kind == _VAR_KEYWORD:\n            formatted = '**' + formatted\n\n        return formatted\n\n    def __repr__(self):\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)\n\n    def __hash__(self):\n        return hash((self._name, self._kind, self._annotation, self._default))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Parameter):\n            return NotImplemented\n        return (self._name == other._name and\n                self._kind == other._kind and\n                self._default == other._default and\n                self._annotation == other._annotation)\n\n\nclass BoundArguments:\n    \"\"\"Result of `Signature.bind` call.  Holds the mapping of arguments\n    to the function's parameters.\n\n    Has the following public attributes:\n\n    * arguments : dict\n        An ordered mutable mapping of parameters' names to arguments' values.\n        Does not contain arguments' default values.\n    * signature : Signature\n        The Signature object that created this instance.\n    * args : tuple\n        Tuple of positional arguments values.\n    * kwargs : dict\n        Dict of keyword arguments values.\n    \"\"\"\n\n    __slots__ = ('arguments', '_signature', '__weakref__')\n\n    def __init__(self, signature, arguments):\n        self.arguments = arguments\n        self._signature = signature\n\n    @property\n    def signature(self):\n        return self._signature\n\n    @property\n    def args(self):\n        args = []\n        for param_name, param in self._signature.parameters.items():\n            if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                break\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                # We're done here. Other arguments\n                # will be mapped in 'BoundArguments.kwargs'\n                break\n            else:\n                if param.kind == _VAR_POSITIONAL:\n                    # *args\n                    args.extend(arg)\n                else:\n                    # plain argument\n                    args.append(arg)\n\n        return tuple(args)\n\n    @property\n    def kwargs(self):\n        kwargs = {}\n        kwargs_started = False\n        for param_name, param in self._signature.parameters.items():\n            if not kwargs_started:\n                if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                    kwargs_started = True\n                else:\n                    if param_name not in self.arguments:\n                        kwargs_started = True\n                        continue\n\n            if not kwargs_started:\n                continue\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                pass\n            else:\n                if param.kind == _VAR_KEYWORD:\n                    # **kwargs\n                    kwargs.update(arg)\n                else:\n                    # plain keyword argument\n                    kwargs[param_name] = arg\n\n        return kwargs\n\n    def apply_defaults(self):\n        \"\"\"Set default values for missing arguments.\n\n        For variable-positional arguments (*args) the default is an\n        empty tuple.\n\n        For variable-keyword arguments (**kwargs) the default is an\n        empty dict.\n        \"\"\"\n        arguments = self.arguments\n        new_arguments = []\n        for name, param in self._signature.parameters.items():\n            try:\n                new_arguments.append((name, arguments[name]))\n            except KeyError:\n                if param.default is not _empty:\n                    val = param.default\n                elif param.kind is _VAR_POSITIONAL:\n                    val = ()\n                elif param.kind is _VAR_KEYWORD:\n                    val = {}\n                else:\n                    # This BoundArguments was likely produced by\n                    # Signature.bind_partial().\n                    continue\n                new_arguments.append((name, val))\n        self.arguments = dict(new_arguments)\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, BoundArguments):\n            return NotImplemented\n        return (self.signature == other.signature and\n                self.arguments == other.arguments)\n\n    def __setstate__(self, state):\n        self._signature = state['_signature']\n        self.arguments = state['arguments']\n\n    def __getstate__(self):\n        return {'_signature': self._signature, 'arguments': self.arguments}\n\n    def __repr__(self):\n        args = []\n        for arg, value in self.arguments.items():\n            args.append('{}={!r}'.format(arg, value))\n        return '<{} ({})>'.format(self.__class__.__name__, ', '.join(args))\n\n\nclass Signature:\n    \"\"\"A Signature object represents the overall signature of a function.\n    It stores a Parameter object for each parameter accepted by the\n    function, as well as information specific to the function itself.\n\n    A Signature object has the following public attributes and methods:\n\n    * parameters : OrderedDict\n        An ordered mapping of parameters' names to the corresponding\n        Parameter objects (keyword-only arguments are in the same order\n        as listed in `code.co_varnames`).\n    * return_annotation : object\n        The annotation for the return type of the function if specified.\n        If the function has no annotation for its return type, this\n        attribute is set to `Signature.empty`.\n    * bind(*args, **kwargs) -> BoundArguments\n        Creates a mapping from positional and keyword arguments to\n        parameters.\n    * bind_partial(*args, **kwargs) -> BoundArguments\n        Creates a partial mapping from positional and keyword arguments\n        to parameters (simulating 'functools.partial' behavior.)\n    \"\"\"\n\n    __slots__ = ('_return_annotation', '_parameters')\n\n    _parameter_cls = Parameter\n    _bound_arguments_cls = BoundArguments\n\n    empty = _empty\n\n    def __init__(self, parameters=None, *, return_annotation=_empty,\n                 __validate_parameters__=True):\n        \"\"\"Constructs Signature from the given list of Parameter\n        objects and 'return_annotation'.  All arguments are optional.\n        \"\"\"\n\n        if parameters is None:\n            params = OrderedDict()\n        else:\n            if __validate_parameters__:\n                params = OrderedDict()\n                top_kind = _POSITIONAL_ONLY\n                seen_default = False\n\n                for param in parameters:\n                    kind = param.kind\n                    name = param.name\n\n                    if kind < top_kind:\n                        msg = (\n                            'wrong parameter order: {} parameter before {} '\n                            'parameter'\n                        )\n                        msg = msg.format(top_kind.description,\n                                         kind.description)\n                        raise ValueError(msg)\n                    elif kind > top_kind:\n                        top_kind = kind\n\n                    if kind in (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n                        if param.default is _empty:\n                            if seen_default:\n                                # No default for this parameter, but the\n                                # previous parameter of had a default\n                                msg = 'non-default argument follows default ' \\\n                                      'argument'\n                                raise ValueError(msg)\n                        else:\n                            # There is a default for this parameter.\n                            seen_default = True\n\n                    if name in params:\n                        msg = 'duplicate parameter name: {!r}'.format(name)\n                        raise ValueError(msg)\n\n                    params[name] = param\n            else:\n                params = OrderedDict((param.name, param) for param in parameters)\n\n        self._parameters = types.MappingProxyType(params)\n        self._return_annotation = return_annotation\n\n    @classmethod\n    def from_callable(cls, obj, *,\n                      follow_wrapped=True, globals=None, locals=None, eval_str=False):\n        \"\"\"Constructs Signature for the given callable object.\"\"\"\n        return _signature_from_callable(obj, sigcls=cls,\n                                        follow_wrapper_chains=follow_wrapped,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @property\n    def return_annotation(self):\n        return self._return_annotation\n\n    def replace(self, *, parameters=_void, return_annotation=_void):\n        \"\"\"Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters,\n                          return_annotation=return_annotation)\n\n    def _hash_basis(self):\n        params = tuple(param for param in self.parameters.values()\n                             if param.kind != _KEYWORD_ONLY)\n\n        kwo_params = {param.name: param for param in self.parameters.values()\n                                        if param.kind == _KEYWORD_ONLY}\n\n        return params, kwo_params, self.return_annotation\n\n    def __hash__(self):\n        params, kwo_params, return_annotation = self._hash_basis()\n        kwo_params = frozenset(kwo_params.values())\n        return hash((params, kwo_params, return_annotation))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Signature):\n            return NotImplemented\n        return self._hash_basis() == other._hash_basis()\n\n    def _bind(self, args, kwargs, *, partial=False):\n        \"\"\"Private method. Don't use directly.\"\"\"\n\n        arguments = {}\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        pos_only_param_in_kwargs = []\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            if param.default is _empty:\n                                msg = f'missing a required positional-only argument: {param.name!r}'\n                                raise TypeError(msg)\n                            # Raise a TypeError once we are sure there is no\n                            # **kwargs param later.\n                            pos_only_param_in_kwargs.append(param)\n                            continue\n                        parameters_ex = (param,)\n                        break\n                    elif (param.kind == _VAR_KEYWORD or\n                                                param.default is not _empty):\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,\n                        # not in `kwargs`\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            if param.kind == _KEYWORD_ONLY:\n                                argtype = ' keyword-only'\n                            else:\n                                argtype = ''\n                            msg = 'missing a required{argtype} argument: {arg!r}'\n                            msg = msg.format(arg=param.name, argtype=argtype)\n                            raise TypeError(msg) from None\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError('too many positional arguments') from None\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\n                            'too many positional arguments') from None\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:\n                        raise TypeError(\n                            'multiple values for argument {arg!r}'.format(\n                                arg=param.name)) from None\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            if param.kind == _VAR_POSITIONAL:\n                # Named arguments don't refer to '*args'-like parameters.\n                # We only arrive here if the positional arguments ended\n                # before reaching the last parameter before *args.\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (not partial and param.kind != _VAR_POSITIONAL and\n                                                    param.default is _empty):\n                    raise TypeError('missing a required argument: {arg!r}'. \\\n                                    format(arg=param_name)) from None\n\n            else:\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            elif pos_only_param_in_kwargs:\n                raise TypeError(\n                    'got some positional-only arguments passed as '\n                    'keyword arguments: {arg!r}'.format(\n                        arg=', '.join(\n                            param.name\n                            for param in pos_only_param_in_kwargs\n                        ),\n                    ),\n                )\n            else:\n                raise TypeError(\n                    'got an unexpected keyword argument {arg!r}'.format(\n                        arg=next(iter(kwargs))))\n\n        return self._bound_arguments_cls(self, arguments)\n\n    def bind(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that maps the passed `args`\n        and `kwargs` to the function's signature.  Raises `TypeError`\n        if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs)\n\n    def bind_partial(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)\n\n    def __reduce__(self):\n        return (type(self),\n                (tuple(self._parameters.values()),),\n                {'_return_annotation': self._return_annotation})\n\n    def __setstate__(self, state):\n        self._return_annotation = state['_return_annotation']\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__, self)\n\n    def __str__(self):\n        result = []\n        render_pos_only_separator = False\n        render_kw_only_separator = True\n        for param in self.parameters.values():\n            formatted = str(param)\n\n            kind = param.kind\n\n            if kind == _POSITIONAL_ONLY:\n                render_pos_only_separator = True\n            elif render_pos_only_separator:\n                # It's not a positional-only parameter, and the flag\n                # is set to 'True' (there were pos-only params before.)\n                result.append('/')\n                render_pos_only_separator = False\n\n            if kind == _VAR_POSITIONAL:\n                # OK, we have an '*args'-like parameter, so we won't need\n                # a '*' to separate keyword-only arguments\n                render_kw_only_separator = False\n            elif kind == _KEYWORD_ONLY and render_kw_only_separator:\n                # We have a keyword-only parameter to render and we haven't\n                # rendered an '*args'-like parameter before, so add a '*'\n                # separator to the parameters list (\"foo(arg1, *, arg2)\" case)\n                result.append('*')\n                # This condition should be only triggered once, so\n                # reset the flag\n                render_kw_only_separator = False\n\n            result.append(formatted)\n\n        if render_pos_only_separator:\n            # There were only positional-only parameters, hence the\n            # flag was not reset to 'False'\n            result.append('/')\n\n        rendered = '({})'.format(', '.join(result))\n\n        if self.return_annotation is not _empty:\n            anno = formatannotation(self.return_annotation)\n            rendered += ' -> {}'.format(anno)\n\n        return rendered\n\n\ndef signature(obj, *, follow_wrapped=True, globals=None, locals=None, eval_str=False):\n    \"\"\"Get a signature object for the passed callable.\"\"\"\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n                                   globals=globals, locals=locals, eval_str=eval_str)\n\n\nclass BufferFlags(enum.IntFlag):\n    SIMPLE = 0x0\n    WRITABLE = 0x1\n    FORMAT = 0x4\n    ND = 0x8\n    STRIDES = 0x10 | ND\n    C_CONTIGUOUS = 0x20 | STRIDES\n    F_CONTIGUOUS = 0x40 | STRIDES\n    ANY_CONTIGUOUS = 0x80 | STRIDES\n    INDIRECT = 0x100 | STRIDES\n    CONTIG = ND | WRITABLE\n    CONTIG_RO = ND\n    STRIDED = STRIDES | WRITABLE\n    STRIDED_RO = STRIDES\n    RECORDS = STRIDES | WRITABLE | FORMAT\n    RECORDS_RO = STRIDES | FORMAT\n    FULL = INDIRECT | WRITABLE | FORMAT\n    FULL_RO = INDIRECT | FORMAT\n    READ = 0x100\n    WRITE = 0x200\n\n\ndef _main():\n    \"\"\" Logic for inspecting an object given at command line \"\"\"\n    import argparse\n    import importlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'object',\n         help=\"The object to be analysed. \"\n              \"It supports the 'module:qualname' syntax\")\n    parser.add_argument(\n        '-d', '--details', action='store_true',\n        help='Display info about the module rather than its source code')\n\n    args = parser.parse_args()\n\n    target = args.object\n    mod_name, has_attrs, attrs = target.partition(\":\")\n    try:\n        obj = module = importlib.import_module(mod_name)\n    except Exception as exc:\n        msg = \"Failed to import {} ({}: {})\".format(mod_name,\n                                                    type(exc).__name__,\n                                                    exc)\n        print(msg, file=sys.stderr)\n        sys.exit(2)\n\n    if has_attrs:\n        parts = attrs.split(\".\")\n        obj = module\n        for part in parts:\n            obj = getattr(obj, part)\n\n    if module.__name__ in sys.builtin_module_names:\n        print(\"Can't get info for builtin modules.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.details:\n        print('Target: {}'.format(target))\n        print('Origin: {}'.format(getsourcefile(module)))\n        print('Cached: {}'.format(module.__cached__))\n        if obj is module:\n            print('Loader: {}'.format(repr(module.__loader__)))\n            if hasattr(module, '__path__'):\n                print('Submodule search path: {}'.format(module.__path__))\n        else:\n            try:\n                __, lineno = findsource(obj)\n            except Exception:\n                pass\n            else:\n                print('Line: {}'.format(lineno))\n\n        print('\\n')\n    else:\n        print(getsource(obj))\n\n\nif __name__ == \"__main__\":\n    _main()\n", 3433], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dataclasses.py": ["import re\nimport sys\nimport copy\nimport types\nimport inspect\nimport keyword\nimport functools\nimport itertools\nimport abc\nimport _thread\nfrom types import FunctionType, GenericAlias\n\n\n__all__ = ['dataclass',\n           'field',\n           'Field',\n           'FrozenInstanceError',\n           'InitVar',\n           'KW_ONLY',\n           'MISSING',\n\n           # Helper functions.\n           'fields',\n           'asdict',\n           'astuple',\n           'make_dataclass',\n           'replace',\n           'is_dataclass',\n           ]\n\n# Conditions for adding methods.  The boxes indicate what action the\n# dataclass decorator takes.  For all of these tables, when I talk\n# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I'm\n# referring to the arguments to the @dataclass decorator.  When\n# checking if a dunder method already exists, I mean check for an\n# entry in the class's __dict__.  I never check to see if an attribute\n# is defined in a base class.\n\n# Key:\n# +=========+=========================================+\n# + Value   | Meaning                                 |\n# +=========+=========================================+\n# | <blank> | No action: no method is added.          |\n# +---------+-----------------------------------------+\n# | add     | Generated method is added.              |\n# +---------+-----------------------------------------+\n# | raise   | TypeError is raised.                    |\n# +---------+-----------------------------------------+\n# | None    | Attribute is set to None.               |\n# +=========+=========================================+\n\n# __init__\n#\n#   +--- init= parameter\n#   |\n#   v     |       |       |\n#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __repr__\n#\n#    +--- repr= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n\n# __setattr__\n# __delattr__\n#\n#    +--- frozen= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because not adding these methods would break the \"frozen-ness\"\n# of the class.\n\n# __eq__\n#\n#    +--- eq= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __lt__\n# __le__\n# __gt__\n# __ge__\n#\n#    +--- order= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because to allow this case would interfere with using\n# functools.total_ordering.\n\n# __hash__\n\n#    +------------------- unsafe_hash= parameter\n#    |       +----------- eq= parameter\n#    |       |       +--- frozen= parameter\n#    |       |       |\n#    v       v       v    |        |        |\n#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n# +=======+=======+=======+========+========+\n# | False | False | False |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | False | True  |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | True  | False | None   |        | <-- the default, not hashable\n# +-------+-------+-------+--------+--------+\n# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n# +-------+-------+-------+--------+--------+\n# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | False | add    | raise  | Not frozen, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | True  | add    | raise  | Frozen, so hashable\n# +=======+=======+=======+========+========+\n# For boxes that are blank, __hash__ is untouched and therefore\n# inherited from the base class.  If the base is object, then\n# id-based hashing is used.\n#\n# Note that a class may already have __hash__=None if it specified an\n# __eq__ method in the class body (not one that was created by\n# @dataclass).\n#\n# See _hash_action (below) for a coded version of this table.\n\n# __match_args__\n#\n#    +--- match_args= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __match_args__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n# __match_args__ is always added unless the class already defines it. It is a\n# tuple of __init__ parameter names; non-init fields must be matched by keyword.\n\n\n# Raised when an attempt is made to modify a frozen class.\nclass FrozenInstanceError(AttributeError): pass\n\n# A sentinel object for default values to signal that a default\n# factory will be used.  This is given a nice repr() which will appear\n# in the function signature of dataclasses' constructors.\nclass _HAS_DEFAULT_FACTORY_CLASS:\n    def __repr__(self):\n        return '<factory>'\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n\n# A sentinel object to detect if a parameter is supplied or not.  Use\n# a class to give it a better repr.\nclass _MISSING_TYPE:\n    pass\nMISSING = _MISSING_TYPE()\n\n# A sentinel object to indicate that following fields are keyword-only by\n# default.  Use a class to give it a better repr.\nclass _KW_ONLY_TYPE:\n    pass\nKW_ONLY = _KW_ONLY_TYPE()\n\n# Since most per-field metadata will be unused, create an empty\n# read-only proxy that can be shared among all fields.\n_EMPTY_METADATA = types.MappingProxyType({})\n\n# Markers for the various kinds of fields and pseudo-fields.\nclass _FIELD_BASE:\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return self.name\n_FIELD = _FIELD_BASE('_FIELD')\n_FIELD_CLASSVAR = _FIELD_BASE('_FIELD_CLASSVAR')\n_FIELD_INITVAR = _FIELD_BASE('_FIELD_INITVAR')\n\n# The name of an attribute on the class where we store the Field\n# objects.  Also used to check if a class is a Data Class.\n_FIELDS = '__dataclass_fields__'\n\n# The name of an attribute on the class that stores the parameters to\n# @dataclass.\n_PARAMS = '__dataclass_params__'\n\n# The name of the function, that if it exists, is called at the end of\n# __init__.\n_POST_INIT_NAME = '__post_init__'\n\n# String regex that string annotations for ClassVar or InitVar must match.\n# Allows \"identifier.identifier[\" or \"identifier[\".\n# https://bugs.python.org/issue33453 for details.\n_MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n\n# Atomic immutable types which don't require any recursive handling and for which deepcopy\n# returns the same object. We can provide a fast-path for these types in asdict and astuple.\n_ATOMIC_TYPES = frozenset({\n    # Common JSON Serializable types\n    types.NoneType,\n    bool,\n    int,\n    float,\n    str,\n    # Other common types\n    complex,\n    bytes,\n    # Other types that are also unaffected by deepcopy\n    types.EllipsisType,\n    types.NotImplementedType,\n    types.CodeType,\n    types.BuiltinFunctionType,\n    types.FunctionType,\n    type,\n    range,\n    property,\n})\n\n# This function's logic is copied from \"recursive_repr\" function in\n# reprlib module to avoid dependency.\ndef _recursive_repr(user_function):\n    # Decorator to make a repr function return \"...\" for a recursive\n    # call.\n    repr_running = set()\n\n    @functools.wraps(user_function)\n    def wrapper(self):\n        key = id(self), _thread.get_ident()\n        if key in repr_running:\n            return '...'\n        repr_running.add(key)\n        try:\n            result = user_function(self)\n        finally:\n            repr_running.discard(key)\n        return result\n    return wrapper\n\nclass InitVar:\n    __slots__ = ('type', )\n\n    def __init__(self, type):\n        self.type = type\n\n    def __repr__(self):\n        if isinstance(self.type, type):\n            type_name = self.type.__name__\n        else:\n            # typing objects, e.g. List[int]\n            type_name = repr(self.type)\n        return f'dataclasses.InitVar[{type_name}]'\n\n    def __class_getitem__(cls, type):\n        return InitVar(type)\n\n# Instances of Field are only ever created from within this module,\n# and only from the field() function, although Field instances are\n# exposed externally as (conceptually) read-only objects.\n#\n# name and type are filled in after the fact, not in __init__.\n# They're not known at the time this class is instantiated, but it's\n# convenient if they're available later.\n#\n# When cls._FIELDS is filled in with a list of Field objects, the name\n# and type fields will have been populated.\nclass Field:\n    __slots__ = ('name',\n                 'type',\n                 'default',\n                 'default_factory',\n                 'repr',\n                 'hash',\n                 'init',\n                 'compare',\n                 'metadata',\n                 'kw_only',\n                 '_field_type',  # Private: not to be used by user code.\n                 )\n\n    def __init__(self, default, default_factory, init, repr, hash, compare,\n                 metadata, kw_only):\n        self.name = None\n        self.type = None\n        self.default = default\n        self.default_factory = default_factory\n        self.init = init\n        self.repr = repr\n        self.hash = hash\n        self.compare = compare\n        self.metadata = (_EMPTY_METADATA\n                         if metadata is None else\n                         types.MappingProxyType(metadata))\n        self.kw_only = kw_only\n        self._field_type = None\n\n    @_recursive_repr\n    def __repr__(self):\n        return ('Field('\n                f'name={self.name!r},'\n                f'type={self.type!r},'\n                f'default={self.default!r},'\n                f'default_factory={self.default_factory!r},'\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'hash={self.hash!r},'\n                f'compare={self.compare!r},'\n                f'metadata={self.metadata!r},'\n                f'kw_only={self.kw_only!r},'\n                f'_field_type={self._field_type}'\n                ')')\n\n    # This is used to support the PEP 487 __set_name__ protocol in the\n    # case where we're using a field that contains a descriptor as a\n    # default value.  For details on __set_name__, see\n    # https://peps.python.org/pep-0487/#implementation-details.\n    #\n    # Note that in _process_class, this Field object is overwritten\n    # with the default value, so the end result is a descriptor that\n    # had __set_name__ called on it at the right time.\n    def __set_name__(self, owner, name):\n        func = getattr(type(self.default), '__set_name__', None)\n        if func:\n            # There is a __set_name__ method on the descriptor, call\n            # it.\n            func(self.default, owner, name)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass _DataclassParams:\n    __slots__ = ('init',\n                 'repr',\n                 'eq',\n                 'order',\n                 'unsafe_hash',\n                 'frozen',\n                 'match_args',\n                 'kw_only',\n                 'slots',\n                 'weakref_slot',\n                 )\n\n    def __init__(self,\n                 init, repr, eq, order, unsafe_hash, frozen,\n                 match_args, kw_only, slots, weakref_slot):\n        self.init = init\n        self.repr = repr\n        self.eq = eq\n        self.order = order\n        self.unsafe_hash = unsafe_hash\n        self.frozen = frozen\n        self.match_args = match_args\n        self.kw_only = kw_only\n        self.slots = slots\n        self.weakref_slot = weakref_slot\n\n    def __repr__(self):\n        return ('_DataclassParams('\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'eq={self.eq!r},'\n                f'order={self.order!r},'\n                f'unsafe_hash={self.unsafe_hash!r},'\n                f'frozen={self.frozen!r},'\n                f'match_args={self.match_args!r},'\n                f'kw_only={self.kw_only!r},'\n                f'slots={self.slots!r},'\n                f'weakref_slot={self.weakref_slot!r}'\n                ')')\n\n\n# This function is used instead of exposing Field creation directly,\n# so that a type checker can be told (via overloads) that this is a\n# function whose type depends on its parameters.\ndef field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n          hash=None, compare=True, metadata=None, kw_only=MISSING):\n    \"\"\"Return an object to identify dataclass fields.\n\n    default is the default value of the field.  default_factory is a\n    0-argument function called to initialize a field's value.  If init\n    is true, the field will be a parameter to the class's __init__()\n    function.  If repr is true, the field will be included in the\n    object's repr().  If hash is true, the field will be included in the\n    object's hash().  If compare is true, the field will be used in\n    comparison functions.  metadata, if specified, must be a mapping\n    which is stored but not otherwise examined by dataclass.  If kw_only\n    is true, the field will become a keyword-only parameter to\n    __init__().\n\n    It is an error to specify both default and default_factory.\n    \"\"\"\n\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError('cannot specify both default and default_factory')\n    return Field(default, default_factory, init, repr, hash, compare,\n                 metadata, kw_only)\n\n\ndef _fields_in_init_order(fields):\n    # Returns the fields as __init__ will output them.  It returns 2 tuples:\n    # the first for normal args, and the second for keyword args.\n\n    return (tuple(f for f in fields if f.init and not f.kw_only),\n            tuple(f for f in fields if f.init and f.kw_only)\n            )\n\n\ndef _tuple_str(obj_name, fields):\n    # Return a string representing each field of obj_name as a tuple\n    # member.  So, if fields is ['x', 'y'] and obj_name is \"self\",\n    # return \"(self.x,self.y)\".\n\n    # Special case for the 0-tuple.\n    if not fields:\n        return '()'\n    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'\n\n\ndef _create_fn(name, args, body, *, globals=None, locals=None,\n               return_type=MISSING):\n    # Note that we may mutate locals. Callers beware!\n    # The only callers are internal to this module, so no\n    # worries about external callers.\n    if locals is None:\n        locals = {}\n    return_annotation = ''\n    if return_type is not MISSING:\n        locals['__dataclass_return_type__'] = return_type\n        return_annotation = '->__dataclass_return_type__'\n    args = ','.join(args)\n    body = '\\n'.join(f'  {b}' for b in body)\n\n    # Compute the text of the entire function.\n    txt = f' def {name}({args}){return_annotation}:\\n{body}'\n\n    # Free variables in exec are resolved in the global namespace.\n    # The global namespace we have is user-provided, so we can't modify it for\n    # our purposes. So we put the things we need into locals and introduce a\n    # scope to allow the function we're creating to close over them.\n    local_vars = ', '.join(locals.keys())\n    txt = f\"def __create_fn__({local_vars}):\\n{txt}\\n return {name}\"\n    ns = {}\n    exec(txt, globals, ns)\n    return ns['__create_fn__'](**locals)\n\n\ndef _field_assign(frozen, name, value, self_name):\n    # If we're a frozen class, then assign to our fields in __init__\n    # via object.__setattr__.  Otherwise, just use a simple\n    # assignment.\n    #\n    # self_name is what \"self\" is called in this function: don't\n    # hard-code \"self\", since that might be a field name.\n    if frozen:\n        return f'__dataclass_builtins_object__.__setattr__({self_name},{name!r},{value})'\n    return f'{self_name}.{name}={value}'\n\n\ndef _field_init(f, frozen, globals, self_name, slots):\n    # Return the text of the line in the body of __init__ that will\n    # initialize this field.\n\n    default_name = f'__dataclass_dflt_{f.name}__'\n    if f.default_factory is not MISSING:\n        if f.init:\n            # This field has a default factory.  If a parameter is\n            # given, use it.  If not, call the factory.\n            globals[default_name] = f.default_factory\n            value = (f'{default_name}() '\n                     f'if {f.name} is __dataclass_HAS_DEFAULT_FACTORY__ '\n                     f'else {f.name}')\n        else:\n            # This is a field that's not in the __init__ params, but\n            # has a default factory function.  It needs to be\n            # initialized here by calling the factory function,\n            # because there's no other way to initialize it.\n\n            # For a field initialized with a default=defaultvalue, the\n            # class dict just has the default value\n            # (cls.fieldname=defaultvalue).  But that won't work for a\n            # default factory, the factory must be called in __init__\n            # and we must assign that to self.fieldname.  We can't\n            # fall back to the class dict's value, both because it's\n            # not set, and because it might be different per-class\n            # (which, after all, is why we have a factory function!).\n\n            globals[default_name] = f.default_factory\n            value = f'{default_name}()'\n    else:\n        # No default factory.\n        if f.init:\n            if f.default is MISSING:\n                # There's no default, just do an assignment.\n                value = f.name\n            elif f.default is not MISSING:\n                globals[default_name] = f.default\n                value = f.name\n        else:\n            # If the class has slots, then initialize this field.\n            if slots and f.default is not MISSING:\n                globals[default_name] = f.default\n                value = default_name\n            else:\n                # This field does not need initialization: reading from it will\n                # just use the class attribute that contains the default.\n                # Signify that to the caller by returning None.\n                return None\n\n    # Only test this now, so that we can create variables for the\n    # default.  However, return None to signify that we're not going\n    # to actually do the assignment statement for InitVars.\n    if f._field_type is _FIELD_INITVAR:\n        return None\n\n    # Now, actually generate the field assignment.\n    return _field_assign(frozen, f.name, value, self_name)\n\n\ndef _init_param(f):\n    # Return the __init__ parameter string for this field.  For\n    # example, the equivalent of 'x:int=3' (except instead of 'int',\n    # reference a variable set to int, and instead of '3', reference a\n    # variable set to 3).\n    if f.default is MISSING and f.default_factory is MISSING:\n        # There's no default, and no default_factory, just output the\n        # variable name and type.\n        default = ''\n    elif f.default is not MISSING:\n        # There's a default, this will be the name that's used to look\n        # it up.\n        default = f'=__dataclass_dflt_{f.name}__'\n    elif f.default_factory is not MISSING:\n        # There's a factory function.  Set a marker.\n        default = '=__dataclass_HAS_DEFAULT_FACTORY__'\n    return f'{f.name}:__dataclass_type_{f.name}__{default}'\n\n\ndef _init_fn(fields, std_fields, kw_only_fields, frozen, has_post_init,\n             self_name, globals, slots):\n    # fields contains both real fields and InitVar pseudo-fields.\n\n    # Make sure we don't have fields without defaults following fields\n    # with defaults.  This actually would be caught when exec-ing the\n    # function source code, but catching it here gives a better error\n    # message, and future-proofs us in case we build up the function\n    # using ast.\n\n    seen_default = False\n    for f in std_fields:\n        # Only consider the non-kw-only fields in the __init__ call.\n        if f.init:\n            if not (f.default is MISSING and f.default_factory is MISSING):\n                seen_default = True\n            elif seen_default:\n                raise TypeError(f'non-default argument {f.name!r} '\n                                'follows default argument')\n\n    locals = {f'__dataclass_type_{f.name}__': f.type for f in fields}\n    locals.update({\n        '__dataclass_HAS_DEFAULT_FACTORY__': _HAS_DEFAULT_FACTORY,\n        '__dataclass_builtins_object__': object,\n    })\n\n    body_lines = []\n    for f in fields:\n        line = _field_init(f, frozen, locals, self_name, slots)\n        # line is None means that this field doesn't require\n        # initialization (it's a pseudo-field).  Just skip it.\n        if line:\n            body_lines.append(line)\n\n    # Does this class have a post-init function?\n    if has_post_init:\n        params_str = ','.join(f.name for f in fields\n                              if f._field_type is _FIELD_INITVAR)\n        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\n\n    # If no body lines, use 'pass'.\n    if not body_lines:\n        body_lines = ['pass']\n\n    _init_params = [_init_param(f) for f in std_fields]\n    if kw_only_fields:\n        # Add the keyword-only args.  Because the * can only be added if\n        # there's at least one keyword-only arg, there needs to be a test here\n        # (instead of just concatenting the lists together).\n        _init_params += ['*']\n        _init_params += [_init_param(f) for f in kw_only_fields]\n    return _create_fn('__init__',\n                      [self_name] + _init_params,\n                      body_lines,\n                      locals=locals,\n                      globals=globals,\n                      return_type=None)\n\n\ndef _repr_fn(fields, globals):\n    fn = _create_fn('__repr__',\n                    ('self',),\n                    ['return self.__class__.__qualname__ + f\"(' +\n                     ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\n                                for f in fields]) +\n                     ')\"'],\n                     globals=globals)\n    return _recursive_repr(fn)\n\n\ndef _frozen_get_del_attr(cls, fields, globals):\n    locals = {'cls': cls,\n              'FrozenInstanceError': FrozenInstanceError}\n    condition = 'type(self) is cls'\n    if fields:\n        condition += ' or name in {' + ', '.join(repr(f.name) for f in fields) + '}'\n    return (_create_fn('__setattr__',\n                      ('self', 'name', 'value'),\n                      (f'if {condition}:',\n                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\n                       f'super(cls, self).__setattr__(name, value)'),\n                       locals=locals,\n                       globals=globals),\n            _create_fn('__delattr__',\n                      ('self', 'name'),\n                      (f'if {condition}:',\n                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\n                       f'super(cls, self).__delattr__(name)'),\n                       locals=locals,\n                       globals=globals),\n            )\n\n\ndef _cmp_fn(name, op, self_tuple, other_tuple, globals):\n    # Create a comparison function.  If the fields in the object are\n    # named 'x' and 'y', then self_tuple is the string\n    # '(self.x,self.y)' and other_tuple is the string\n    # '(other.x,other.y)'.\n\n    return _create_fn(name,\n                      ('self', 'other'),\n                      [ 'if other.__class__ is self.__class__:',\n                       f' return {self_tuple}{op}{other_tuple}',\n                        'return NotImplemented'],\n                      globals=globals)\n\n\ndef _hash_fn(fields, globals):\n    self_tuple = _tuple_str('self', fields)\n    return _create_fn('__hash__',\n                      ('self',),\n                      [f'return hash({self_tuple})'],\n                      globals=globals)\n\n\ndef _is_classvar(a_type, typing):\n    # This test uses a typing internal class, but it's the best way to\n    # test if this is a ClassVar.\n    return (a_type is typing.ClassVar\n            or (type(a_type) is typing._GenericAlias\n                and a_type.__origin__ is typing.ClassVar))\n\n\ndef _is_initvar(a_type, dataclasses):\n    # The module we're checking against is the module we're\n    # currently in (dataclasses.py).\n    return (a_type is dataclasses.InitVar\n            or type(a_type) is dataclasses.InitVar)\n\ndef _is_kw_only(a_type, dataclasses):\n    return a_type is dataclasses.KW_ONLY\n\n\ndef _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n    # Given a type annotation string, does it refer to a_type in\n    # a_module?  For example, when checking that annotation denotes a\n    # ClassVar, then a_module is typing, and a_type is\n    # typing.ClassVar.\n\n    # It's possible to look up a_module given a_type, but it involves\n    # looking in sys.modules (again!), and seems like a waste since\n    # the caller already knows a_module.\n\n    # - annotation is a string type annotation\n    # - cls is the class that this annotation was found in\n    # - a_module is the module we want to match\n    # - a_type is the type in that module we want to match\n    # - is_type_predicate is a function called with (obj, a_module)\n    #   that determines if obj is of the desired type.\n\n    # Since this test does not do a local namespace lookup (and\n    # instead only a module (global) lookup), there are some things it\n    # gets wrong.\n\n    # With string annotations, cv0 will be detected as a ClassVar:\n    #   CV = ClassVar\n    #   @dataclass\n    #   class C0:\n    #     cv0: CV\n\n    # But in this example cv1 will not be detected as a ClassVar:\n    #   @dataclass\n    #   class C1:\n    #     CV = ClassVar\n    #     cv1: CV\n\n    # In C1, the code in this function (_is_type) will look up \"CV\" in\n    # the module and not find it, so it will not consider cv1 as a\n    # ClassVar.  This is a fairly obscure corner case, and the best\n    # way to fix it would be to eval() the string \"CV\" with the\n    # correct global and local namespaces.  However that would involve\n    # a eval() penalty for every single field of every dataclass\n    # that's defined.  It was judged not worth it.\n\n    match = _MODULE_IDENTIFIER_RE.match(annotation)\n    if match:\n        ns = None\n        module_name = match.group(1)\n        if not module_name:\n            # No module name, assume the class's module did\n            # \"from dataclasses import InitVar\".\n            ns = sys.modules.get(cls.__module__).__dict__\n        else:\n            # Look up module_name in the class's module.\n            module = sys.modules.get(cls.__module__)\n            if module and module.__dict__.get(module_name) is a_module:\n                ns = sys.modules.get(a_type.__module__).__dict__\n        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n            return True\n    return False\n\n\ndef _get_field(cls, a_name, a_type, default_kw_only):\n    # Return a Field object for this field name and type.  ClassVars and\n    # InitVars are also returned, but marked as such (see f._field_type).\n    # default_kw_only is the value of kw_only to use if there isn't a field()\n    # that defines it.\n\n    # If the default value isn't derived from Field, then it's only a\n    # normal default value.  Convert it to a Field().\n    default = getattr(cls, a_name, MISSING)\n    if isinstance(default, Field):\n        f = default\n    else:\n        if isinstance(default, types.MemberDescriptorType):\n            # This is a field in __slots__, so it has no default value.\n            default = MISSING\n        f = field(default=default)\n\n    # Only at this point do we know the name and the type.  Set them.\n    f.name = a_name\n    f.type = a_type\n\n    # Assume it's a normal field until proven otherwise.  We're next\n    # going to decide if it's a ClassVar or InitVar, everything else\n    # is just a normal field.\n    f._field_type = _FIELD\n\n    # In addition to checking for actual types here, also check for\n    # string annotations.  get_type_hints() won't always work for us\n    # (see https://github.com/python/typing/issues/508 for example),\n    # plus it's expensive and would require an eval for every string\n    # annotation.  So, make a best effort to see if this is a ClassVar\n    # or InitVar using regex's and checking that the thing referenced\n    # is actually of the correct type.\n\n    # For the complete discussion, see https://bugs.python.org/issue33453\n\n    # If typing has not been imported, then it's impossible for any\n    # annotation to be a ClassVar.  So, only look for ClassVar if\n    # typing has been imported by any module (not necessarily cls's\n    # module).\n    typing = sys.modules.get('typing')\n    if typing:\n        if (_is_classvar(a_type, typing)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, typing, typing.ClassVar,\n                             _is_classvar))):\n            f._field_type = _FIELD_CLASSVAR\n\n    # If the type is InitVar, or if it's a matching string annotation,\n    # then it's an InitVar.\n    if f._field_type is _FIELD:\n        # The module we're checking against is the module we're\n        # currently in (dataclasses.py).\n        dataclasses = sys.modules[__name__]\n        if (_is_initvar(a_type, dataclasses)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n                             _is_initvar))):\n            f._field_type = _FIELD_INITVAR\n\n    # Validations for individual fields.  This is delayed until now,\n    # instead of in the Field() constructor, since only here do we\n    # know the field name, which allows for better error reporting.\n\n    # Special restrictions for ClassVar and InitVar.\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n        if f.default_factory is not MISSING:\n            raise TypeError(f'field {f.name} cannot have a '\n                            'default factory')\n        # Should I check for other field settings? default_factory\n        # seems the most serious to check for.  Maybe add others.  For\n        # example, how about init=False (or really,\n        # init=<not-the-default-init-value>)?  It makes no sense for\n        # ClassVar and InitVar to specify init=<anything>.\n\n    # kw_only validation and assignment.\n    if f._field_type in (_FIELD, _FIELD_INITVAR):\n        # For real and InitVar fields, if kw_only wasn't specified use the\n        # default value.\n        if f.kw_only is MISSING:\n            f.kw_only = default_kw_only\n    else:\n        # Make sure kw_only isn't set for ClassVars\n        assert f._field_type is _FIELD_CLASSVAR\n        if f.kw_only is not MISSING:\n            raise TypeError(f'field {f.name} is a ClassVar but specifies '\n                            'kw_only')\n\n    # For real fields, disallow mutable defaults.  Use unhashable as a proxy\n    # indicator for mutability.  Read the __hash__ attribute from the class,\n    # not the instance.\n    if f._field_type is _FIELD and f.default.__class__.__hash__ is None:\n        raise ValueError(f'mutable default {type(f.default)} for field '\n                         f'{f.name} is not allowed: use default_factory')\n\n    return f\n\ndef _set_qualname(cls, value):\n    # Ensure that the functions returned from _create_fn uses the proper\n    # __qualname__ (the class they belong to).\n    if isinstance(value, FunctionType):\n        value.__qualname__ = f\"{cls.__qualname__}.{value.__name__}\"\n    return value\n\ndef _set_new_attribute(cls, name, value):\n    # Never overwrites an existing attribute.  Returns True if the\n    # attribute already exists.\n    if name in cls.__dict__:\n        return True\n    _set_qualname(cls, value)\n    setattr(cls, name, value)\n    return False\n\n\n# Decide if/how we're going to create a hash function.  Key is\n# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n# take.  The common case is to do nothing, so instead of providing a\n# function that is a no-op, use None to signify that.\n\ndef _hash_set_none(cls, fields, globals):\n    return None\n\ndef _hash_add(cls, fields, globals):\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n    return _set_qualname(cls, _hash_fn(flds, globals))\n\ndef _hash_exception(cls, fields, globals):\n    # Raise an exception.\n    raise TypeError(f'Cannot overwrite attribute __hash__ '\n                    f'in class {cls.__name__}')\n\n#\n#                +-------------------------------------- unsafe_hash?\n#                |      +------------------------------- eq?\n#                |      |      +------------------------ frozen?\n#                |      |      |      +----------------  has-explicit-hash?\n#                |      |      |      |\n#                |      |      |      |        +-------  action\n#                |      |      |      |        |\n#                v      v      v      v        v\n_hash_action = {(False, False, False, False): None,\n                (False, False, False, True ): None,\n                (False, False, True,  False): None,\n                (False, False, True,  True ): None,\n                (False, True,  False, False): _hash_set_none,\n                (False, True,  False, True ): None,\n                (False, True,  True,  False): _hash_add,\n                (False, True,  True,  True ): None,\n                (True,  False, False, False): _hash_add,\n                (True,  False, False, True ): _hash_exception,\n                (True,  False, True,  False): _hash_add,\n                (True,  False, True,  True ): _hash_exception,\n                (True,  True,  False, False): _hash_add,\n                (True,  True,  False, True ): _hash_exception,\n                (True,  True,  True,  False): _hash_add,\n                (True,  True,  True,  True ): _hash_exception,\n                }\n# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n# version of this table.\n\n\ndef _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\n                   match_args, kw_only, slots, weakref_slot):\n    # Now that dicts retain insertion order, there's no reason to use\n    # an ordered dict.  I am leveraging that ordering here, because\n    # derived class fields overwrite base class fields, but the order\n    # is defined by the base class, which is found first.\n    fields = {}\n\n    if cls.__module__ in sys.modules:\n        globals = sys.modules[cls.__module__].__dict__\n    else:\n        # Theoretically this can happen if someone writes\n        # a custom string to cls.__module__.  In which case\n        # such dataclass won't be fully introspectable\n        # (w.r.t. typing.get_type_hints) but will still function\n        # correctly.\n        globals = {}\n\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n                                           unsafe_hash, frozen,\n                                           match_args, kw_only,\n                                           slots, weakref_slot))\n\n    # Find our base classes in reverse MRO order, and exclude\n    # ourselves.  In reversed order so that more derived classes\n    # override earlier field definitions in base classes.  As long as\n    # we're iterating over them, see if any are frozen.\n    any_frozen_base = False\n    has_dataclass_bases = False\n    for b in cls.__mro__[-1:0:-1]:\n        # Only process classes that have been processed by our\n        # decorator.  That is, they have a _FIELDS attribute.\n        base_fields = getattr(b, _FIELDS, None)\n        if base_fields is not None:\n            has_dataclass_bases = True\n            for f in base_fields.values():\n                fields[f.name] = f\n            if getattr(b, _PARAMS).frozen:\n                any_frozen_base = True\n\n    # Annotations defined specifically in this class (not in base classes).\n    #\n    # Fields are found from cls_annotations, which is guaranteed to be\n    # ordered.  Default values are from class attributes, if a field\n    # has a default.  If the default value is a Field(), then it\n    # contains additional info beyond (and possibly including) the\n    # actual default value.  Pseudo-fields ClassVars and InitVars are\n    # included, despite the fact that they're not real fields.  That's\n    # dealt with later.\n    cls_annotations = inspect.get_annotations(cls)\n\n    # Now find fields in our class.  While doing so, validate some\n    # things, and set the default values (as class attributes) where\n    # we can.\n    cls_fields = []\n    # Get a reference to this module for the _is_kw_only() test.\n    KW_ONLY_seen = False\n    dataclasses = sys.modules[__name__]\n    for name, type in cls_annotations.items():\n        # See if this is a marker to change the value of kw_only.\n        if (_is_kw_only(type, dataclasses)\n            or (isinstance(type, str)\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\n                             _is_kw_only))):\n            # Switch the default to kw_only=True, and ignore this\n            # annotation: it's not a real field.\n            if KW_ONLY_seen:\n                raise TypeError(f'{name!r} is KW_ONLY, but KW_ONLY '\n                                'has already been specified')\n            KW_ONLY_seen = True\n            kw_only = True\n        else:\n            # Otherwise it's a field of some type.\n            cls_fields.append(_get_field(cls, name, type, kw_only))\n\n    for f in cls_fields:\n        fields[f.name] = f\n\n        # If the class attribute (which is the default value for this\n        # field) exists and is of type 'Field', replace it with the\n        # real default.  This is so that normal class introspection\n        # sees a real default value, not a Field.\n        if isinstance(getattr(cls, f.name, None), Field):\n            if f.default is MISSING:\n                # If there's no default, delete the class attribute.\n                # This happens if we specify field(repr=False), for\n                # example (that is, we specified a field object, but\n                # no default value).  Also if we're using a default\n                # factory.  The class attribute should not be set at\n                # all in the post-processed class.\n                delattr(cls, f.name)\n            else:\n                setattr(cls, f.name, f.default)\n\n    # Do we have any Field members that don't also have annotations?\n    for name, value in cls.__dict__.items():\n        if isinstance(value, Field) and not name in cls_annotations:\n            raise TypeError(f'{name!r} is a field but has no type annotation')\n\n    # Check rules that apply if we are derived from any dataclasses.\n    if has_dataclass_bases:\n        # Raise an exception if any of our bases are frozen, but we're not.\n        if any_frozen_base and not frozen:\n            raise TypeError('cannot inherit non-frozen dataclass from a '\n                            'frozen one')\n\n        # Raise an exception if we're frozen, but none of our bases are.\n        if not any_frozen_base and frozen:\n            raise TypeError('cannot inherit frozen dataclass from a '\n                            'non-frozen one')\n\n    # Remember all of the fields on our class (including bases).  This\n    # also marks this class as being a dataclass.\n    setattr(cls, _FIELDS, fields)\n\n    # Was this class defined with an explicit __hash__?  Note that if\n    # __eq__ is defined in this class, then python will automatically\n    # set __hash__ to None.  This is a heuristic, as it's possible\n    # that such a __hash__ == None was not auto-generated, but it\n    # close enough.\n    class_hash = cls.__dict__.get('__hash__', MISSING)\n    has_explicit_hash = not (class_hash is MISSING or\n                             (class_hash is None and '__eq__' in cls.__dict__))\n\n    # If we're generating ordering methods, we must be generating the\n    # eq methods.\n    if order and not eq:\n        raise ValueError('eq must be true if order is true')\n\n    # Include InitVars and regular fields (so, not ClassVars).  This is\n    # initialized here, outside of the \"if init:\" test, because std_init_fields\n    # is used with match_args, below.\n    all_init_fields = [f for f in fields.values()\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\n    (std_init_fields,\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\n\n    if init:\n        # Does this class have a post-init function?\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\n\n        _set_new_attribute(cls, '__init__',\n                           _init_fn(all_init_fields,\n                                    std_init_fields,\n                                    kw_only_init_fields,\n                                    frozen,\n                                    has_post_init,\n                                    # The name to use for the \"self\"\n                                    # param in __init__.  Use \"self\"\n                                    # if possible.\n                                    '__dataclass_self__' if 'self' in fields\n                                            else 'self',\n                                    globals,\n                                    slots,\n                          ))\n\n    # Get the fields as a list, and include only real fields.  This is\n    # used in all of the following methods.\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n\n    if repr:\n        flds = [f for f in field_list if f.repr]\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\n\n    if eq:\n        # Create __eq__ method.  There's no need for a __ne__ method,\n        # since python will call __eq__ and negate it.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        _set_new_attribute(cls, '__eq__',\n                           _cmp_fn('__eq__', '==',\n                                   self_tuple, other_tuple,\n                                   globals=globals))\n\n    if order:\n        # Create and set the ordering methods.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        for name, op in [('__lt__', '<'),\n                         ('__le__', '<='),\n                         ('__gt__', '>'),\n                         ('__ge__', '>='),\n                         ]:\n            if _set_new_attribute(cls, name,\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\n                                          globals=globals)):\n                raise TypeError(f'Cannot overwrite attribute {name} '\n                                f'in class {cls.__name__}. Consider using '\n                                'functools.total_ordering')\n\n    if frozen:\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\n            if _set_new_attribute(cls, fn.__name__, fn):\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\n                                f'in class {cls.__name__}')\n\n    # Decide if/how we're going to create a hash function.\n    hash_action = _hash_action[bool(unsafe_hash),\n                               bool(eq),\n                               bool(frozen),\n                               has_explicit_hash]\n    if hash_action:\n        # No need to call _set_new_attribute here, since by the time\n        # we're here the overwriting is unconditional.\n        cls.__hash__ = hash_action(cls, field_list, globals)\n\n    if not getattr(cls, '__doc__'):\n        # Create a class doc-string.\n        try:\n            # In some cases fetching a signature is not possible.\n            # But, we surely should not fail in this case.\n            text_sig = str(inspect.signature(cls)).replace(' -> None', '')\n        except (TypeError, ValueError):\n            text_sig = ''\n        cls.__doc__ = (cls.__name__ + text_sig)\n\n    if match_args:\n        # I could probably compute this once\n        _set_new_attribute(cls, '__match_args__',\n                           tuple(f.name for f in std_init_fields))\n\n    # It's an error to specify weakref_slot if slots is False.\n    if weakref_slot and not slots:\n        raise TypeError('weakref_slot is True but slots is False')\n    if slots:\n        cls = _add_slots(cls, frozen, weakref_slot)\n\n    abc.update_abstractmethods(cls)\n\n    return cls\n\n\n# _dataclass_getstate and _dataclass_setstate are needed for pickling frozen\n# classes with slots.  These could be slightly more performant if we generated\n# the code instead of iterating over fields.  But that can be a project for\n# another day, if performance becomes an issue.\ndef _dataclass_getstate(self):\n    return [getattr(self, f.name) for f in fields(self)]\n\n\ndef _dataclass_setstate(self, state):\n    for field, value in zip(fields(self), state):\n        # use setattr because dataclass may be frozen\n        object.__setattr__(self, field.name, value)\n\n\ndef _get_slots(cls):\n    match cls.__dict__.get('__slots__'):\n        # `__dictoffset__` and `__weakrefoffset__` can tell us whether\n        # the base type has dict/weakref slots, in a way that works correctly\n        # for both Python classes and C extension types. Extension types\n        # don't use `__slots__` for slot creation\n        case None:\n            slots = []\n            if getattr(cls, '__weakrefoffset__', -1) != 0:\n                slots.append('__weakref__')\n            if getattr(cls, '__dictoffset__', -1) != 0:\n                slots.append('__dict__')\n            yield from slots\n        case str(slot):\n            yield slot\n        # Slots may be any iterable, but we cannot handle an iterator\n        # because it will already be (partially) consumed.\n        case iterable if not hasattr(iterable, '__next__'):\n            yield from iterable\n        case _:\n            raise TypeError(f\"Slots of '{cls.__name__}' cannot be determined\")\n\n\ndef _add_slots(cls, is_frozen, weakref_slot):\n    # Need to create a new class, since we can't set __slots__\n    #  after a class has been created.\n\n    # Make sure __slots__ isn't already set.\n    if '__slots__' in cls.__dict__:\n        raise TypeError(f'{cls.__name__} already specifies __slots__')\n\n    # Create a new dict for our new class.\n    cls_dict = dict(cls.__dict__)\n    field_names = tuple(f.name for f in fields(cls))\n    # Make sure slots don't overlap with those in base classes.\n    inherited_slots = set(\n        itertools.chain.from_iterable(map(_get_slots, cls.__mro__[1:-1]))\n    )\n    # The slots for our class.  Remove slots from our base classes.  Add\n    # '__weakref__' if weakref_slot was given, unless it is already present.\n    cls_dict[\"__slots__\"] = tuple(\n        itertools.filterfalse(\n            inherited_slots.__contains__,\n            itertools.chain(\n                # gh-93521: '__weakref__' also needs to be filtered out if\n                # already present in inherited_slots\n                field_names, ('__weakref__',) if weakref_slot else ()\n            )\n        ),\n    )\n\n    for field_name in field_names:\n        # Remove our attributes, if present. They'll still be\n        #  available in _MARKER.\n        cls_dict.pop(field_name, None)\n\n    # Remove __dict__ itself.\n    cls_dict.pop('__dict__', None)\n\n    # Clear existing `__weakref__` descriptor, it belongs to a previous type:\n    cls_dict.pop('__weakref__', None)  # gh-102069\n\n    # And finally create the class.\n    qualname = getattr(cls, '__qualname__', None)\n    cls = type(cls)(cls.__name__, cls.__bases__, cls_dict)\n    if qualname is not None:\n        cls.__qualname__ = qualname\n\n    if is_frozen:\n        # Need this for pickling frozen classes with slots.\n        if '__getstate__' not in cls_dict:\n            cls.__getstate__ = _dataclass_getstate\n        if '__setstate__' not in cls_dict:\n            cls.__setstate__ = _dataclass_setstate\n\n    return cls\n\n\ndef dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False, match_args=True,\n              kw_only=False, slots=False, weakref_slot=False):\n    \"\"\"Add dunder methods based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If repr\n    is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method is added. If frozen is true, fields may not be\n    assigned to after instance creation. If match_args is true, the\n    __match_args__ tuple is added. If kw_only is true, then by default\n    all fields are keyword-only. If slots is true, a new class with a\n    __slots__ attribute is returned.\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash,\n                              frozen, match_args, kw_only, slots,\n                              weakref_slot)\n\n    # See if we're being called as @dataclass or @dataclass().\n    if cls is None:\n        # We're called with parens.\n        return wrap\n\n    # We're called as @dataclass without parens.\n    return wrap(cls)\n\n\ndef fields(class_or_instance):\n    \"\"\"Return a tuple describing the fields of this dataclass.\n\n    Accepts a dataclass or an instance of one. Tuple elements are of\n    type Field.\n    \"\"\"\n\n    # Might it be worth caching this, per class?\n    try:\n        fields = getattr(class_or_instance, _FIELDS)\n    except AttributeError:\n        raise TypeError('must be called with a dataclass type or instance') from None\n\n    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n    # order, so the order of the tuple is as the fields were defined.\n    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\n    return hasattr(type(obj), _FIELDS)\n\n\ndef is_dataclass(obj):\n    \"\"\"Returns True if obj is a dataclass or an instance of a\n    dataclass.\"\"\"\n    cls = obj if isinstance(obj, type) else type(obj)\n    return hasattr(cls, _FIELDS)\n\n\ndef asdict(obj, *, dict_factory=dict):\n    \"\"\"Return the fields of a dataclass instance as a new dictionary mapping\n    field names to field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert asdict(c) == {'x': 1, 'y': 2}\n\n    If given, 'dict_factory' will be used instead of built-in dict.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts. Other objects are copied with 'copy.deepcopy()'.\n    \"\"\"\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n    return _asdict_inner(obj, dict_factory)\n\n\ndef _asdict_inner(obj, dict_factory):\n    if type(obj) in _ATOMIC_TYPES:\n        return obj\n    elif _is_dataclass_instance(obj):\n        # fast path for the common case\n        if dict_factory is dict:\n            return {\n                f.name: _asdict_inner(getattr(obj, f.name), dict)\n                for f in fields(obj)\n            }\n        else:\n            result = []\n            for f in fields(obj):\n                value = _asdict_inner(getattr(obj, f.name), dict_factory)\n                result.append((f.name, value))\n            return dict_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n\n        # I'm not using namedtuple's _asdict()\n        # method, because:\n        # - it does not recurse in to the namedtuple fields and\n        #   convert them to dicts (using dict_factory).\n        # - I don't actually want to return a dict here.  The main\n        #   use case here is json.dumps, and it handles converting\n        #   namedtuples to lists.  Admittedly we're losing some\n        #   information here when we produce a json list instead of a\n        #   dict.  Note that if we returned dicts here instead of\n        #   namedtuples, we could no longer call asdict() on a data\n        #   structure where a namedtuple was used as a dict key.\n\n        return type(obj)(*[_asdict_inner(v, dict_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)\n    elif isinstance(obj, dict):\n        if hasattr(type(obj), 'default_factory'):\n            # obj is a defaultdict, which has a different constructor from\n            # dict as it requires the default_factory as its first arg.\n            result = type(obj)(getattr(obj, 'default_factory'))\n            for k, v in obj.items():\n                result[_asdict_inner(k, dict_factory)] = _asdict_inner(v, dict_factory)\n            return result\n        return type(obj)((_asdict_inner(k, dict_factory),\n                          _asdict_inner(v, dict_factory))\n                         for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef astuple(obj, *, tuple_factory=tuple):\n    \"\"\"Return the fields of a dataclass instance as a new tuple of field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert astuple(c) == (1, 2)\n\n    If given, 'tuple_factory' will be used instead of built-in tuple.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts. Other objects are copied with 'copy.deepcopy()'.\n    \"\"\"\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"astuple() should be called on dataclass instances\")\n    return _astuple_inner(obj, tuple_factory)\n\n\ndef _astuple_inner(obj, tuple_factory):\n    if type(obj) in _ATOMIC_TYPES:\n        return obj\n    elif _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _astuple_inner(getattr(obj, f.name), tuple_factory)\n            result.append(value)\n        return tuple_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n    elif isinstance(obj, dict):\n        obj_type = type(obj)\n        if hasattr(obj_type, 'default_factory'):\n            # obj is a defaultdict, which has a different constructor from\n            # dict as it requires the default_factory as its first arg.\n            result = obj_type(getattr(obj, 'default_factory'))\n            for k, v in obj.items():\n                result[_astuple_inner(k, tuple_factory)] = _astuple_inner(v, tuple_factory)\n            return result\n        return obj_type((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n                          for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n                   repr=True, eq=True, order=False, unsafe_hash=False,\n                   frozen=False, match_args=True, kw_only=False, slots=False,\n                   weakref_slot=False, module=None):\n    \"\"\"Return a new dynamically created dataclass.\n\n    The dataclass name will be 'cls_name'.  'fields' is an iterable\n    of either (name), (name, type) or (name, type, Field) objects. If type is\n    omitted, use the string 'typing.Any'.  Field objects are created by\n    the equivalent of calling 'field(name, type [, Field-info])'.::\n\n      C = make_dataclass('C', ['x', ('y', int), ('z', int, field(init=False))], bases=(Base,))\n\n    is equivalent to::\n\n      @dataclass\n      class C(Base):\n          x: 'typing.Any'\n          y: int\n          z: int = field(init=False)\n\n    For the bases and namespace parameters, see the builtin type() function.\n\n    The parameters init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only,\n    slots, and weakref_slot are passed to dataclass().\n\n    If module parameter is defined, the '__module__' attribute of the dataclass is\n    set to that value.\n    \"\"\"\n\n    if namespace is None:\n        namespace = {}\n\n    # While we're looking through the field names, validate that they\n    # are identifiers, are not keywords, and not duplicates.\n    seen = set()\n    annotations = {}\n    defaults = {}\n    for item in fields:\n        if isinstance(item, str):\n            name = item\n            tp = 'typing.Any'\n        elif len(item) == 2:\n            name, tp, = item\n        elif len(item) == 3:\n            name, tp, spec = item\n            defaults[name] = spec\n        else:\n            raise TypeError(f'Invalid field: {item!r}')\n\n        if not isinstance(name, str) or not name.isidentifier():\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise TypeError(f'Field names must not be keywords: {name!r}')\n        if name in seen:\n            raise TypeError(f'Field name duplicated: {name!r}')\n\n        seen.add(name)\n        annotations[name] = tp\n\n    # Update 'ns' with the user-supplied namespace plus our calculated values.\n    def exec_body_callback(ns):\n        ns.update(namespace)\n        ns.update(defaults)\n        ns['__annotations__'] = annotations\n\n    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n    # of generic dataclasses.\n    cls = types.new_class(cls_name, bases, {}, exec_body_callback)\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the dataclass is created.\n    if module is None:\n        try:\n            module = sys._getframemodulename(1) or '__main__'\n        except AttributeError:\n            try:\n                module = sys._getframe(1).f_globals.get('__name__', '__main__')\n            except (AttributeError, ValueError):\n                pass\n    if module is not None:\n        cls.__module__ = module\n\n    # Apply the normal decorator.\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n                     unsafe_hash=unsafe_hash, frozen=frozen,\n                     match_args=match_args, kw_only=kw_only, slots=slots,\n                     weakref_slot=weakref_slot)\n\n\ndef replace(obj, /, **changes):\n    \"\"\"Return a new object replacing specified fields with new values.\n\n    This is especially useful for frozen classes.  Example usage::\n\n      @dataclass(frozen=True)\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      c1 = replace(c, x=3)\n      assert c1.x == 3 and c1.y == 2\n    \"\"\"\n\n    # We're going to mutate 'changes', but that's okay because it's a\n    # new dict, even if called with 'replace(obj, **my_changes)'.\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"replace() should be called on dataclass instances\")\n\n    # It's an error to have init=False fields in 'changes'.\n    # If a field is not in 'changes', read its value from the provided obj.\n\n    for f in getattr(obj, _FIELDS).values():\n        # Only consider normal fields or InitVars.\n        if f._field_type is _FIELD_CLASSVAR:\n            continue\n\n        if not f.init:\n            # Error if this field is specified in changes.\n            if f.name in changes:\n                raise ValueError(f'field {f.name} is declared with '\n                                 'init=False, it cannot be specified with '\n                                 'replace()')\n            continue\n\n        if f.name not in changes:\n            if f._field_type is _FIELD_INITVAR and f.default is MISSING:\n                raise ValueError(f\"InitVar {f.name!r} \"\n                                 'must be specified with replace()')\n            changes[f.name] = getattr(obj, f.name)\n\n    # Create the new object, which calls __init__() and\n    # __post_init__() (if defined), using all of the init fields we've\n    # added and/or left in 'changes'.  If there are values supplied in\n    # changes that aren't fields, this will correctly raise a\n    # TypeError.\n    return obj.__class__(**changes)\n", 1588], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\urllib\\parse.py": ["\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zawinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\nThe WHATWG URL Parser spec should also be considered.  We are not compliant with\nit either due to existing user code API behavior expectations (Hyrum's Law).\nIt serves as a useful guide when making changes.\n\"\"\"\n\nfrom collections import namedtuple\nimport functools\nimport math\nimport re\nimport types\nimport warnings\nimport ipaddress\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"urlencode\", \"parse_qs\",\n           \"parse_qsl\", \"quote\", \"quote_plus\", \"quote_from_bytes\",\n           \"unquote\", \"unquote_plus\", \"unquote_to_bytes\",\n           \"DefragResult\", \"ParseResult\", \"SplitResult\",\n           \"DefragResultBytes\", \"ParseResultBytes\", \"SplitResultBytes\"]\n\n# A classification of schemes.\n# The empty string classifies URLs with no scheme specified,\n# being the default value returned by \u201curlsplit\u201d and \u201curlparse\u201d.\n\nuses_relative = ['', 'ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtsps', 'rtspu', 'sftp',\n                 'svn', 'svn+ssh', 'ws', 'wss']\n\nuses_netloc = ['', 'ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtsps', 'rtspu', 'rsync',\n               'svn', 'svn+ssh', 'sftp', 'nfs', 'git', 'git+ssh',\n               'ws', 'wss', 'itms-services']\n\nuses_params = ['', 'ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtsps', 'rtspu', 'sip',\n               'sips', 'mms', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\n\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\n\nuses_query = ['', 'http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtsps', 'rtspu', 'sip', 'sips']\n\nuses_fragment = ['', 'ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\n# Leading and trailing C0 control and space to be stripped per WHATWG spec.\n# == \"\".join([chr(i) for i in range(0, 0x20 + 1)])\n_WHATWG_C0_CONTROL_OR_SPACE = '\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f '\n\n# Unsafe bytes to be removed per WHATWG spec\n_UNSAFE_URL_BYTES_TO_REMOVE = ['\\t', '\\r', '\\n']\n\ndef clear_cache():\n    \"\"\"Clear internal performance caches. Undocumented; some tests want it.\"\"\"\n    urlsplit.cache_clear()\n    _byte_quoter_factory.cache_clear()\n\n# Helpers for bytes handling\n# For 3.2, we deliberately require applications that\n# handle improperly quoted URLs to do their own\n# decoding and encoding. If valid use cases are\n# presented, we may relax this by using latin-1\n# decoding internally for 3.3\n_implicit_encoding = 'ascii'\n_implicit_errors = 'strict'\n\ndef _noop(obj):\n    return obj\n\ndef _encode_result(obj, encoding=_implicit_encoding,\n                        errors=_implicit_errors):\n    return obj.encode(encoding, errors)\n\ndef _decode_args(args, encoding=_implicit_encoding,\n                       errors=_implicit_errors):\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n\ndef _coerce_args(*args):\n    # Invokes decode if necessary to create str args\n    # and returns the coerced inputs along with\n    # an appropriate result coercion function\n    #   - noop for str inputs\n    #   - encoding function otherwise\n    str_input = isinstance(args[0], str)\n    for arg in args[1:]:\n        # We special-case the empty string to support the\n        # \"scheme=''\" default argument to some functions\n        if arg and isinstance(arg, str) != str_input:\n            raise TypeError(\"Cannot mix str and non-str arguments\")\n    if str_input:\n        return args + (_noop,)\n    return _decode_args(args) + (_encode_result,)\n\n# Result objects are more helpful than simple tuples\nclass _ResultMixinStr(object):\n    \"\"\"Standard approach to encoding parsed results from str to bytes\"\"\"\n    __slots__ = ()\n\n    def encode(self, encoding='ascii', errors='strict'):\n        return self._encoded_counterpart(*(x.encode(encoding, errors) for x in self))\n\n\nclass _ResultMixinBytes(object):\n    \"\"\"Standard approach to decoding parsed results from bytes to str\"\"\"\n    __slots__ = ()\n\n    def decode(self, encoding='ascii', errors='strict'):\n        return self._decoded_counterpart(*(x.decode(encoding, errors) for x in self))\n\n\nclass _NetlocResultMixinBase(object):\n    \"\"\"Shared methods for the parsed result objects containing a netloc element\"\"\"\n    __slots__ = ()\n\n    @property\n    def username(self):\n        return self._userinfo[0]\n\n    @property\n    def password(self):\n        return self._userinfo[1]\n\n    @property\n    def hostname(self):\n        hostname = self._hostinfo[0]\n        if not hostname:\n            return None\n        # Scoped IPv6 address may have zone info, which must not be lowercased\n        # like http://[fe80::822a:a8ff:fe49:470c%tESt]:1234/keys\n        separator = '%' if isinstance(hostname, str) else b'%'\n        hostname, percent, zone = hostname.partition(separator)\n        return hostname.lower() + percent + zone\n\n    @property\n    def port(self):\n        port = self._hostinfo[1]\n        if port is not None:\n            if port.isdigit() and port.isascii():\n                port = int(port)\n            else:\n                raise ValueError(f\"Port could not be cast to integer value as {port!r}\")\n            if not (0 <= port <= 65535):\n                raise ValueError(\"Port out of range 0-65535\")\n        return port\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nclass _NetlocResultMixinStr(_NetlocResultMixinBase, _ResultMixinStr):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition('@')\n        if have_info:\n            username, have_password, password = userinfo.partition(':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition('@')\n        _, have_open_br, bracketed = hostinfo.partition('[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(']')\n            _, _, port = port.partition(':')\n        else:\n            hostname, _, port = hostinfo.partition(':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nclass _NetlocResultMixinBytes(_NetlocResultMixinBase, _ResultMixinBytes):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition(b'@')\n        if have_info:\n            username, have_password, password = userinfo.partition(b':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition(b'@')\n        _, have_open_br, bracketed = hostinfo.partition(b'[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(b']')\n            _, _, port = port.partition(b':')\n        else:\n            hostname, _, port = hostinfo.partition(b':')\n        if not port:\n            port = None\n        return hostname, port\n\n\n_DefragResultBase = namedtuple('DefragResult', 'url fragment')\n_SplitResultBase = namedtuple(\n    'SplitResult', 'scheme netloc path query fragment')\n_ParseResultBase = namedtuple(\n    'ParseResult', 'scheme netloc path params query fragment')\n\n_DefragResultBase.__doc__ = \"\"\"\nDefragResult(url, fragment)\n\nA 2-tuple that contains the url without fragment identifier and the fragment\nidentifier as a separate argument.\n\"\"\"\n\n_DefragResultBase.url.__doc__ = \"\"\"The URL with no fragment identifier.\"\"\"\n\n_DefragResultBase.fragment.__doc__ = \"\"\"\nFragment identifier separated from URL, that allows indirect identification of a\nsecondary resource by reference to a primary resource and additional identifying\ninformation.\n\"\"\"\n\n_SplitResultBase.__doc__ = \"\"\"\nSplitResult(scheme, netloc, path, query, fragment)\n\nA 5-tuple that contains the different components of a URL. Similar to\nParseResult, but does not split params.\n\"\"\"\n\n_SplitResultBase.scheme.__doc__ = \"\"\"Specifies URL scheme for the request.\"\"\"\n\n_SplitResultBase.netloc.__doc__ = \"\"\"\nNetwork location where the request is made to.\n\"\"\"\n\n_SplitResultBase.path.__doc__ = \"\"\"\nThe hierarchical path, such as the path to a file to download.\n\"\"\"\n\n_SplitResultBase.query.__doc__ = \"\"\"\nThe query component, that contains non-hierarchical data, that along with data\nin path component, identifies a resource in the scope of URI's scheme and\nnetwork location.\n\"\"\"\n\n_SplitResultBase.fragment.__doc__ = \"\"\"\nFragment identifier, that allows indirect identification of a secondary resource\nby reference to a primary resource and additional identifying information.\n\"\"\"\n\n_ParseResultBase.__doc__ = \"\"\"\nParseResult(scheme, netloc, path, params, query, fragment)\n\nA 6-tuple that contains components of a parsed URL.\n\"\"\"\n\n_ParseResultBase.scheme.__doc__ = _SplitResultBase.scheme.__doc__\n_ParseResultBase.netloc.__doc__ = _SplitResultBase.netloc.__doc__\n_ParseResultBase.path.__doc__ = _SplitResultBase.path.__doc__\n_ParseResultBase.params.__doc__ = \"\"\"\nParameters for last path element used to dereference the URI in order to provide\naccess to perform some operation on the resource.\n\"\"\"\n\n_ParseResultBase.query.__doc__ = _SplitResultBase.query.__doc__\n_ParseResultBase.fragment.__doc__ = _SplitResultBase.fragment.__doc__\n\n\n# For backwards compatibility, alias _NetlocResultMixinStr\n# ResultBase is no longer part of the documented API, but it is\n# retained since deprecating it isn't worth the hassle\nResultBase = _NetlocResultMixinStr\n\n# Structured result objects for string data\nclass DefragResult(_DefragResultBase, _ResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + '#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResult(_SplitResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResult(_ParseResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Structured result objects for bytes data\nclass DefragResultBytes(_DefragResultBase, _ResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + b'#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResultBytes(_SplitResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResultBytes(_ParseResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Set up the encode/decode result pairs\ndef _fix_result_transcoding():\n    _result_pairs = (\n        (DefragResult, DefragResultBytes),\n        (SplitResult, SplitResultBytes),\n        (ParseResult, ParseResultBytes),\n    )\n    for _decoded, _encoded in _result_pairs:\n        _decoded._encoded_counterpart = _encoded\n        _encoded._decoded_counterpart = _decoded\n\n_fix_result_transcoding()\ndel _fix_result_transcoding\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    The result is a named 6-tuple with fields corresponding to the\n    above. It is either a ParseResult or ParseResultBytes object,\n    depending on the type of the url parameter.\n\n    The username, password, hostname, and port sub-components of netloc\n    can also be accessed as attributes of the returned object.\n\n    The scheme argument provides the default value of the scheme\n    component when no scheme is found in url.\n\n    If allow_fragments is False, no attempt is made to separate the\n    fragment component from the previous component, which can be either\n    path or query.\n\n    Note that % escapes are not expanded.\n    \"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef _checknetloc(netloc):\n    if not netloc or netloc.isascii():\n        return\n    # looking for characters like \\u2100 that expand to 'a/c'\n    # IDNA uses NFKC equivalence, so normalize for this check\n    import unicodedata\n    n = netloc.replace('@', '')   # ignore characters already included\n    n = n.replace(':', '')        # but not the surrounding text\n    n = n.replace('#', '')\n    n = n.replace('?', '')\n    netloc2 = unicodedata.normalize('NFKC', n)\n    if n == netloc2:\n        return\n    for c in '/?#@:':\n        if c in netloc2:\n            raise ValueError(\"netloc '\" + netloc + \"' contains invalid \" +\n                             \"characters under NFKC normalization\")\n\ndef _check_bracketed_netloc(netloc):\n    # Note that this function must mirror the splitting\n    # done in NetlocResultMixins._hostinfo().\n    hostname_and_port = netloc.rpartition('@')[2]\n    before_bracket, have_open_br, bracketed = hostname_and_port.partition('[')\n    if have_open_br:\n        # No data is allowed before a bracket.\n        if before_bracket:\n            raise ValueError(\"Invalid IPv6 URL\")\n        hostname, _, port = bracketed.partition(']')\n        # No data is allowed after the bracket but before the port delimiter.\n        if port and not port.startswith(\":\"):\n            raise ValueError(\"Invalid IPv6 URL\")\n    else:\n        hostname, _, port = hostname_and_port.partition(':')\n    _check_bracketed_host(hostname)\n\n# Valid bracketed hosts are defined in\n# https://www.rfc-editor.org/rfc/rfc3986#page-49 and https://url.spec.whatwg.org/\ndef _check_bracketed_host(hostname):\n    if hostname.startswith('v'):\n        if not re.match(r\"\\Av[a-fA-F0-9]+\\..+\\Z\", hostname):\n            raise ValueError(f\"IPvFuture address is invalid\")\n    else:\n        ip = ipaddress.ip_address(hostname) # Throws Value Error if not IPv6 or IPv4\n        if isinstance(ip, ipaddress.IPv4Address):\n            raise ValueError(f\"An IPv4 address cannot be in brackets\")\n\n# typed=True avoids BytesWarnings being emitted during cache key\n# comparison since this API supports both bytes and str input.\n@functools.lru_cache(typed=True)\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n\n    The result is a named 5-tuple with fields corresponding to the\n    above. It is either a SplitResult or SplitResultBytes object,\n    depending on the type of the url parameter.\n\n    The username, password, hostname, and port sub-components of netloc\n    can also be accessed as attributes of the returned object.\n\n    The scheme argument provides the default value of the scheme\n    component when no scheme is found in url.\n\n    If allow_fragments is False, no attempt is made to separate the\n    fragment component from the previous component, which can be either\n    path or query.\n\n    Note that % escapes are not expanded.\n    \"\"\"\n\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    # Only lstrip url as some applications rely on preserving trailing space.\n    # (https://url.spec.whatwg.org/#concept-basic-url-parser would strip both)\n    url = url.lstrip(_WHATWG_C0_CONTROL_OR_SPACE)\n    scheme = scheme.strip(_WHATWG_C0_CONTROL_OR_SPACE)\n\n    for b in _UNSAFE_URL_BYTES_TO_REMOVE:\n        url = url.replace(b, \"\")\n        scheme = scheme.replace(b, \"\")\n\n    allow_fragments = bool(allow_fragments)\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0 and url[0].isascii() and url[0].isalpha():\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            scheme, url = url[:i].lower(), url[i+1:]\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n        if '[' in netloc and ']' in netloc:\n            _check_bracketed_netloc(netloc)\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    _checknetloc(netloc)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    return _coerce_result(v)\n\ndef urlunparse(components):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment, _coerce_result = (\n                                                  _coerce_args(*components))\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))\n\ndef urlunsplit(components):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc:\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + netloc + url\n    elif url[:2] == '//':\n        url = '//' + url\n    elif scheme and scheme in uses_netloc and (not url or url[:1] == '/'):\n        url = '//' + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n\n    base, url, _coerce_result = _coerce_args(base, url)\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n\n    if scheme != bscheme or scheme not in uses_relative:\n        return _coerce_result(url)\n    if scheme in uses_netloc:\n        if netloc:\n            return _coerce_result(urlunparse((scheme, netloc, path,\n                                              params, query, fragment)))\n        netloc = bnetloc\n\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return _coerce_result(urlunparse((scheme, netloc, path,\n                                          params, query, fragment)))\n\n    base_parts = bpath.split('/')\n    if base_parts[-1] != '':\n        # the last item is not a directory, so will not be taken into account\n        # in resolving the relative path\n        del base_parts[-1]\n\n    # for rfc3986, ignore all base path should the first character be root.\n    if path[:1] == '/':\n        segments = path.split('/')\n    else:\n        segments = base_parts + path.split('/')\n        # filter out elements that would cause redundant slashes on re-joining\n        # the resolved_path\n        segments[1:-1] = filter(None, segments[1:-1])\n\n    resolved_path = []\n\n    for seg in segments:\n        if seg == '..':\n            try:\n                resolved_path.pop()\n            except IndexError:\n                # ignore any .. segments that would otherwise cause an IndexError\n                # when popped from resolved_path if resolving for rfc3986\n                pass\n        elif seg == '.':\n            continue\n        else:\n            resolved_path.append(seg)\n\n    if segments[-1] in ('.', '..'):\n        # do some post-processing here. if the last segment was a relative dir,\n        # then we need to append the trailing '/'\n        resolved_path.append('')\n\n    return _coerce_result(urlunparse((scheme, netloc, '/'.join(\n        resolved_path) or '/', params, query, fragment)))\n\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    url, _coerce_result = _coerce_args(url)\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n    else:\n        frag = ''\n        defrag = url\n    return _coerce_result(DefragResult(defrag, frag))\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextobyte = None\n\ndef unquote_to_bytes(string):\n    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n    return bytes(_unquote_impl(string))\n\ndef _unquote_impl(string: bytes | bytearray | str) -> bytes | bytearray:\n    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n    # unescaped non-ASCII characters, which URIs should not.\n    if not string:\n        # Is it a string-like object?\n        string.split\n        return b''\n    if isinstance(string, str):\n        string = string.encode('utf-8')\n    bits = string.split(b'%')\n    if len(bits) == 1:\n        return string\n    res = bytearray(bits[0])\n    append = res.extend\n    # Delay the initialization of the table to not waste memory\n    # if the function is never called\n    global _hextobyte\n    if _hextobyte is None:\n        _hextobyte = {(a + b).encode(): bytes.fromhex(a + b)\n                      for a in _hexdig for b in _hexdig}\n    for item in bits[1:]:\n        try:\n            append(_hextobyte[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append(b'%')\n            append(item)\n    return res\n\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef _generate_unquoted_parts(string, encoding, errors):\n    previous_match_end = 0\n    for ascii_match in _asciire.finditer(string):\n        start, end = ascii_match.span()\n        yield string[previous_match_end:start]  # Non-ASCII\n        # The ascii_match[1] group == string[start:end].\n        yield _unquote_impl(ascii_match[1]).decode(encoding, errors)\n        previous_match_end = end\n    yield string[previous_match_end:]  # Non-ASCII tail\n\ndef unquote(string, encoding='utf-8', errors='replace'):\n    \"\"\"Replace %xx escapes by their single-character equivalent. The optional\n    encoding and errors parameters specify how to decode percent-encoded\n    sequences into Unicode characters, as accepted by the bytes.decode()\n    method.\n    By default, percent-encoded sequences are decoded with UTF-8, and invalid\n    sequences are replaced by a placeholder character.\n\n    unquote('abc%20def') -> 'abc def'.\n    \"\"\"\n    if isinstance(string, bytes):\n        return _unquote_impl(string).decode(encoding, errors)\n    if '%' not in string:\n        # Is it a string-like object?\n        string.split\n        return string\n    if encoding is None:\n        encoding = 'utf-8'\n    if errors is None:\n        errors = 'replace'\n    return ''.join(_generate_unquoted_parts(string, encoding, errors))\n\n\ndef parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n             encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError if there\n            are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a dictionary.\n    \"\"\"\n    parsed_result = {}\n    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n                      encoding=encoding, errors=errors,\n                      max_num_fields=max_num_fields, separator=separator)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result\n\n\ndef parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n              encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError\n            if there are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a list, as G-d intended.\n    \"\"\"\n\n    if not separator or not isinstance(separator, (str, bytes)):\n        raise ValueError(\"Separator must be of type string or bytes.\")\n    if isinstance(qs, str):\n        if not isinstance(separator, str):\n            separator = str(separator, 'ascii')\n        eq = '='\n        def _unquote(s):\n            return unquote_plus(s, encoding=encoding, errors=errors)\n    else:\n        if not qs:\n            return []\n        # Use memoryview() to reject integers and iterables,\n        # acceptable by the bytes constructor.\n        qs = bytes(memoryview(qs))\n        if isinstance(separator, str):\n            separator = bytes(separator, 'ascii')\n        eq = b'='\n        def _unquote(s):\n            return unquote_to_bytes(s.replace(b'+', b' '))\n\n    if not qs:\n        return []\n\n    # If max_num_fields is defined then check that the number of fields\n    # is less than max_num_fields. This prevents a memory exhaustion DOS\n    # attack via post bodies with many fields.\n    if max_num_fields is not None:\n        num_fields = 1 + qs.count(separator)\n        if max_num_fields < num_fields:\n            raise ValueError('Max number of fields exceeded')\n\n    r = []\n    for name_value in qs.split(separator):\n        if name_value or strict_parsing:\n            name, has_eq, value = name_value.partition(eq)\n            if not has_eq and strict_parsing:\n                raise ValueError(\"bad query field: %r\" % (name_value,))\n            if value or keep_blank_values:\n                name = _unquote(name)\n                value = _unquote(value)\n                r.append((name, value))\n    return r\n\ndef unquote_plus(string, encoding='utf-8', errors='replace'):\n    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n    unquoting HTML form values.\n\n    unquote_plus('%7e/abc+def') -> '~/abc def'\n    \"\"\"\n    string = string.replace('+', ' ')\n    return unquote(string, encoding, errors)\n\n_ALWAYS_SAFE = frozenset(b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                         b'abcdefghijklmnopqrstuvwxyz'\n                         b'0123456789'\n                         b'_.-~')\n_ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)\n\ndef __getattr__(name):\n    if name == 'Quoter':\n        warnings.warn('Deprecated in 3.11. '\n                      'urllib.parse.Quoter will be removed in Python 3.14. '\n                      'It was not intended to be a public API.',\n                      DeprecationWarning, stacklevel=2)\n        return _Quoter\n    raise AttributeError(f'module {__name__!r} has no attribute {name!r}')\n\nclass _Quoter(dict):\n    \"\"\"A mapping from bytes numbers (in range(0,256)) to strings.\n\n    String values are percent-encoded byte values, unless the key < 128, and\n    in either of the specified safe set, or the always safe set.\n    \"\"\"\n    # Keeps a cache internally, via __missing__, for efficiency (lookups\n    # of cached keys don't call Python code at all).\n    def __init__(self, safe):\n        \"\"\"safe: bytes object.\"\"\"\n        self.safe = _ALWAYS_SAFE.union(safe)\n\n    def __repr__(self):\n        return f\"<Quoter {dict(self)!r}>\"\n\n    def __missing__(self, b):\n        # Handle a cache miss. Store quoted string in cache and return.\n        res = chr(b) if b in self.safe else '%{:02X}'.format(b)\n        self[b] = res\n        return res\n\ndef quote(string, safe='/', encoding=None, errors=None):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted. The\n    quote function offers a cautious (not minimal) way to quote a\n    string for most of these parts.\n\n    RFC 3986 Uniform Resource Identifier (URI): Generic Syntax lists\n    the following (un)reserved characters.\n\n    unreserved    = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    reserved      = gen-delims / sub-delims\n    gen-delims    = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    sub-delims    = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n                  / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\n    Each of the reserved characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    The quote function %-escapes all characters that are neither in the\n    unreserved chars (\"always safe\") nor the additional chars set via the\n    safe arg.\n\n    The default for the safe arg is '/'. The character is reserved, but in\n    typical usage the quote function is being called on a path where the\n    existing slash characters are to be preserved.\n\n    Python 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.\n    Now, \"~\" is included in the set of unreserved characters.\n\n    string and safe may be either str or bytes objects. encoding and errors\n    must not be specified if string is a bytes object.\n\n    The optional encoding and errors parameters specify how to deal with\n    non-ASCII characters, as accepted by the str.encode method.\n    By default, encoding='utf-8' (characters are encoded with UTF-8), and\n    errors='strict' (unsupported characters raise a UnicodeEncodeError).\n    \"\"\"\n    if isinstance(string, str):\n        if not string:\n            return string\n        if encoding is None:\n            encoding = 'utf-8'\n        if errors is None:\n            errors = 'strict'\n        string = string.encode(encoding, errors)\n    else:\n        if encoding is not None:\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n        if errors is not None:\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n    return quote_from_bytes(string, safe)\n\ndef quote_plus(string, safe='', encoding=None, errors=None):\n    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n    HTML form values. Plus signs in the original string are escaped unless\n    they are included in safe. It also does not have safe default to '/'.\n    \"\"\"\n    # Check if ' ' in string, where string may either be a str or bytes.  If\n    # there are no spaces, the regular quote will produce the right answer.\n    if ((isinstance(string, str) and ' ' not in string) or\n        (isinstance(string, bytes) and b' ' not in string)):\n        return quote(string, safe, encoding, errors)\n    if isinstance(safe, str):\n        space = ' '\n    else:\n        space = b' '\n    string = quote(string, safe + space, encoding, errors)\n    return string.replace(' ', '+')\n\n# Expectation: A typical program is unlikely to create more than 5 of these.\n@functools.lru_cache\ndef _byte_quoter_factory(safe):\n    return _Quoter(safe).__getitem__\n\ndef quote_from_bytes(bs, safe='/'):\n    \"\"\"Like quote(), but accepts a bytes object rather than a str, and does\n    not perform string-to-bytes encoding.  It always returns an ASCII string.\n    quote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\n    \"\"\"\n    if not isinstance(bs, (bytes, bytearray)):\n        raise TypeError(\"quote_from_bytes() expected bytes\")\n    if not bs:\n        return ''\n    if isinstance(safe, str):\n        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n        safe = safe.encode('ascii', 'ignore')\n    else:\n        # List comprehensions are faster than generator expressions.\n        safe = bytes([c for c in safe if c < 128])\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n        return bs.decode()\n    quoter = _byte_quoter_factory(safe)\n    if (bs_len := len(bs)) < 200_000:\n        return ''.join(map(quoter, bs))\n    else:\n        # This saves memory - https://github.com/python/cpython/issues/95865\n        chunk_size = math.isqrt(bs_len)\n        chunks = [''.join(map(quoter, bs[i:i+chunk_size]))\n                  for i in range(0, bs_len, chunk_size)]\n        return ''.join(chunks)\n\ndef urlencode(query, doseq=False, safe='', encoding=None, errors=None,\n              quote_via=quote_plus):\n    \"\"\"Encode a dict or sequence of two-element tuples into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n\n    The components of a query arg may each be either a string or a bytes type.\n\n    The safe, encoding, and errors parameters are passed down to the function\n    specified by quote_via (encoding and errors only if a component is a str).\n    \"\"\"\n\n    if hasattr(query, \"items\"):\n        query = query.items()\n    else:\n        # It's a bother at times that strings and string-like objects are\n        # sequences.\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # Zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit.  Since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError as err:\n            raise TypeError(\"not a valid non-string sequence \"\n                            \"or mapping object\") from err\n\n    l = []\n    if not doseq:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n            else:\n                v = quote_via(str(v), safe, encoding, errors)\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n                l.append(k + '=' + v)\n            elif isinstance(v, str):\n                v = quote_via(v, safe, encoding, errors)\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # Is this a sufficient test for sequence-ness?\n                    x = len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_via(str(v), safe, encoding, errors)\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        if isinstance(elt, bytes):\n                            elt = quote_via(elt, safe)\n                        else:\n                            elt = quote_via(str(elt), safe, encoding, errors)\n                        l.append(k + '=' + elt)\n    return '&'.join(l)\n\n\ndef to_bytes(url):\n    warnings.warn(\"urllib.parse.to_bytes() is deprecated as of 3.8\",\n                  DeprecationWarning, stacklevel=2)\n    return _to_bytes(url)\n\n\ndef _to_bytes(url):\n    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed.\n    # XXX get rid of to_bytes()\n    if isinstance(url, str):\n        try:\n            url = url.encode(\"ASCII\").decode()\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\n\ndef unwrap(url):\n    \"\"\"Transform a string like '<URL:scheme://host/path>' into 'scheme://host/path'.\n\n    The string is returned unchanged if it's not a wrapped URL.\n    \"\"\"\n    url = str(url).strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:':\n        url = url[4:].strip()\n    return url\n\n\ndef splittype(url):\n    warnings.warn(\"urllib.parse.splittype() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittype(url)\n\n\n_typeprog = None\ndef _splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        _typeprog = re.compile('([^/:]+):(.*)', re.DOTALL)\n\n    match = _typeprog.match(url)\n    if match:\n        scheme, data = match.groups()\n        return scheme.lower(), data\n    return None, url\n\n\ndef splithost(url):\n    warnings.warn(\"urllib.parse.splithost() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splithost(url)\n\n\n_hostprog = None\ndef _splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        _hostprog = re.compile('//([^/#?]*)(.*)', re.DOTALL)\n\n    match = _hostprog.match(url)\n    if match:\n        host_port, path = match.groups()\n        if path and path[0] != '/':\n            path = '/' + path\n        return host_port, path\n    return None, url\n\n\ndef splituser(host):\n    warnings.warn(\"urllib.parse.splituser() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splituser(host)\n\n\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\ndef splitpasswd(user):\n    warnings.warn(\"urllib.parse.splitpasswd() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitpasswd(user)\n\n\ndef _splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    user, delim, passwd = user.partition(':')\n    return user, (passwd if delim else None)\n\n\ndef splitport(host):\n    warnings.warn(\"urllib.parse.splitport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitport(host)\n\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef _splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)\n\n    match = _portprog.fullmatch(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\n\ndef splitnport(host, defport=-1):\n    warnings.warn(\"urllib.parse.splitnport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitnport(host, defport)\n\n\ndef _splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number is found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    host, delim, port = host.rpartition(':')\n    if not delim:\n        host = port\n    elif port:\n        if port.isdigit() and port.isascii():\n            nport = int(port)\n        else:\n            nport = None\n        return host, nport\n    return host, defport\n\n\ndef splitquery(url):\n    warnings.warn(\"urllib.parse.splitquery() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitquery(url)\n\n\ndef _splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    path, delim, query = url.rpartition('?')\n    if delim:\n        return path, query\n    return url, None\n\n\ndef splittag(url):\n    warnings.warn(\"urllib.parse.splittag() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittag(url)\n\n\ndef _splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    path, delim, tag = url.rpartition('#')\n    if delim:\n        return path, tag\n    return url, None\n\n\ndef splitattr(url):\n    warnings.warn(\"urllib.parse.splitattr() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitattr(url)\n\n\ndef _splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\n\ndef splitvalue(attr):\n    warnings.warn(\"urllib.parse.splitvalue() is deprecated as of 3.8, \"\n                  \"use urllib.parse.parse_qsl() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitvalue(attr)\n\n\ndef _splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    attr, delim, value = attr.partition('=')\n    return attr, (value if delim else None)\n", 1262], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\pathlib.py": ["\"\"\"Object-oriented filesystem paths.\n\nThis module provides classes to represent abstract paths and concrete\npaths with operations that have semantics appropriate for different\noperating systems.\n\"\"\"\n\nimport fnmatch\nimport functools\nimport io\nimport ntpath\nimport os\nimport posixpath\nimport re\nimport sys\nimport warnings\nfrom _collections_abc import Sequence\nfrom errno import ENOENT, ENOTDIR, EBADF, ELOOP\nfrom stat import S_ISDIR, S_ISLNK, S_ISREG, S_ISSOCK, S_ISBLK, S_ISCHR, S_ISFIFO\nfrom urllib.parse import quote_from_bytes as urlquote_from_bytes\n\n\n__all__ = [\n    \"PurePath\", \"PurePosixPath\", \"PureWindowsPath\",\n    \"Path\", \"PosixPath\", \"WindowsPath\",\n    ]\n\n#\n# Internals\n#\n\n# Reference for Windows paths can be found at\n# https://learn.microsoft.com/en-gb/windows/win32/fileio/naming-a-file .\n_WIN_RESERVED_NAMES = frozenset(\n    {'CON', 'PRN', 'AUX', 'NUL', 'CONIN$', 'CONOUT$'} |\n    {f'COM{c}' for c in '123456789\\xb9\\xb2\\xb3'} |\n    {f'LPT{c}' for c in '123456789\\xb9\\xb2\\xb3'}\n)\n\n_WINERROR_NOT_READY = 21  # drive exists but is not accessible\n_WINERROR_INVALID_NAME = 123  # fix for bpo-35306\n_WINERROR_CANT_RESOLVE_FILENAME = 1921  # broken symlink pointing to itself\n\n# EBADF - guard against macOS `stat` throwing EBADF\n_IGNORED_ERRNOS = (ENOENT, ENOTDIR, EBADF, ELOOP)\n\n_IGNORED_WINERRORS = (\n    _WINERROR_NOT_READY,\n    _WINERROR_INVALID_NAME,\n    _WINERROR_CANT_RESOLVE_FILENAME)\n\ndef _ignore_error(exception):\n    return (getattr(exception, 'errno', None) in _IGNORED_ERRNOS or\n            getattr(exception, 'winerror', None) in _IGNORED_WINERRORS)\n\n\n@functools.cache\ndef _is_case_sensitive(flavour):\n    return flavour.normcase('Aa') == 'Aa'\n\n#\n# Globbing helpers\n#\n\n\n# fnmatch.translate() returns a regular expression that includes a prefix and\n# a suffix, which enable matching newlines and ensure the end of the string is\n# matched, respectively. These features are undesirable for our implementation\n# of PurePatch.match(), which represents path separators as newlines and joins\n# pattern segments together. As a workaround, we define a slice object that\n# can remove the prefix and suffix from any translate() result. See the\n# _compile_pattern_lines() function for more details.\n_FNMATCH_PREFIX, _FNMATCH_SUFFIX = fnmatch.translate('_').split('_')\n_FNMATCH_SLICE = slice(len(_FNMATCH_PREFIX), -len(_FNMATCH_SUFFIX))\n_SWAP_SEP_AND_NEWLINE = {\n    '/': str.maketrans({'/': '\\n', '\\n': '/'}),\n    '\\\\': str.maketrans({'\\\\': '\\n', '\\n': '\\\\'}),\n}\n\n\n@functools.lru_cache()\ndef _make_selector(pattern_parts, flavour, case_sensitive):\n    pat = pattern_parts[0]\n    if not pat:\n        return _TerminatingSelector()\n    if pat == '**':\n        child_parts_idx = 1\n        while child_parts_idx < len(pattern_parts) and pattern_parts[child_parts_idx] == '**':\n            child_parts_idx += 1\n        child_parts = pattern_parts[child_parts_idx:]\n        if '**' in child_parts:\n            cls = _DoubleRecursiveWildcardSelector\n        else:\n            cls = _RecursiveWildcardSelector\n    else:\n        child_parts = pattern_parts[1:]\n        if pat == '..':\n            cls = _ParentSelector\n        elif '**' in pat:\n            raise ValueError(\"Invalid pattern: '**' can only be an entire path component\")\n        else:\n            cls = _WildcardSelector\n    return cls(pat, child_parts, flavour, case_sensitive)\n\n\n@functools.lru_cache(maxsize=256)\ndef _compile_pattern(pat, case_sensitive):\n    flags = re.NOFLAG if case_sensitive else re.IGNORECASE\n    return re.compile(fnmatch.translate(pat), flags).match\n\n\n@functools.lru_cache()\ndef _compile_pattern_lines(pattern_lines, case_sensitive):\n    \"\"\"Compile the given pattern lines to an `re.Pattern` object.\n\n    The *pattern_lines* argument is a glob-style pattern (e.g. '*/*.py') with\n    its path separators and newlines swapped (e.g. '*\\n*.py`). By using\n    newlines to separate path components, and not setting `re.DOTALL`, we\n    ensure that the `*` wildcard cannot match path separators.\n\n    The returned `re.Pattern` object may have its `match()` method called to\n    match a complete pattern, or `search()` to match from the right. The\n    argument supplied to these methods must also have its path separators and\n    newlines swapped.\n    \"\"\"\n\n    # Match the start of the path, or just after a path separator\n    parts = ['^']\n    for part in pattern_lines.splitlines(keepends=True):\n        if part == '*\\n':\n            part = r'.+\\n'\n        elif part == '*':\n            part = r'.+'\n        else:\n            # Any other component: pass to fnmatch.translate(). We slice off\n            # the common prefix and suffix added by translate() to ensure that\n            # re.DOTALL is not set, and the end of the string not matched,\n            # respectively. With DOTALL not set, '*' wildcards will not match\n            # path separators, because the '.' characters in the pattern will\n            # not match newlines.\n            part = fnmatch.translate(part)[_FNMATCH_SLICE]\n        parts.append(part)\n    # Match the end of the path, always.\n    parts.append(r'\\Z')\n    flags = re.MULTILINE\n    if not case_sensitive:\n        flags |= re.IGNORECASE\n    return re.compile(''.join(parts), flags=flags)\n\n\nclass _Selector:\n    \"\"\"A selector matches a specific glob pattern part against the children\n    of a given path.\"\"\"\n\n    def __init__(self, child_parts, flavour, case_sensitive):\n        self.child_parts = child_parts\n        if child_parts:\n            self.successor = _make_selector(child_parts, flavour, case_sensitive)\n            self.dironly = True\n        else:\n            self.successor = _TerminatingSelector()\n            self.dironly = False\n\n    def select_from(self, parent_path):\n        \"\"\"Iterate over all child paths of `parent_path` matched by this\n        selector.  This can contain parent_path itself.\"\"\"\n        path_cls = type(parent_path)\n        scandir = path_cls._scandir\n        if not parent_path.is_dir():\n            return iter([])\n        return self._select_from(parent_path, scandir)\n\n\nclass _TerminatingSelector:\n\n    def _select_from(self, parent_path, scandir):\n        yield parent_path\n\n\nclass _ParentSelector(_Selector):\n\n    def __init__(self, name, child_parts, flavour, case_sensitive):\n        _Selector.__init__(self, child_parts, flavour, case_sensitive)\n\n    def _select_from(self,  parent_path, scandir):\n        path = parent_path._make_child_relpath('..')\n        for p in self.successor._select_from(path, scandir):\n            yield p\n\n\nclass _WildcardSelector(_Selector):\n\n    def __init__(self, pat, child_parts, flavour, case_sensitive):\n        _Selector.__init__(self, child_parts, flavour, case_sensitive)\n        if case_sensitive is None:\n            # TODO: evaluate case-sensitivity of each directory in _select_from()\n            case_sensitive = _is_case_sensitive(flavour)\n        self.match = _compile_pattern(pat, case_sensitive)\n\n    def _select_from(self, parent_path, scandir):\n        try:\n            # We must close the scandir() object before proceeding to\n            # avoid exhausting file descriptors when globbing deep trees.\n            with scandir(parent_path) as scandir_it:\n                entries = list(scandir_it)\n        except OSError:\n            pass\n        else:\n            for entry in entries:\n                if self.dironly:\n                    try:\n                        if not entry.is_dir():\n                            continue\n                    except OSError:\n                        continue\n                name = entry.name\n                if self.match(name):\n                    path = parent_path._make_child_relpath(name)\n                    for p in self.successor._select_from(path, scandir):\n                        yield p\n\n\nclass _RecursiveWildcardSelector(_Selector):\n\n    def __init__(self, pat, child_parts, flavour, case_sensitive):\n        _Selector.__init__(self, child_parts, flavour, case_sensitive)\n\n    def _iterate_directories(self, parent_path):\n        yield parent_path\n        for dirpath, dirnames, _ in parent_path.walk():\n            for dirname in dirnames:\n                yield dirpath._make_child_relpath(dirname)\n\n    def _select_from(self, parent_path, scandir):\n        successor_select = self.successor._select_from\n        for starting_point in self._iterate_directories(parent_path):\n            for p in successor_select(starting_point, scandir):\n                yield p\n\n\nclass _DoubleRecursiveWildcardSelector(_RecursiveWildcardSelector):\n    \"\"\"\n    Like _RecursiveWildcardSelector, but also de-duplicates results from\n    successive selectors. This is necessary if the pattern contains\n    multiple non-adjacent '**' segments.\n    \"\"\"\n\n    def _select_from(self, parent_path, scandir):\n        yielded = set()\n        try:\n            for p in super()._select_from(parent_path, scandir):\n                if p not in yielded:\n                    yield p\n                    yielded.add(p)\n        finally:\n            yielded.clear()\n\n\n#\n# Public API\n#\n\nclass _PathParents(Sequence):\n    \"\"\"This object provides sequence-like access to the logical ancestors\n    of a path.  Don't try to construct it yourself.\"\"\"\n    __slots__ = ('_path', '_drv', '_root', '_tail')\n\n    def __init__(self, path):\n        self._path = path\n        self._drv = path.drive\n        self._root = path.root\n        self._tail = path._tail\n\n    def __len__(self):\n        return len(self._tail)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            return tuple(self[i] for i in range(*idx.indices(len(self))))\n\n        if idx >= len(self) or idx < -len(self):\n            raise IndexError(idx)\n        if idx < 0:\n            idx += len(self)\n        return self._path._from_parsed_parts(self._drv, self._root,\n                                             self._tail[:-idx - 1])\n\n    def __repr__(self):\n        return \"<{}.parents>\".format(type(self._path).__name__)\n\n\nclass PurePath(object):\n    \"\"\"Base class for manipulating paths without I/O.\n\n    PurePath represents a filesystem path and offers operations which\n    don't imply any actual filesystem I/O.  Depending on your system,\n    instantiating a PurePath will return either a PurePosixPath or a\n    PureWindowsPath object.  You can also instantiate either of these classes\n    directly, regardless of your system.\n    \"\"\"\n\n    __slots__ = (\n        # The `_raw_paths` slot stores unnormalized string paths. This is set\n        # in the `__init__()` method.\n        '_raw_paths',\n\n        # The `_drv`, `_root` and `_tail_cached` slots store parsed and\n        # normalized parts of the path. They are set when any of the `drive`,\n        # `root` or `_tail` properties are accessed for the first time. The\n        # three-part division corresponds to the result of\n        # `os.path.splitroot()`, except that the tail is further split on path\n        # separators (i.e. it is a list of strings), and that the root and\n        # tail are normalized.\n        '_drv', '_root', '_tail_cached',\n\n        # The `_str` slot stores the string representation of the path,\n        # computed from the drive, root and tail when `__str__()` is called\n        # for the first time. It's used to implement `_str_normcase`\n        '_str',\n\n        # The `_str_normcase_cached` slot stores the string path with\n        # normalized case. It is set when the `_str_normcase` property is\n        # accessed for the first time. It's used to implement `__eq__()`\n        # `__hash__()`, and `_parts_normcase`\n        '_str_normcase_cached',\n\n        # The `_parts_normcase_cached` slot stores the case-normalized\n        # string path after splitting on path separators. It's set when the\n        # `_parts_normcase` property is accessed for the first time. It's used\n        # to implement comparison methods like `__lt__()`.\n        '_parts_normcase_cached',\n\n        # The `_lines_cached` slot stores the string path with path separators\n        # and newlines swapped. This is used to implement `match()`.\n        '_lines_cached',\n\n        # The `_hash` slot stores the hash of the case-normalized string\n        # path. It's set when `__hash__()` is called for the first time.\n        '_hash',\n    )\n    _flavour = os.path\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Construct a PurePath from one or several strings and or existing\n        PurePath objects.  The strings and path objects are combined so as\n        to yield a canonicalized path, which is incorporated into the\n        new PurePath object.\n        \"\"\"\n        if cls is PurePath:\n            cls = PureWindowsPath if os.name == 'nt' else PurePosixPath\n        return object.__new__(cls)\n\n    def __reduce__(self):\n        # Using the parts tuple helps share interned path parts\n        # when pickling related paths.\n        return (self.__class__, self.parts)\n\n    def __init__(self, *args):\n        paths = []\n        for arg in args:\n            if isinstance(arg, PurePath):\n                if arg._flavour is not self._flavour:\n                    # GH-103631: Convert separators for backwards compatibility.\n                    paths.append(arg.as_posix())\n                else:\n                    paths.extend(arg._raw_paths)\n            else:\n                try:\n                    path = os.fspath(arg)\n                except TypeError:\n                    path = arg\n                if not isinstance(path, str):\n                    raise TypeError(\n                        \"argument should be a str or an os.PathLike \"\n                        \"object where __fspath__ returns a str, \"\n                        f\"not {type(path).__name__!r}\")\n                paths.append(path)\n        self._raw_paths = paths\n\n    def with_segments(self, *pathsegments):\n        \"\"\"Construct a new path object from any number of path-like objects.\n        Subclasses may override this method to customize how new path objects\n        are created from methods like `iterdir()`.\n        \"\"\"\n        return type(self)(*pathsegments)\n\n    @classmethod\n    def _parse_path(cls, path):\n        if not path:\n            return '', '', []\n        sep = cls._flavour.sep\n        altsep = cls._flavour.altsep\n        if altsep:\n            path = path.replace(altsep, sep)\n        drv, root, rel = cls._flavour.splitroot(path)\n        if not root and drv.startswith(sep) and not drv.endswith(sep):\n            drv_parts = drv.split(sep)\n            if len(drv_parts) == 4 and drv_parts[2] not in '?.':\n                # e.g. //server/share\n                root = sep\n            elif len(drv_parts) == 6:\n                # e.g. //?/unc/server/share\n                root = sep\n        parsed = [sys.intern(str(x)) for x in rel.split(sep) if x and x != '.']\n        return drv, root, parsed\n\n    def _load_parts(self):\n        paths = self._raw_paths\n        if len(paths) == 0:\n            path = ''\n        elif len(paths) == 1:\n            path = paths[0]\n        else:\n            path = self._flavour.join(*paths)\n        drv, root, tail = self._parse_path(path)\n        self._drv = drv\n        self._root = root\n        self._tail_cached = tail\n\n    def _from_parsed_parts(self, drv, root, tail):\n        path_str = self._format_parsed_parts(drv, root, tail)\n        path = self.with_segments(path_str)\n        path._str = path_str or '.'\n        path._drv = drv\n        path._root = root\n        path._tail_cached = tail\n        return path\n\n    @classmethod\n    def _format_parsed_parts(cls, drv, root, tail):\n        if drv or root:\n            return drv + root + cls._flavour.sep.join(tail)\n        elif tail and cls._flavour.splitdrive(tail[0])[0]:\n            tail = ['.'] + tail\n        return cls._flavour.sep.join(tail)\n\n    def __str__(self):\n        \"\"\"Return the string representation of the path, suitable for\n        passing to system calls.\"\"\"\n        try:\n            return self._str\n        except AttributeError:\n            self._str = self._format_parsed_parts(self.drive, self.root,\n                                                  self._tail) or '.'\n            return self._str\n\n    def __fspath__(self):\n        return str(self)\n\n    def as_posix(self):\n        \"\"\"Return the string representation of the path with forward (/)\n        slashes.\"\"\"\n        f = self._flavour\n        return str(self).replace(f.sep, '/')\n\n    def __bytes__(self):\n        \"\"\"Return the bytes representation of the path.  This is only\n        recommended to use under Unix.\"\"\"\n        return os.fsencode(self)\n\n    def __repr__(self):\n        return \"{}({!r})\".format(self.__class__.__name__, self.as_posix())\n\n    def as_uri(self):\n        \"\"\"Return the path as a 'file' URI.\"\"\"\n        if not self.is_absolute():\n            raise ValueError(\"relative path can't be expressed as a file URI\")\n\n        drive = self.drive\n        if len(drive) == 2 and drive[1] == ':':\n            # It's a path on a local drive => 'file:///c:/a/b'\n            prefix = 'file:///' + drive\n            path = self.as_posix()[2:]\n        elif drive:\n            # It's a path on a network drive => 'file://host/share/a/b'\n            prefix = 'file:'\n            path = self.as_posix()\n        else:\n            # It's a posix path => 'file:///etc/hosts'\n            prefix = 'file://'\n            path = str(self)\n        return prefix + urlquote_from_bytes(os.fsencode(path))\n\n    @property\n    def _str_normcase(self):\n        # String with normalized case, for hashing and equality checks\n        try:\n            return self._str_normcase_cached\n        except AttributeError:\n            if _is_case_sensitive(self._flavour):\n                self._str_normcase_cached = str(self)\n            else:\n                self._str_normcase_cached = str(self).lower()\n            return self._str_normcase_cached\n\n    @property\n    def _parts_normcase(self):\n        # Cached parts with normalized case, for comparisons.\n        try:\n            return self._parts_normcase_cached\n        except AttributeError:\n            self._parts_normcase_cached = self._str_normcase.split(self._flavour.sep)\n            return self._parts_normcase_cached\n\n    @property\n    def _lines(self):\n        # Path with separators and newlines swapped, for pattern matching.\n        try:\n            return self._lines_cached\n        except AttributeError:\n            path_str = str(self)\n            if path_str == '.':\n                self._lines_cached = ''\n            else:\n                trans = _SWAP_SEP_AND_NEWLINE[self._flavour.sep]\n                self._lines_cached = path_str.translate(trans)\n            return self._lines_cached\n\n    def __eq__(self, other):\n        if not isinstance(other, PurePath):\n            return NotImplemented\n        return self._str_normcase == other._str_normcase and self._flavour is other._flavour\n\n    def __hash__(self):\n        try:\n            return self._hash\n        except AttributeError:\n            self._hash = hash(self._str_normcase)\n            return self._hash\n\n    def __lt__(self, other):\n        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n            return NotImplemented\n        return self._parts_normcase < other._parts_normcase\n\n    def __le__(self, other):\n        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n            return NotImplemented\n        return self._parts_normcase <= other._parts_normcase\n\n    def __gt__(self, other):\n        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n            return NotImplemented\n        return self._parts_normcase > other._parts_normcase\n\n    def __ge__(self, other):\n        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n            return NotImplemented\n        return self._parts_normcase >= other._parts_normcase\n\n    @property\n    def drive(self):\n        \"\"\"The drive prefix (letter or UNC path), if any.\"\"\"\n        try:\n            return self._drv\n        except AttributeError:\n            self._load_parts()\n            return self._drv\n\n    @property\n    def root(self):\n        \"\"\"The root of the path, if any.\"\"\"\n        try:\n            return self._root\n        except AttributeError:\n            self._load_parts()\n            return self._root\n\n    @property\n    def _tail(self):\n        try:\n            return self._tail_cached\n        except AttributeError:\n            self._load_parts()\n            return self._tail_cached\n\n    @property\n    def anchor(self):\n        \"\"\"The concatenation of the drive and root, or ''.\"\"\"\n        anchor = self.drive + self.root\n        return anchor\n\n    @property\n    def name(self):\n        \"\"\"The final path component, if any.\"\"\"\n        tail = self._tail\n        if not tail:\n            return ''\n        return tail[-1]\n\n    @property\n    def suffix(self):\n        \"\"\"\n        The final component's last suffix, if any.\n\n        This includes the leading period. For example: '.txt'\n        \"\"\"\n        name = self.name\n        i = name.rfind('.')\n        if 0 < i < len(name) - 1:\n            return name[i:]\n        else:\n            return ''\n\n    @property\n    def suffixes(self):\n        \"\"\"\n        A list of the final component's suffixes, if any.\n\n        These include the leading periods. For example: ['.tar', '.gz']\n        \"\"\"\n        name = self.name\n        if name.endswith('.'):\n            return []\n        name = name.lstrip('.')\n        return ['.' + suffix for suffix in name.split('.')[1:]]\n\n    @property\n    def stem(self):\n        \"\"\"The final path component, minus its last suffix.\"\"\"\n        name = self.name\n        i = name.rfind('.')\n        if 0 < i < len(name) - 1:\n            return name[:i]\n        else:\n            return name\n\n    def with_name(self, name):\n        \"\"\"Return a new path with the file name changed.\"\"\"\n        if not self.name:\n            raise ValueError(\"%r has an empty name\" % (self,))\n        f = self._flavour\n        if not name or f.sep in name or (f.altsep and f.altsep in name) or name == '.':\n            raise ValueError(\"Invalid name %r\" % (name))\n        return self._from_parsed_parts(self.drive, self.root,\n                                       self._tail[:-1] + [name])\n\n    def with_stem(self, stem):\n        \"\"\"Return a new path with the stem changed.\"\"\"\n        return self.with_name(stem + self.suffix)\n\n    def with_suffix(self, suffix):\n        \"\"\"Return a new path with the file suffix changed.  If the path\n        has no suffix, add given suffix.  If the given suffix is an empty\n        string, remove the suffix from the path.\n        \"\"\"\n        f = self._flavour\n        if f.sep in suffix or f.altsep and f.altsep in suffix:\n            raise ValueError(\"Invalid suffix %r\" % (suffix,))\n        if suffix and not suffix.startswith('.') or suffix == '.':\n            raise ValueError(\"Invalid suffix %r\" % (suffix))\n        name = self.name\n        if not name:\n            raise ValueError(\"%r has an empty name\" % (self,))\n        old_suffix = self.suffix\n        if not old_suffix:\n            name = name + suffix\n        else:\n            name = name[:-len(old_suffix)] + suffix\n        return self._from_parsed_parts(self.drive, self.root,\n                                       self._tail[:-1] + [name])\n\n    def relative_to(self, other, /, *_deprecated, walk_up=False):\n        \"\"\"Return the relative path to another path identified by the passed\n        arguments.  If the operation is not possible (because this is not\n        related to the other path), raise ValueError.\n\n        The *walk_up* parameter controls whether `..` may be used to resolve\n        the path.\n        \"\"\"\n        if _deprecated:\n            msg = (\"support for supplying more than one positional argument \"\n                   \"to pathlib.PurePath.relative_to() is deprecated and \"\n                   \"scheduled for removal in Python {remove}\")\n            warnings._deprecated(\"pathlib.PurePath.relative_to(*args)\", msg,\n                                 remove=(3, 14))\n        other = self.with_segments(other, *_deprecated)\n        for step, path in enumerate([other] + list(other.parents)):\n            if self.is_relative_to(path):\n                break\n            elif not walk_up:\n                raise ValueError(f\"{str(self)!r} is not in the subpath of {str(other)!r}\")\n            elif path.name == '..':\n                raise ValueError(f\"'..' segment in {str(other)!r} cannot be walked\")\n        else:\n            raise ValueError(f\"{str(self)!r} and {str(other)!r} have different anchors\")\n        parts = ['..'] * step + self._tail[len(path._tail):]\n        return self.with_segments(*parts)\n\n    def is_relative_to(self, other, /, *_deprecated):\n        \"\"\"Return True if the path is relative to another path or False.\n        \"\"\"\n        if _deprecated:\n            msg = (\"support for supplying more than one argument to \"\n                   \"pathlib.PurePath.is_relative_to() is deprecated and \"\n                   \"scheduled for removal in Python {remove}\")\n            warnings._deprecated(\"pathlib.PurePath.is_relative_to(*args)\",\n                                 msg, remove=(3, 14))\n        other = self.with_segments(other, *_deprecated)\n        return other == self or other in self.parents\n\n    @property\n    def parts(self):\n        \"\"\"An object providing sequence-like access to the\n        components in the filesystem path.\"\"\"\n        if self.drive or self.root:\n            return (self.drive + self.root,) + tuple(self._tail)\n        else:\n            return tuple(self._tail)\n\n    def joinpath(self, *pathsegments):\n        \"\"\"Combine this path with one or several arguments, and return a\n        new path representing either a subpath (if all arguments are relative\n        paths) or a totally different path (if one of the arguments is\n        anchored).\n        \"\"\"\n        return self.with_segments(self, *pathsegments)\n\n    def __truediv__(self, key):\n        try:\n            return self.joinpath(key)\n        except TypeError:\n            return NotImplemented\n\n    def __rtruediv__(self, key):\n        try:\n            return self.with_segments(key, self)\n        except TypeError:\n            return NotImplemented\n\n    @property\n    def parent(self):\n        \"\"\"The logical parent of the path.\"\"\"\n        drv = self.drive\n        root = self.root\n        tail = self._tail\n        if not tail:\n            return self\n        return self._from_parsed_parts(drv, root, tail[:-1])\n\n    @property\n    def parents(self):\n        \"\"\"A sequence of this path's logical parents.\"\"\"\n        # The value of this property should not be cached on the path object,\n        # as doing so would introduce a reference cycle.\n        return _PathParents(self)\n\n    def is_absolute(self):\n        \"\"\"True if the path is absolute (has both a root and, if applicable,\n        a drive).\"\"\"\n        if self._flavour is ntpath:\n            # ntpath.isabs() is defective - see GH-44626.\n            return bool(self.drive and self.root)\n        elif self._flavour is posixpath:\n            # Optimization: work with raw paths on POSIX.\n            for path in self._raw_paths:\n                if path.startswith('/'):\n                    return True\n            return False\n        else:\n            return self._flavour.isabs(str(self))\n\n    def is_reserved(self):\n        \"\"\"Return True if the path contains one of the special names reserved\n        by the system, if any.\"\"\"\n        if self._flavour is posixpath or not self._tail:\n            return False\n\n        # NOTE: the rules for reserved names seem somewhat complicated\n        # (e.g. r\"..\\NUL\" is reserved but not r\"foo\\NUL\" if \"foo\" does not\n        # exist). We err on the side of caution and return True for paths\n        # which are not considered reserved by Windows.\n        if self.drive.startswith('\\\\\\\\'):\n            # UNC paths are never reserved.\n            return False\n        name = self._tail[-1].partition('.')[0].partition(':')[0].rstrip(' ')\n        return name.upper() in _WIN_RESERVED_NAMES\n\n    def match(self, path_pattern, *, case_sensitive=None):\n        \"\"\"\n        Return True if this path matches the given pattern.\n        \"\"\"\n        if not isinstance(path_pattern, PurePath):\n            path_pattern = self.with_segments(path_pattern)\n        if case_sensitive is None:\n            case_sensitive = _is_case_sensitive(self._flavour)\n        pattern = _compile_pattern_lines(path_pattern._lines, case_sensitive)\n        if path_pattern.drive or path_pattern.root:\n            return pattern.match(self._lines) is not None\n        elif path_pattern._tail:\n            return pattern.search(self._lines) is not None\n        else:\n            raise ValueError(\"empty pattern\")\n\n\n# Can't subclass os.PathLike from PurePath and keep the constructor\n# optimizations in PurePath.__slots__.\nos.PathLike.register(PurePath)\n\n\nclass PurePosixPath(PurePath):\n    \"\"\"PurePath subclass for non-Windows systems.\n\n    On a POSIX system, instantiating a PurePath should return this object.\n    However, you can also instantiate it directly on any system.\n    \"\"\"\n    _flavour = posixpath\n    __slots__ = ()\n\n\nclass PureWindowsPath(PurePath):\n    \"\"\"PurePath subclass for Windows systems.\n\n    On a Windows system, instantiating a PurePath should return this object.\n    However, you can also instantiate it directly on any system.\n    \"\"\"\n    _flavour = ntpath\n    __slots__ = ()\n\n\n# Filesystem-accessing classes\n\n\nclass Path(PurePath):\n    \"\"\"PurePath subclass that can make system calls.\n\n    Path represents a filesystem path but unlike PurePath, also offers\n    methods to do system calls on path objects. Depending on your system,\n    instantiating a Path will return either a PosixPath or a WindowsPath\n    object. You can also instantiate a PosixPath or WindowsPath directly,\n    but cannot instantiate a WindowsPath on a POSIX system or vice versa.\n    \"\"\"\n    __slots__ = ()\n\n    def stat(self, *, follow_symlinks=True):\n        \"\"\"\n        Return the result of the stat() system call on this path, like\n        os.stat() does.\n        \"\"\"\n        return os.stat(self, follow_symlinks=follow_symlinks)\n\n    def lstat(self):\n        \"\"\"\n        Like stat(), except if the path points to a symlink, the symlink's\n        status information is returned, rather than its target's.\n        \"\"\"\n        return self.stat(follow_symlinks=False)\n\n\n    # Convenience functions for querying the stat results\n\n    def exists(self, *, follow_symlinks=True):\n        \"\"\"\n        Whether this path exists.\n\n        This method normally follows symlinks; to check whether a symlink exists,\n        add the argument follow_symlinks=False.\n        \"\"\"\n        try:\n            self.stat(follow_symlinks=follow_symlinks)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n        return True\n\n    def is_dir(self):\n        \"\"\"\n        Whether this path is a directory.\n        \"\"\"\n        try:\n            return S_ISDIR(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_file(self):\n        \"\"\"\n        Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n        \"\"\"\n        try:\n            return S_ISREG(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_mount(self):\n        \"\"\"\n        Check if this path is a mount point\n        \"\"\"\n        return self._flavour.ismount(self)\n\n    def is_symlink(self):\n        \"\"\"\n        Whether this path is a symbolic link.\n        \"\"\"\n        try:\n            return S_ISLNK(self.lstat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_junction(self):\n        \"\"\"\n        Whether this path is a junction.\n        \"\"\"\n        return self._flavour.isjunction(self)\n\n    def is_block_device(self):\n        \"\"\"\n        Whether this path is a block device.\n        \"\"\"\n        try:\n            return S_ISBLK(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_char_device(self):\n        \"\"\"\n        Whether this path is a character device.\n        \"\"\"\n        try:\n            return S_ISCHR(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_fifo(self):\n        \"\"\"\n        Whether this path is a FIFO.\n        \"\"\"\n        try:\n            return S_ISFIFO(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def is_socket(self):\n        \"\"\"\n        Whether this path is a socket.\n        \"\"\"\n        try:\n            return S_ISSOCK(self.stat().st_mode)\n        except OSError as e:\n            if not _ignore_error(e):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see http://web.archive.org/web/20200623061726/https://bitbucket.org/pitrou/pathlib/issues/12/ )\n            return False\n        except ValueError:\n            # Non-encodable path\n            return False\n\n    def samefile(self, other_path):\n        \"\"\"Return whether other_path is the same or not as this file\n        (as returned by os.path.samefile()).\n        \"\"\"\n        st = self.stat()\n        try:\n            other_st = other_path.stat()\n        except AttributeError:\n            other_st = self.with_segments(other_path).stat()\n        return self._flavour.samestat(st, other_st)\n\n    def open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"\n        Open the file pointed to by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n        return io.open(self, mode, buffering, encoding, errors, newline)\n\n    def read_bytes(self):\n        \"\"\"\n        Open the file in bytes mode, read it, and close the file.\n        \"\"\"\n        with self.open(mode='rb') as f:\n            return f.read()\n\n    def read_text(self, encoding=None, errors=None):\n        \"\"\"\n        Open the file in text mode, read it, and close the file.\n        \"\"\"\n        encoding = io.text_encoding(encoding)\n        with self.open(mode='r', encoding=encoding, errors=errors) as f:\n            return f.read()\n\n    def write_bytes(self, data):\n        \"\"\"\n        Open the file in bytes mode, write to it, and close the file.\n        \"\"\"\n        # type-check for the buffer interface before truncating the file\n        view = memoryview(data)\n        with self.open(mode='wb') as f:\n            return f.write(view)\n\n    def write_text(self, data, encoding=None, errors=None, newline=None):\n        \"\"\"\n        Open the file in text mode, write to it, and close the file.\n        \"\"\"\n        if not isinstance(data, str):\n            raise TypeError('data must be str, not %s' %\n                            data.__class__.__name__)\n        encoding = io.text_encoding(encoding)\n        with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) as f:\n            return f.write(data)\n\n    def iterdir(self):\n        \"\"\"Yield path objects of the directory contents.\n\n        The children are yielded in arbitrary order, and the\n        special entries '.' and '..' are not included.\n        \"\"\"\n        for name in os.listdir(self):\n            yield self._make_child_relpath(name)\n\n    def _scandir(self):\n        # bpo-24132: a future version of pathlib will support subclassing of\n        # pathlib.Path to customize how the filesystem is accessed. This\n        # includes scandir(), which is used to implement glob().\n        return os.scandir(self)\n\n    def _make_child_relpath(self, name):\n        path_str = str(self)\n        tail = self._tail\n        if tail:\n            path_str = f'{path_str}{self._flavour.sep}{name}'\n        elif path_str != '.':\n            path_str = f'{path_str}{name}'\n        else:\n            path_str = name\n        path = self.with_segments(path_str)\n        path._str = path_str\n        path._drv = self.drive\n        path._root = self.root\n        path._tail_cached = tail + [name]\n        return path\n\n    def glob(self, pattern, *, case_sensitive=None):\n        \"\"\"Iterate over this subtree and yield all existing files (of any\n        kind, including directories) matching the given relative pattern.\n        \"\"\"\n        sys.audit(\"pathlib.Path.glob\", self, pattern)\n        if not pattern:\n            raise ValueError(\"Unacceptable pattern: {!r}\".format(pattern))\n        drv, root, pattern_parts = self._parse_path(pattern)\n        if drv or root:\n            raise NotImplementedError(\"Non-relative patterns are unsupported\")\n        if pattern[-1] in (self._flavour.sep, self._flavour.altsep):\n            pattern_parts.append('')\n        selector = _make_selector(tuple(pattern_parts), self._flavour, case_sensitive)\n        for p in selector.select_from(self):\n            yield p\n\n    def rglob(self, pattern, *, case_sensitive=None):\n        \"\"\"Recursively yield all existing files (of any kind, including\n        directories) matching the given relative pattern, anywhere in\n        this subtree.\n        \"\"\"\n        sys.audit(\"pathlib.Path.rglob\", self, pattern)\n        drv, root, pattern_parts = self._parse_path(pattern)\n        if drv or root:\n            raise NotImplementedError(\"Non-relative patterns are unsupported\")\n        if pattern and pattern[-1] in (self._flavour.sep, self._flavour.altsep):\n            pattern_parts.append('')\n        selector = _make_selector((\"**\",) + tuple(pattern_parts), self._flavour, case_sensitive)\n        for p in selector.select_from(self):\n            yield p\n\n    def walk(self, top_down=True, on_error=None, follow_symlinks=False):\n        \"\"\"Walk the directory tree from this directory, similar to os.walk().\"\"\"\n        sys.audit(\"pathlib.Path.walk\", self, on_error, follow_symlinks)\n        paths = [self]\n\n        while paths:\n            path = paths.pop()\n            if isinstance(path, tuple):\n                yield path\n                continue\n\n            # We may not have read permission for self, in which case we can't\n            # get a list of the files the directory contains. os.walk()\n            # always suppressed the exception in that instance, rather than\n            # blow up for a minor reason when (say) a thousand readable\n            # directories are still left to visit. That logic is copied here.\n            try:\n                scandir_it = path._scandir()\n            except OSError as error:\n                if on_error is not None:\n                    on_error(error)\n                continue\n\n            with scandir_it:\n                dirnames = []\n                filenames = []\n                for entry in scandir_it:\n                    try:\n                        is_dir = entry.is_dir(follow_symlinks=follow_symlinks)\n                    except OSError:\n                        # Carried over from os.path.isdir().\n                        is_dir = False\n\n                    if is_dir:\n                        dirnames.append(entry.name)\n                    else:\n                        filenames.append(entry.name)\n\n            if top_down:\n                yield path, dirnames, filenames\n            else:\n                paths.append((path, dirnames, filenames))\n\n            paths += [path._make_child_relpath(d) for d in reversed(dirnames)]\n\n    def __init__(self, *args, **kwargs):\n        if kwargs:\n            msg = (\"support for supplying keyword arguments to pathlib.PurePath \"\n                   \"is deprecated and scheduled for removal in Python {remove}\")\n            warnings._deprecated(\"pathlib.PurePath(**kwargs)\", msg, remove=(3, 14))\n        super().__init__(*args)\n\n    def __new__(cls, *args, **kwargs):\n        if cls is Path:\n            cls = WindowsPath if os.name == 'nt' else PosixPath\n        return object.__new__(cls)\n\n    def __enter__(self):\n        # In previous versions of pathlib, __exit__() marked this path as\n        # closed; subsequent attempts to perform I/O would raise an IOError.\n        # This functionality was never documented, and had the effect of\n        # making Path objects mutable, contrary to PEP 428.\n        # In Python 3.9 __exit__() was made a no-op.\n        # In Python 3.11 __enter__() began emitting DeprecationWarning.\n        # In Python 3.13 __enter__() and __exit__() should be removed.\n        warnings.warn(\"pathlib.Path.__enter__() is deprecated and scheduled \"\n                      \"for removal in Python 3.13; Path objects as a context \"\n                      \"manager is a no-op\",\n                      DeprecationWarning, stacklevel=2)\n        return self\n\n    def __exit__(self, t, v, tb):\n        pass\n\n    # Public API\n\n    @classmethod\n    def cwd(cls):\n        \"\"\"Return a new path pointing to the current working directory.\"\"\"\n        # We call 'absolute()' rather than using 'os.getcwd()' directly to\n        # enable users to replace the implementation of 'absolute()' in a\n        # subclass and benefit from the new behaviour here. This works because\n        # os.path.abspath('.') == os.getcwd().\n        return cls().absolute()\n\n    @classmethod\n    def home(cls):\n        \"\"\"Return a new path pointing to the user's home directory (as\n        returned by os.path.expanduser('~')).\n        \"\"\"\n        return cls(\"~\").expanduser()\n\n    def absolute(self):\n        \"\"\"Return an absolute version of this path by prepending the current\n        working directory. No normalization or symlink resolution is performed.\n\n        Use resolve() to get the canonical path to a file.\n        \"\"\"\n        if self.is_absolute():\n            return self\n        elif self.drive:\n            # There is a CWD on each drive-letter drive.\n            cwd = self._flavour.abspath(self.drive)\n        else:\n            cwd = os.getcwd()\n            # Fast path for \"empty\" paths, e.g. Path(\".\"), Path(\"\") or Path().\n            # We pass only one argument to with_segments() to avoid the cost\n            # of joining, and we exploit the fact that getcwd() returns a\n            # fully-normalized string by storing it in _str. This is used to\n            # implement Path.cwd().\n            if not self.root and not self._tail:\n                result = self.with_segments(cwd)\n                result._str = cwd\n                return result\n        return self.with_segments(cwd, self)\n\n    def resolve(self, strict=False):\n        \"\"\"\n        Make the path absolute, resolving all symlinks on the way and also\n        normalizing it.\n        \"\"\"\n\n        def check_eloop(e):\n            winerror = getattr(e, 'winerror', 0)\n            if e.errno == ELOOP or winerror == _WINERROR_CANT_RESOLVE_FILENAME:\n                raise RuntimeError(\"Symlink loop from %r\" % e.filename)\n\n        try:\n            s = self._flavour.realpath(self, strict=strict)\n        except OSError as e:\n            check_eloop(e)\n            raise\n        p = self.with_segments(s)\n\n        # In non-strict mode, realpath() doesn't raise on symlink loops.\n        # Ensure we get an exception by calling stat()\n        if not strict:\n            try:\n                p.stat()\n            except OSError as e:\n                check_eloop(e)\n        return p\n\n    def owner(self):\n        \"\"\"\n        Return the login name of the file owner.\n        \"\"\"\n        try:\n            import pwd\n            return pwd.getpwuid(self.stat().st_uid).pw_name\n        except ImportError:\n            raise NotImplementedError(\"Path.owner() is unsupported on this system\")\n\n    def group(self):\n        \"\"\"\n        Return the group name of the file gid.\n        \"\"\"\n\n        try:\n            import grp\n            return grp.getgrgid(self.stat().st_gid).gr_name\n        except ImportError:\n            raise NotImplementedError(\"Path.group() is unsupported on this system\")\n\n    def readlink(self):\n        \"\"\"\n        Return the path to which the symbolic link points.\n        \"\"\"\n        if not hasattr(os, \"readlink\"):\n            raise NotImplementedError(\"os.readlink() not available on this system\")\n        return self.with_segments(os.readlink(self))\n\n    def touch(self, mode=0o666, exist_ok=True):\n        \"\"\"\n        Create this file with the given access mode, if it doesn't exist.\n        \"\"\"\n\n        if exist_ok:\n            # First try to bump modification time\n            # Implementation note: GNU touch uses the UTIME_NOW option of\n            # the utimensat() / futimens() functions.\n            try:\n                os.utime(self, None)\n            except OSError:\n                # Avoid exception chaining\n                pass\n            else:\n                return\n        flags = os.O_CREAT | os.O_WRONLY\n        if not exist_ok:\n            flags |= os.O_EXCL\n        fd = os.open(self, flags, mode)\n        os.close(fd)\n\n    def mkdir(self, mode=0o777, parents=False, exist_ok=False):\n        \"\"\"\n        Create a new directory at this given path.\n        \"\"\"\n        try:\n            os.mkdir(self, mode)\n        except FileNotFoundError:\n            if not parents or self.parent == self:\n                raise\n            self.parent.mkdir(parents=True, exist_ok=True)\n            self.mkdir(mode, parents=False, exist_ok=exist_ok)\n        except OSError:\n            # Cannot rely on checking for EEXIST, since the operating system\n            # could give priority to other errors like EACCES or EROFS\n            if not exist_ok or not self.is_dir():\n                raise\n\n    def chmod(self, mode, *, follow_symlinks=True):\n        \"\"\"\n        Change the permissions of the path, like os.chmod().\n        \"\"\"\n        os.chmod(self, mode, follow_symlinks=follow_symlinks)\n\n    def lchmod(self, mode):\n        \"\"\"\n        Like chmod(), except if the path points to a symlink, the symlink's\n        permissions are changed, rather than its target's.\n        \"\"\"\n        self.chmod(mode, follow_symlinks=False)\n\n    def unlink(self, missing_ok=False):\n        \"\"\"\n        Remove this file or link.\n        If the path is a directory, use rmdir() instead.\n        \"\"\"\n        try:\n            os.unlink(self)\n        except FileNotFoundError:\n            if not missing_ok:\n                raise\n\n    def rmdir(self):\n        \"\"\"\n        Remove this directory.  The directory must be empty.\n        \"\"\"\n        os.rmdir(self)\n\n    def rename(self, target):\n        \"\"\"\n        Rename this path to the target path.\n\n        The target path may be absolute or relative. Relative paths are\n        interpreted relative to the current working directory, *not* the\n        directory of the Path object.\n\n        Returns the new Path instance pointing to the target path.\n        \"\"\"\n        os.rename(self, target)\n        return self.with_segments(target)\n\n    def replace(self, target):\n        \"\"\"\n        Rename this path to the target path, overwriting if that path exists.\n\n        The target path may be absolute or relative. Relative paths are\n        interpreted relative to the current working directory, *not* the\n        directory of the Path object.\n\n        Returns the new Path instance pointing to the target path.\n        \"\"\"\n        os.replace(self, target)\n        return self.with_segments(target)\n\n    def symlink_to(self, target, target_is_directory=False):\n        \"\"\"\n        Make this path a symlink pointing to the target path.\n        Note the order of arguments (link, target) is the reverse of os.symlink.\n        \"\"\"\n        if not hasattr(os, \"symlink\"):\n            raise NotImplementedError(\"os.symlink() not available on this system\")\n        os.symlink(target, self, target_is_directory)\n\n    def hardlink_to(self, target):\n        \"\"\"\n        Make this path a hard link pointing to the same file as *target*.\n\n        Note the order of arguments (self, target) is the reverse of os.link's.\n        \"\"\"\n        if not hasattr(os, \"link\"):\n            raise NotImplementedError(\"os.link() not available on this system\")\n        os.link(target, self)\n\n    def expanduser(self):\n        \"\"\" Return a new path with expanded ~ and ~user constructs\n        (as returned by os.path.expanduser)\n        \"\"\"\n        if (not (self.drive or self.root) and\n            self._tail and self._tail[0][:1] == '~'):\n            homedir = self._flavour.expanduser(self._tail[0])\n            if homedir[:1] == \"~\":\n                raise RuntimeError(\"Could not determine home directory.\")\n            drv, root, tail = self._parse_path(homedir)\n            return self._from_parsed_parts(drv, root, tail + self._tail[1:])\n\n        return self\n\n\nclass PosixPath(Path, PurePosixPath):\n    \"\"\"Path subclass for non-Windows systems.\n\n    On a POSIX system, instantiating a Path should return this object.\n    \"\"\"\n    __slots__ = ()\n\n    if os.name == 'nt':\n        def __new__(cls, *args, **kwargs):\n            raise NotImplementedError(\n                f\"cannot instantiate {cls.__name__!r} on your system\")\n\nclass WindowsPath(Path, PureWindowsPath):\n    \"\"\"Path subclass for Windows systems.\n\n    On a Windows system, instantiating a Path should return this object.\n    \"\"\"\n    __slots__ = ()\n\n    if os.name != 'nt':\n        def __new__(cls, *args, **kwargs):\n            raise NotImplementedError(\n                f\"cannot instantiate {cls.__name__!r} on your system\")\n", 1435], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\zipfile\\__init__.py": ["\"\"\"\nRead and write ZIP files.\n\nXXX references to utf-8 need further investigation.\n\"\"\"\nimport binascii\nimport importlib.util\nimport io\nimport os\nimport shutil\nimport stat\nimport struct\nimport sys\nimport threading\nimport time\n\ntry:\n    import zlib # We may need its compression method\n    crc32 = zlib.crc32\nexcept ImportError:\n    zlib = None\n    crc32 = binascii.crc32\n\ntry:\n    import bz2 # We may need its compression method\nexcept ImportError:\n    bz2 = None\n\ntry:\n    import lzma # We may need its compression method\nexcept ImportError:\n    lzma = None\n\n__all__ = [\"BadZipFile\", \"BadZipfile\", \"error\",\n           \"ZIP_STORED\", \"ZIP_DEFLATED\", \"ZIP_BZIP2\", \"ZIP_LZMA\",\n           \"is_zipfile\", \"ZipInfo\", \"ZipFile\", \"PyZipFile\", \"LargeZipFile\",\n           \"Path\"]\n\nclass BadZipFile(Exception):\n    pass\n\n\nclass LargeZipFile(Exception):\n    \"\"\"\n    Raised when writing a zipfile, the zipfile requires ZIP64 extensions\n    and those extensions are disabled.\n    \"\"\"\n\nerror = BadZipfile = BadZipFile      # Pre-3.2 compatibility names\n\n\nZIP64_LIMIT = (1 << 31) - 1\nZIP_FILECOUNT_LIMIT = (1 << 16) - 1\nZIP_MAX_COMMENT = (1 << 16) - 1\n\n# constants for Zip file compression methods\nZIP_STORED = 0\nZIP_DEFLATED = 8\nZIP_BZIP2 = 12\nZIP_LZMA = 14\n# Other ZIP compression methods not supported\n\nDEFAULT_VERSION = 20\nZIP64_VERSION = 45\nBZIP2_VERSION = 46\nLZMA_VERSION = 63\n# we recognize (but not necessarily support) all features up to that version\nMAX_EXTRACT_VERSION = 63\n\n# Below are some formats and associated data for reading/writing headers using\n# the struct module.  The names and structures of headers/records are those used\n# in the PKWARE description of the ZIP file format:\n#     http://www.pkware.com/documents/casestudies/APPNOTE.TXT\n# (URL valid as of January 2008)\n\n# The \"end of central directory\" structure, magic number, size, and indices\n# (section V.I in the format document)\nstructEndArchive = b\"<4s4H2LH\"\nstringEndArchive = b\"PK\\005\\006\"\nsizeEndCentDir = struct.calcsize(structEndArchive)\n\n_ECD_SIGNATURE = 0\n_ECD_DISK_NUMBER = 1\n_ECD_DISK_START = 2\n_ECD_ENTRIES_THIS_DISK = 3\n_ECD_ENTRIES_TOTAL = 4\n_ECD_SIZE = 5\n_ECD_OFFSET = 6\n_ECD_COMMENT_SIZE = 7\n# These last two indices are not part of the structure as defined in the\n# spec, but they are used internally by this module as a convenience\n_ECD_COMMENT = 8\n_ECD_LOCATION = 9\n\n# The \"central directory\" structure, magic number, size, and indices\n# of entries in the structure (section V.F in the format document)\nstructCentralDir = \"<4s4B4HL2L5H2L\"\nstringCentralDir = b\"PK\\001\\002\"\nsizeCentralDir = struct.calcsize(structCentralDir)\n\n# indexes of entries in the central directory structure\n_CD_SIGNATURE = 0\n_CD_CREATE_VERSION = 1\n_CD_CREATE_SYSTEM = 2\n_CD_EXTRACT_VERSION = 3\n_CD_EXTRACT_SYSTEM = 4\n_CD_FLAG_BITS = 5\n_CD_COMPRESS_TYPE = 6\n_CD_TIME = 7\n_CD_DATE = 8\n_CD_CRC = 9\n_CD_COMPRESSED_SIZE = 10\n_CD_UNCOMPRESSED_SIZE = 11\n_CD_FILENAME_LENGTH = 12\n_CD_EXTRA_FIELD_LENGTH = 13\n_CD_COMMENT_LENGTH = 14\n_CD_DISK_NUMBER_START = 15\n_CD_INTERNAL_FILE_ATTRIBUTES = 16\n_CD_EXTERNAL_FILE_ATTRIBUTES = 17\n_CD_LOCAL_HEADER_OFFSET = 18\n\n# General purpose bit flags\n# Zip Appnote: 4.4.4 general purpose bit flag: (2 bytes)\n_MASK_ENCRYPTED = 1 << 0\n# Bits 1 and 2 have different meanings depending on the compression used.\n_MASK_COMPRESS_OPTION_1 = 1 << 1\n# _MASK_COMPRESS_OPTION_2 = 1 << 2\n# _MASK_USE_DATA_DESCRIPTOR: If set, crc-32, compressed size and uncompressed\n# size are zero in the local header and the real values are written in the data\n# descriptor immediately following the compressed data.\n_MASK_USE_DATA_DESCRIPTOR = 1 << 3\n# Bit 4: Reserved for use with compression method 8, for enhanced deflating.\n# _MASK_RESERVED_BIT_4 = 1 << 4\n_MASK_COMPRESSED_PATCH = 1 << 5\n_MASK_STRONG_ENCRYPTION = 1 << 6\n# _MASK_UNUSED_BIT_7 = 1 << 7\n# _MASK_UNUSED_BIT_8 = 1 << 8\n# _MASK_UNUSED_BIT_9 = 1 << 9\n# _MASK_UNUSED_BIT_10 = 1 << 10\n_MASK_UTF_FILENAME = 1 << 11\n# Bit 12: Reserved by PKWARE for enhanced compression.\n# _MASK_RESERVED_BIT_12 = 1 << 12\n# _MASK_ENCRYPTED_CENTRAL_DIR = 1 << 13\n# Bit 14, 15: Reserved by PKWARE\n# _MASK_RESERVED_BIT_14 = 1 << 14\n# _MASK_RESERVED_BIT_15 = 1 << 15\n\n# The \"local file header\" structure, magic number, size, and indices\n# (section V.A in the format document)\nstructFileHeader = \"<4s2B4HL2L2H\"\nstringFileHeader = b\"PK\\003\\004\"\nsizeFileHeader = struct.calcsize(structFileHeader)\n\n_FH_SIGNATURE = 0\n_FH_EXTRACT_VERSION = 1\n_FH_EXTRACT_SYSTEM = 2\n_FH_GENERAL_PURPOSE_FLAG_BITS = 3\n_FH_COMPRESSION_METHOD = 4\n_FH_LAST_MOD_TIME = 5\n_FH_LAST_MOD_DATE = 6\n_FH_CRC = 7\n_FH_COMPRESSED_SIZE = 8\n_FH_UNCOMPRESSED_SIZE = 9\n_FH_FILENAME_LENGTH = 10\n_FH_EXTRA_FIELD_LENGTH = 11\n\n# The \"Zip64 end of central directory locator\" structure, magic number, and size\nstructEndArchive64Locator = \"<4sLQL\"\nstringEndArchive64Locator = b\"PK\\x06\\x07\"\nsizeEndCentDir64Locator = struct.calcsize(structEndArchive64Locator)\n\n# The \"Zip64 end of central directory\" record, magic number, size, and indices\n# (section V.G in the format document)\nstructEndArchive64 = \"<4sQ2H2L4Q\"\nstringEndArchive64 = b\"PK\\x06\\x06\"\nsizeEndCentDir64 = struct.calcsize(structEndArchive64)\n\n_CD64_SIGNATURE = 0\n_CD64_DIRECTORY_RECSIZE = 1\n_CD64_CREATE_VERSION = 2\n_CD64_EXTRACT_VERSION = 3\n_CD64_DISK_NUMBER = 4\n_CD64_DISK_NUMBER_START = 5\n_CD64_NUMBER_ENTRIES_THIS_DISK = 6\n_CD64_NUMBER_ENTRIES_TOTAL = 7\n_CD64_DIRECTORY_SIZE = 8\n_CD64_OFFSET_START_CENTDIR = 9\n\n_DD_SIGNATURE = 0x08074b50\n\n_EXTRA_FIELD_STRUCT = struct.Struct('<HH')\n\ndef _strip_extra(extra, xids):\n    # Remove Extra Fields with specified IDs.\n    unpack = _EXTRA_FIELD_STRUCT.unpack\n    modified = False\n    buffer = []\n    start = i = 0\n    while i + 4 <= len(extra):\n        xid, xlen = unpack(extra[i : i + 4])\n        j = i + 4 + xlen\n        if xid in xids:\n            if i != start:\n                buffer.append(extra[start : i])\n            start = j\n            modified = True\n        i = j\n    if not modified:\n        return extra\n    if start != len(extra):\n        buffer.append(extra[start:])\n    return b''.join(buffer)\n\ndef _check_zipfile(fp):\n    try:\n        if _EndRecData(fp):\n            return True         # file has correct magic number\n    except OSError:\n        pass\n    return False\n\ndef is_zipfile(filename):\n    \"\"\"Quickly see if a file is a ZIP file by checking the magic number.\n\n    The filename argument may be a file or file-like object too.\n    \"\"\"\n    result = False\n    try:\n        if hasattr(filename, \"read\"):\n            result = _check_zipfile(fp=filename)\n        else:\n            with open(filename, \"rb\") as fp:\n                result = _check_zipfile(fp)\n    except (OSError, BadZipFile):\n        pass\n    return result\n\ndef _EndRecData64(fpin, offset, endrec):\n    \"\"\"\n    Read the ZIP64 end-of-archive records and use that to update endrec\n    \"\"\"\n    offset -= sizeEndCentDir64Locator\n    if offset < 0:\n        # The file is not large enough to contain a ZIP64\n        # end-of-archive record, so just return the end record we were given.\n        return endrec\n    fpin.seek(offset)\n    data = fpin.read(sizeEndCentDir64Locator)\n    if len(data) != sizeEndCentDir64Locator:\n        raise OSError(\"Unknown I/O error\")\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\n    if sig != stringEndArchive64Locator:\n        return endrec\n\n    if diskno != 0 or disks > 1:\n        raise BadZipFile(\"zipfiles that span multiple disks are not supported\")\n\n    offset -= sizeEndCentDir64\n    if reloff > offset:\n        raise BadZipFile(\"Corrupt zip64 end of central directory locator\")\n    # First, check the assumption that there is no prepended data.\n    fpin.seek(reloff)\n    extrasz = offset - reloff\n    data = fpin.read(sizeEndCentDir64)\n    if len(data) != sizeEndCentDir64:\n        raise OSError(\"Unknown I/O error\")\n    if not data.startswith(stringEndArchive64) and reloff != offset:\n        # Since we already have seen the Zip64 EOCD Locator, it's\n        # possible we got here because there is prepended data.\n        # Assume no 'zip64 extensible data'\n        fpin.seek(offset)\n        extrasz = 0\n        data = fpin.read(sizeEndCentDir64)\n        if len(data) != sizeEndCentDir64:\n            raise OSError(\"Unknown I/O error\")\n    if not data.startswith(stringEndArchive64):\n        raise BadZipFile(\"Zip64 end of central directory record not found\")\n\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\n        dircount, dircount2, dirsize, diroffset = \\\n        struct.unpack(structEndArchive64, data)\n    if (diroffset + dirsize != reloff or\n        sz + 12 != sizeEndCentDir64 + extrasz):\n        raise BadZipFile(\"Corrupt zip64 end of central directory record\")\n\n    # Update the original endrec using data from the ZIP64 record\n    endrec[_ECD_SIGNATURE] = sig\n    endrec[_ECD_DISK_NUMBER] = disk_num\n    endrec[_ECD_DISK_START] = disk_dir\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\n    endrec[_ECD_SIZE] = dirsize\n    endrec[_ECD_OFFSET] = diroffset\n    endrec[_ECD_LOCATION] = offset - extrasz\n    return endrec\n\n\ndef _EndRecData(fpin):\n    \"\"\"Return data from the \"End of Central Directory\" record, or None.\n\n    The data is a list of the nine items in the ZIP \"End of central dir\"\n    record followed by a tenth item, the file seek offset of this record.\"\"\"\n\n    # Determine file size\n    fpin.seek(0, 2)\n    filesize = fpin.tell()\n\n    # Check to see if this is ZIP file with no archive comment (the\n    # \"end of central directory\" structure should be the last item in the\n    # file if this is the case).\n    try:\n        fpin.seek(-sizeEndCentDir, 2)\n    except OSError:\n        return None\n    data = fpin.read(sizeEndCentDir)\n    if (len(data) == sizeEndCentDir and\n        data[0:4] == stringEndArchive and\n        data[-2:] == b\"\\000\\000\"):\n        # the signature is correct and there's no comment, unpack structure\n        endrec = struct.unpack(structEndArchive, data)\n        endrec=list(endrec)\n\n        # Append a blank comment and record start offset\n        endrec.append(b\"\")\n        endrec.append(filesize - sizeEndCentDir)\n\n        # Try to read the \"Zip64 end of central directory\" structure\n        return _EndRecData64(fpin, filesize - sizeEndCentDir, endrec)\n\n    # Either this is not a ZIP file, or it is a ZIP file with an archive\n    # comment.  Search the end of the file for the \"end of central directory\"\n    # record signature. The comment is the last item in the ZIP file and may be\n    # up to 64K long.  It is assumed that the \"end of central directory\" magic\n    # number does not appear in the comment.\n    maxCommentStart = max(filesize - ZIP_MAX_COMMENT - sizeEndCentDir, 0)\n    fpin.seek(maxCommentStart, 0)\n    data = fpin.read(ZIP_MAX_COMMENT + sizeEndCentDir)\n    start = data.rfind(stringEndArchive)\n    if start >= 0:\n        # found the magic number; attempt to unpack and interpret\n        recData = data[start:start+sizeEndCentDir]\n        if len(recData) != sizeEndCentDir:\n            # Zip file is corrupted.\n            return None\n        endrec = list(struct.unpack(structEndArchive, recData))\n        commentSize = endrec[_ECD_COMMENT_SIZE] #as claimed by the zip file\n        comment = data[start+sizeEndCentDir:start+sizeEndCentDir+commentSize]\n        endrec.append(comment)\n        endrec.append(maxCommentStart + start)\n\n        # Try to read the \"Zip64 end of central directory\" structure\n        return _EndRecData64(fpin, maxCommentStart + start, endrec)\n\n    # Unable to find a valid end of central directory structure\n    return None\n\ndef _sanitize_filename(filename):\n    \"\"\"Terminate the file name at the first null byte and\n    ensure paths always use forward slashes as the directory separator.\"\"\"\n\n    # Terminate the file name at the first null byte.  Null bytes in file\n    # names are used as tricks by viruses in archives.\n    null_byte = filename.find(chr(0))\n    if null_byte >= 0:\n        filename = filename[0:null_byte]\n    # This is used to ensure paths in generated ZIP files always use\n    # forward slashes as the directory separator, as required by the\n    # ZIP format specification.\n    if os.sep != \"/\" and os.sep in filename:\n        filename = filename.replace(os.sep, \"/\")\n    if os.altsep and os.altsep != \"/\" and os.altsep in filename:\n        filename = filename.replace(os.altsep, \"/\")\n    return filename\n\n\nclass ZipInfo (object):\n    \"\"\"Class with attributes describing each file in the ZIP archive.\"\"\"\n\n    __slots__ = (\n        'orig_filename',\n        'filename',\n        'date_time',\n        'compress_type',\n        '_compresslevel',\n        'comment',\n        'extra',\n        'create_system',\n        'create_version',\n        'extract_version',\n        'reserved',\n        'flag_bits',\n        'volume',\n        'internal_attr',\n        'external_attr',\n        'header_offset',\n        'CRC',\n        'compress_size',\n        'file_size',\n        '_raw_time',\n        '_end_offset',\n    )\n\n    def __init__(self, filename=\"NoName\", date_time=(1980,1,1,0,0,0)):\n        self.orig_filename = filename   # Original file name in archive\n\n        # Terminate the file name at the first null byte and\n        # ensure paths always use forward slashes as the directory separator.\n        filename = _sanitize_filename(filename)\n\n        self.filename = filename        # Normalized file name\n        self.date_time = date_time      # year, month, day, hour, min, sec\n\n        if date_time[0] < 1980:\n            raise ValueError('ZIP does not support timestamps before 1980')\n\n        # Standard values:\n        self.compress_type = ZIP_STORED # Type of compression for the file\n        self._compresslevel = None      # Level for the compressor\n        self.comment = b\"\"              # Comment for each file\n        self.extra = b\"\"                # ZIP extra data\n        if sys.platform == 'win32':\n            self.create_system = 0          # System which created ZIP archive\n        else:\n            # Assume everything else is unix-y\n            self.create_system = 3          # System which created ZIP archive\n        self.create_version = DEFAULT_VERSION  # Version which created ZIP archive\n        self.extract_version = DEFAULT_VERSION # Version needed to extract archive\n        self.reserved = 0               # Must be zero\n        self.flag_bits = 0              # ZIP flag bits\n        self.volume = 0                 # Volume number of file header\n        self.internal_attr = 0          # Internal attributes\n        self.external_attr = 0          # External file attributes\n        self.compress_size = 0          # Size of the compressed file\n        self.file_size = 0              # Size of the uncompressed file\n        self._end_offset = None         # Start of the next local header or central directory\n        # Other attributes are set by class ZipFile:\n        # header_offset         Byte offset to the file header\n        # CRC                   CRC-32 of the uncompressed file\n\n    def __repr__(self):\n        result = ['<%s filename=%r' % (self.__class__.__name__, self.filename)]\n        if self.compress_type != ZIP_STORED:\n            result.append(' compress_type=%s' %\n                          compressor_names.get(self.compress_type,\n                                               self.compress_type))\n        hi = self.external_attr >> 16\n        lo = self.external_attr & 0xFFFF\n        if hi:\n            result.append(' filemode=%r' % stat.filemode(hi))\n        if lo:\n            result.append(' external_attr=%#x' % lo)\n        isdir = self.is_dir()\n        if not isdir or self.file_size:\n            result.append(' file_size=%r' % self.file_size)\n        if ((not isdir or self.compress_size) and\n            (self.compress_type != ZIP_STORED or\n             self.file_size != self.compress_size)):\n            result.append(' compress_size=%r' % self.compress_size)\n        result.append('>')\n        return ''.join(result)\n\n    def FileHeader(self, zip64=None):\n        \"\"\"Return the per-file header as a bytes object.\n\n        When the optional zip64 arg is None rather than a bool, we will\n        decide based upon the file_size and compress_size, if known,\n        False otherwise.\n        \"\"\"\n        dt = self.date_time\n        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n        if self.flag_bits & _MASK_USE_DATA_DESCRIPTOR:\n            # Set these to zero because we write them after the file data\n            CRC = compress_size = file_size = 0\n        else:\n            CRC = self.CRC\n            compress_size = self.compress_size\n            file_size = self.file_size\n\n        extra = self.extra\n\n        min_version = 0\n        if zip64 is None:\n            # We always explicitly pass zip64 within this module.... This\n            # remains for anyone using ZipInfo.FileHeader as a public API.\n            zip64 = file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT\n        if zip64:\n            fmt = '<HHQQ'\n            extra = extra + struct.pack(fmt,\n                                        1, struct.calcsize(fmt)-4, file_size, compress_size)\n            file_size = 0xffffffff\n            compress_size = 0xffffffff\n            min_version = ZIP64_VERSION\n\n        if self.compress_type == ZIP_BZIP2:\n            min_version = max(BZIP2_VERSION, min_version)\n        elif self.compress_type == ZIP_LZMA:\n            min_version = max(LZMA_VERSION, min_version)\n\n        self.extract_version = max(min_version, self.extract_version)\n        self.create_version = max(min_version, self.create_version)\n        filename, flag_bits = self._encodeFilenameFlags()\n        header = struct.pack(structFileHeader, stringFileHeader,\n                             self.extract_version, self.reserved, flag_bits,\n                             self.compress_type, dostime, dosdate, CRC,\n                             compress_size, file_size,\n                             len(filename), len(extra))\n        return header + filename + extra\n\n    def _encodeFilenameFlags(self):\n        try:\n            return self.filename.encode('ascii'), self.flag_bits\n        except UnicodeEncodeError:\n            return self.filename.encode('utf-8'), self.flag_bits | _MASK_UTF_FILENAME\n\n    def _decodeExtra(self, filename_crc):\n        # Try to decode the extra field.\n        extra = self.extra\n        unpack = struct.unpack\n        while len(extra) >= 4:\n            tp, ln = unpack('<HH', extra[:4])\n            if ln+4 > len(extra):\n                raise BadZipFile(\"Corrupt extra field %04x (size=%d)\" % (tp, ln))\n            if tp == 0x0001:\n                data = extra[4:ln+4]\n                # ZIP64 extension (large files and/or large archives)\n                try:\n                    if self.file_size in (0xFFFF_FFFF_FFFF_FFFF, 0xFFFF_FFFF):\n                        field = \"File size\"\n                        self.file_size, = unpack('<Q', data[:8])\n                        data = data[8:]\n                    if self.compress_size == 0xFFFF_FFFF:\n                        field = \"Compress size\"\n                        self.compress_size, = unpack('<Q', data[:8])\n                        data = data[8:]\n                    if self.header_offset == 0xFFFF_FFFF:\n                        field = \"Header offset\"\n                        self.header_offset, = unpack('<Q', data[:8])\n                except struct.error:\n                    raise BadZipFile(f\"Corrupt zip64 extra field. \"\n                                     f\"{field} not found.\") from None\n            elif tp == 0x7075:\n                data = extra[4:ln+4]\n                # Unicode Path Extra Field\n                try:\n                    up_version, up_name_crc = unpack('<BL', data[:5])\n                    if up_version == 1 and up_name_crc == filename_crc:\n                        up_unicode_name = data[5:].decode('utf-8')\n                        if up_unicode_name:\n                            self.filename = _sanitize_filename(up_unicode_name)\n                        else:\n                            import warnings\n                            warnings.warn(\"Empty unicode path extra field (0x7075)\", stacklevel=2)\n                except struct.error as e:\n                    raise BadZipFile(\"Corrupt unicode path extra field (0x7075)\") from e\n                except UnicodeDecodeError as e:\n                    raise BadZipFile('Corrupt unicode path extra field (0x7075): invalid utf-8 bytes') from e\n\n            extra = extra[ln+4:]\n\n    @classmethod\n    def from_file(cls, filename, arcname=None, *, strict_timestamps=True):\n        \"\"\"Construct an appropriate ZipInfo for a file on the filesystem.\n\n        filename should be the path to a file or directory on the filesystem.\n\n        arcname is the name which it will have within the archive (by default,\n        this will be the same as filename, but without a drive letter and with\n        leading path separators removed).\n        \"\"\"\n        if isinstance(filename, os.PathLike):\n            filename = os.fspath(filename)\n        st = os.stat(filename)\n        isdir = stat.S_ISDIR(st.st_mode)\n        mtime = time.localtime(st.st_mtime)\n        date_time = mtime[0:6]\n        if not strict_timestamps and date_time[0] < 1980:\n            date_time = (1980, 1, 1, 0, 0, 0)\n        elif not strict_timestamps and date_time[0] > 2107:\n            date_time = (2107, 12, 31, 23, 59, 59)\n        # Create ZipInfo instance to store file information\n        if arcname is None:\n            arcname = filename\n        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])\n        while arcname[0] in (os.sep, os.altsep):\n            arcname = arcname[1:]\n        if isdir:\n            arcname += '/'\n        zinfo = cls(arcname, date_time)\n        zinfo.external_attr = (st.st_mode & 0xFFFF) << 16  # Unix attributes\n        if isdir:\n            zinfo.file_size = 0\n            zinfo.external_attr |= 0x10  # MS-DOS directory flag\n        else:\n            zinfo.file_size = st.st_size\n\n        return zinfo\n\n    def is_dir(self):\n        \"\"\"Return True if this archive member is a directory.\"\"\"\n        if self.filename.endswith('/'):\n            return True\n        # The ZIP format specification requires to use forward slashes\n        # as the directory separator, but in practice some ZIP files\n        # created on Windows can use backward slashes.  For compatibility\n        # with the extraction code which already handles this:\n        if os.path.altsep:\n            return self.filename.endswith((os.path.sep, os.path.altsep))\n        return False\n\n\n# ZIP encryption uses the CRC32 one-byte primitive for scrambling some\n# internal keys. We noticed that a direct implementation is faster than\n# relying on binascii.crc32().\n\n_crctable = None\ndef _gen_crc(crc):\n    for j in range(8):\n        if crc & 1:\n            crc = (crc >> 1) ^ 0xEDB88320\n        else:\n            crc >>= 1\n    return crc\n\n# ZIP supports a password-based form of encryption. Even though known\n# plaintext attacks have been found against it, it is still useful\n# to be able to get data out of such a file.\n#\n# Usage:\n#     zd = _ZipDecrypter(mypwd)\n#     plain_bytes = zd(cypher_bytes)\n\ndef _ZipDecrypter(pwd):\n    key0 = 305419896\n    key1 = 591751049\n    key2 = 878082192\n\n    global _crctable\n    if _crctable is None:\n        _crctable = list(map(_gen_crc, range(256)))\n    crctable = _crctable\n\n    def crc32(ch, crc):\n        \"\"\"Compute the CRC32 primitive on one byte.\"\"\"\n        return (crc >> 8) ^ crctable[(crc ^ ch) & 0xFF]\n\n    def update_keys(c):\n        nonlocal key0, key1, key2\n        key0 = crc32(c, key0)\n        key1 = (key1 + (key0 & 0xFF)) & 0xFFFFFFFF\n        key1 = (key1 * 134775813 + 1) & 0xFFFFFFFF\n        key2 = crc32(key1 >> 24, key2)\n\n    for p in pwd:\n        update_keys(p)\n\n    def decrypter(data):\n        \"\"\"Decrypt a bytes object.\"\"\"\n        result = bytearray()\n        append = result.append\n        for c in data:\n            k = key2 | 2\n            c ^= ((k * (k^1)) >> 8) & 0xFF\n            update_keys(c)\n            append(c)\n        return bytes(result)\n\n    return decrypter\n\n\nclass LZMACompressor:\n\n    def __init__(self):\n        self._comp = None\n\n    def _init(self):\n        props = lzma._encode_filter_properties({'id': lzma.FILTER_LZMA1})\n        self._comp = lzma.LZMACompressor(lzma.FORMAT_RAW, filters=[\n            lzma._decode_filter_properties(lzma.FILTER_LZMA1, props)\n        ])\n        return struct.pack('<BBH', 9, 4, len(props)) + props\n\n    def compress(self, data):\n        if self._comp is None:\n            return self._init() + self._comp.compress(data)\n        return self._comp.compress(data)\n\n    def flush(self):\n        if self._comp is None:\n            return self._init() + self._comp.flush()\n        return self._comp.flush()\n\n\nclass LZMADecompressor:\n\n    def __init__(self):\n        self._decomp = None\n        self._unconsumed = b''\n        self.eof = False\n\n    def decompress(self, data):\n        if self._decomp is None:\n            self._unconsumed += data\n            if len(self._unconsumed) <= 4:\n                return b''\n            psize, = struct.unpack('<H', self._unconsumed[2:4])\n            if len(self._unconsumed) <= 4 + psize:\n                return b''\n\n            self._decomp = lzma.LZMADecompressor(lzma.FORMAT_RAW, filters=[\n                lzma._decode_filter_properties(lzma.FILTER_LZMA1,\n                                               self._unconsumed[4:4 + psize])\n            ])\n            data = self._unconsumed[4 + psize:]\n            del self._unconsumed\n\n        result = self._decomp.decompress(data)\n        self.eof = self._decomp.eof\n        return result\n\n\ncompressor_names = {\n    0: 'store',\n    1: 'shrink',\n    2: 'reduce',\n    3: 'reduce',\n    4: 'reduce',\n    5: 'reduce',\n    6: 'implode',\n    7: 'tokenize',\n    8: 'deflate',\n    9: 'deflate64',\n    10: 'implode',\n    12: 'bzip2',\n    14: 'lzma',\n    18: 'terse',\n    19: 'lz77',\n    97: 'wavpack',\n    98: 'ppmd',\n}\n\ndef _check_compression(compression):\n    if compression == ZIP_STORED:\n        pass\n    elif compression == ZIP_DEFLATED:\n        if not zlib:\n            raise RuntimeError(\n                \"Compression requires the (missing) zlib module\")\n    elif compression == ZIP_BZIP2:\n        if not bz2:\n            raise RuntimeError(\n                \"Compression requires the (missing) bz2 module\")\n    elif compression == ZIP_LZMA:\n        if not lzma:\n            raise RuntimeError(\n                \"Compression requires the (missing) lzma module\")\n    else:\n        raise NotImplementedError(\"That compression method is not supported\")\n\n\ndef _get_compressor(compress_type, compresslevel=None):\n    if compress_type == ZIP_DEFLATED:\n        if compresslevel is not None:\n            return zlib.compressobj(compresslevel, zlib.DEFLATED, -15)\n        return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15)\n    elif compress_type == ZIP_BZIP2:\n        if compresslevel is not None:\n            return bz2.BZ2Compressor(compresslevel)\n        return bz2.BZ2Compressor()\n    # compresslevel is ignored for ZIP_LZMA\n    elif compress_type == ZIP_LZMA:\n        return LZMACompressor()\n    else:\n        return None\n\n\ndef _get_decompressor(compress_type):\n    _check_compression(compress_type)\n    if compress_type == ZIP_STORED:\n        return None\n    elif compress_type == ZIP_DEFLATED:\n        return zlib.decompressobj(-15)\n    elif compress_type == ZIP_BZIP2:\n        return bz2.BZ2Decompressor()\n    elif compress_type == ZIP_LZMA:\n        return LZMADecompressor()\n    else:\n        descr = compressor_names.get(compress_type)\n        if descr:\n            raise NotImplementedError(\"compression type %d (%s)\" % (compress_type, descr))\n        else:\n            raise NotImplementedError(\"compression type %d\" % (compress_type,))\n\n\nclass _SharedFile:\n    def __init__(self, file, pos, close, lock, writing):\n        self._file = file\n        self._pos = pos\n        self._close = close\n        self._lock = lock\n        self._writing = writing\n        self.seekable = file.seekable\n\n    def tell(self):\n        return self._pos\n\n    def seek(self, offset, whence=0):\n        with self._lock:\n            if self._writing():\n                raise ValueError(\"Can't reposition in the ZIP file while \"\n                        \"there is an open writing handle on it. \"\n                        \"Close the writing handle before trying to read.\")\n            if whence == os.SEEK_CUR:\n                self._file.seek(self._pos + offset)\n            else:\n                self._file.seek(offset, whence)\n            self._pos = self._file.tell()\n            return self._pos\n\n    def read(self, n=-1):\n        with self._lock:\n            if self._writing():\n                raise ValueError(\"Can't read from the ZIP file while there \"\n                        \"is an open writing handle on it. \"\n                        \"Close the writing handle before trying to read.\")\n            self._file.seek(self._pos)\n            data = self._file.read(n)\n            self._pos = self._file.tell()\n            return data\n\n    def close(self):\n        if self._file is not None:\n            fileobj = self._file\n            self._file = None\n            self._close(fileobj)\n\n# Provide the tell method for unseekable stream\nclass _Tellable:\n    def __init__(self, fp):\n        self.fp = fp\n        self.offset = 0\n\n    def write(self, data):\n        n = self.fp.write(data)\n        self.offset += n\n        return n\n\n    def tell(self):\n        return self.offset\n\n    def flush(self):\n        self.fp.flush()\n\n    def close(self):\n        self.fp.close()\n\n\nclass ZipExtFile(io.BufferedIOBase):\n    \"\"\"File-like object for reading an archive member.\n       Is returned by ZipFile.open().\n    \"\"\"\n\n    # Max size supported by decompressor.\n    MAX_N = 1 << 31 - 1\n\n    # Read from compressed files in 4k blocks.\n    MIN_READ_SIZE = 4096\n\n    # Chunk size to read during seek\n    MAX_SEEK_READ = 1 << 24\n\n    def __init__(self, fileobj, mode, zipinfo, pwd=None,\n                 close_fileobj=False):\n        self._fileobj = fileobj\n        self._pwd = pwd\n        self._close_fileobj = close_fileobj\n\n        self._compress_type = zipinfo.compress_type\n        self._compress_left = zipinfo.compress_size\n        self._left = zipinfo.file_size\n\n        self._decompressor = _get_decompressor(self._compress_type)\n\n        self._eof = False\n        self._readbuffer = b''\n        self._offset = 0\n\n        self.newlines = None\n\n        self.mode = mode\n        self.name = zipinfo.filename\n\n        if hasattr(zipinfo, 'CRC'):\n            self._expected_crc = zipinfo.CRC\n            self._running_crc = crc32(b'')\n        else:\n            self._expected_crc = None\n\n        self._seekable = False\n        try:\n            if fileobj.seekable():\n                self._orig_compress_start = fileobj.tell()\n                self._orig_compress_size = zipinfo.compress_size\n                self._orig_file_size = zipinfo.file_size\n                self._orig_start_crc = self._running_crc\n                self._orig_crc = self._expected_crc\n                self._seekable = True\n        except AttributeError:\n            pass\n\n        self._decrypter = None\n        if pwd:\n            if zipinfo.flag_bits & _MASK_USE_DATA_DESCRIPTOR:\n                # compare against the file type from extended local headers\n                check_byte = (zipinfo._raw_time >> 8) & 0xff\n            else:\n                # compare against the CRC otherwise\n                check_byte = (zipinfo.CRC >> 24) & 0xff\n            h = self._init_decrypter()\n            if h != check_byte:\n                raise RuntimeError(\"Bad password for file %r\" % zipinfo.orig_filename)\n\n\n    def _init_decrypter(self):\n        self._decrypter = _ZipDecrypter(self._pwd)\n        # The first 12 bytes in the cypher stream is an encryption header\n        #  used to strengthen the algorithm. The first 11 bytes are\n        #  completely random, while the 12th contains the MSB of the CRC,\n        #  or the MSB of the file time depending on the header type\n        #  and is used to check the correctness of the password.\n        header = self._fileobj.read(12)\n        self._compress_left -= 12\n        return self._decrypter(header)[11]\n\n    def __repr__(self):\n        result = ['<%s.%s' % (self.__class__.__module__,\n                              self.__class__.__qualname__)]\n        if not self.closed:\n            result.append(' name=%r mode=%r' % (self.name, self.mode))\n            if self._compress_type != ZIP_STORED:\n                result.append(' compress_type=%s' %\n                              compressor_names.get(self._compress_type,\n                                                   self._compress_type))\n        else:\n            result.append(' [closed]')\n        result.append('>')\n        return ''.join(result)\n\n    def readline(self, limit=-1):\n        \"\"\"Read and return a line from the stream.\n\n        If limit is specified, at most limit bytes will be read.\n        \"\"\"\n\n        if limit < 0:\n            # Shortcut common case - newline found in buffer.\n            i = self._readbuffer.find(b'\\n', self._offset) + 1\n            if i > 0:\n                line = self._readbuffer[self._offset: i]\n                self._offset = i\n                return line\n\n        return io.BufferedIOBase.readline(self, limit)\n\n    def peek(self, n=1):\n        \"\"\"Returns buffered bytes without advancing the position.\"\"\"\n        if n > len(self._readbuffer) - self._offset:\n            chunk = self.read(n)\n            if len(chunk) > self._offset:\n                self._readbuffer = chunk + self._readbuffer[self._offset:]\n                self._offset = 0\n            else:\n                self._offset -= len(chunk)\n\n        # Return up to 512 bytes to reduce allocation overhead for tight loops.\n        return self._readbuffer[self._offset: self._offset + 512]\n\n    def readable(self):\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        return True\n\n    def read(self, n=-1):\n        \"\"\"Read and return up to n bytes.\n        If the argument is omitted, None, or negative, data is read and returned until EOF is reached.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"read from closed file.\")\n        if n is None or n < 0:\n            buf = self._readbuffer[self._offset:]\n            self._readbuffer = b''\n            self._offset = 0\n            while not self._eof:\n                buf += self._read1(self.MAX_N)\n            return buf\n\n        end = n + self._offset\n        if end < len(self._readbuffer):\n            buf = self._readbuffer[self._offset:end]\n            self._offset = end\n            return buf\n\n        n = end - len(self._readbuffer)\n        buf = self._readbuffer[self._offset:]\n        self._readbuffer = b''\n        self._offset = 0\n        while n > 0 and not self._eof:\n            data = self._read1(n)\n            if n < len(data):\n                self._readbuffer = data\n                self._offset = n\n                buf += data[:n]\n                break\n            buf += data\n            n -= len(data)\n        return buf\n\n    def _update_crc(self, newdata):\n        # Update the CRC using the given data.\n        if self._expected_crc is None:\n            # No need to compute the CRC if we don't have a reference value\n            return\n        self._running_crc = crc32(newdata, self._running_crc)\n        # Check the CRC if we're at the end of the file\n        if self._eof and self._running_crc != self._expected_crc:\n            raise BadZipFile(\"Bad CRC-32 for file %r\" % self.name)\n\n    def read1(self, n):\n        \"\"\"Read up to n bytes with at most one read() system call.\"\"\"\n\n        if n is None or n < 0:\n            buf = self._readbuffer[self._offset:]\n            self._readbuffer = b''\n            self._offset = 0\n            while not self._eof:\n                data = self._read1(self.MAX_N)\n                if data:\n                    buf += data\n                    break\n            return buf\n\n        end = n + self._offset\n        if end < len(self._readbuffer):\n            buf = self._readbuffer[self._offset:end]\n            self._offset = end\n            return buf\n\n        n = end - len(self._readbuffer)\n        buf = self._readbuffer[self._offset:]\n        self._readbuffer = b''\n        self._offset = 0\n        if n > 0:\n            while not self._eof:\n                data = self._read1(n)\n                if n < len(data):\n                    self._readbuffer = data\n                    self._offset = n\n                    buf += data[:n]\n                    break\n                if data:\n                    buf += data\n                    break\n        return buf\n\n    def _read1(self, n):\n        # Read up to n compressed bytes with at most one read() system call,\n        # decrypt and decompress them.\n        if self._eof or n <= 0:\n            return b''\n\n        # Read from file.\n        if self._compress_type == ZIP_DEFLATED:\n            ## Handle unconsumed data.\n            data = self._decompressor.unconsumed_tail\n            if n > len(data):\n                data += self._read2(n - len(data))\n        else:\n            data = self._read2(n)\n\n        if self._compress_type == ZIP_STORED:\n            self._eof = self._compress_left <= 0\n        elif self._compress_type == ZIP_DEFLATED:\n            n = max(n, self.MIN_READ_SIZE)\n            data = self._decompressor.decompress(data, n)\n            self._eof = (self._decompressor.eof or\n                         self._compress_left <= 0 and\n                         not self._decompressor.unconsumed_tail)\n            if self._eof:\n                data += self._decompressor.flush()\n        else:\n            data = self._decompressor.decompress(data)\n            self._eof = self._decompressor.eof or self._compress_left <= 0\n\n        data = data[:self._left]\n        self._left -= len(data)\n        if self._left <= 0:\n            self._eof = True\n        self._update_crc(data)\n        return data\n\n    def _read2(self, n):\n        if self._compress_left <= 0:\n            return b''\n\n        n = max(n, self.MIN_READ_SIZE)\n        n = min(n, self._compress_left)\n\n        data = self._fileobj.read(n)\n        self._compress_left -= len(data)\n        if not data:\n            raise EOFError\n\n        if self._decrypter is not None:\n            data = self._decrypter(data)\n        return data\n\n    def close(self):\n        try:\n            if self._close_fileobj:\n                self._fileobj.close()\n        finally:\n            super().close()\n\n    def seekable(self):\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        return self._seekable\n\n    def seek(self, offset, whence=os.SEEK_SET):\n        if self.closed:\n            raise ValueError(\"seek on closed file.\")\n        if not self._seekable:\n            raise io.UnsupportedOperation(\"underlying stream is not seekable\")\n        curr_pos = self.tell()\n        if whence == os.SEEK_SET:\n            new_pos = offset\n        elif whence == os.SEEK_CUR:\n            new_pos = curr_pos + offset\n        elif whence == os.SEEK_END:\n            new_pos = self._orig_file_size + offset\n        else:\n            raise ValueError(\"whence must be os.SEEK_SET (0), \"\n                             \"os.SEEK_CUR (1), or os.SEEK_END (2)\")\n\n        if new_pos > self._orig_file_size:\n            new_pos = self._orig_file_size\n\n        if new_pos < 0:\n            new_pos = 0\n\n        read_offset = new_pos - curr_pos\n        buff_offset = read_offset + self._offset\n\n        if buff_offset >= 0 and buff_offset < len(self._readbuffer):\n            # Just move the _offset index if the new position is in the _readbuffer\n            self._offset = buff_offset\n            read_offset = 0\n        # Fast seek uncompressed unencrypted file\n        elif self._compress_type == ZIP_STORED and self._decrypter is None and read_offset != 0:\n            # disable CRC checking after first seeking - it would be invalid\n            self._expected_crc = None\n            # seek actual file taking already buffered data into account\n            read_offset -= len(self._readbuffer) - self._offset\n            self._fileobj.seek(read_offset, os.SEEK_CUR)\n            self._left -= read_offset\n            self._compress_left -= read_offset\n            self._eof = self._left <= 0\n            read_offset = 0\n            # flush read buffer\n            self._readbuffer = b''\n            self._offset = 0\n        elif read_offset < 0:\n            # Position is before the current position. Reset the ZipExtFile\n            self._fileobj.seek(self._orig_compress_start)\n            self._running_crc = self._orig_start_crc\n            self._expected_crc = self._orig_crc\n            self._compress_left = self._orig_compress_size\n            self._left = self._orig_file_size\n            self._readbuffer = b''\n            self._offset = 0\n            self._decompressor = _get_decompressor(self._compress_type)\n            self._eof = False\n            read_offset = new_pos\n            if self._decrypter is not None:\n                self._init_decrypter()\n\n        while read_offset > 0:\n            read_len = min(self.MAX_SEEK_READ, read_offset)\n            self.read(read_len)\n            read_offset -= read_len\n\n        return self.tell()\n\n    def tell(self):\n        if self.closed:\n            raise ValueError(\"tell on closed file.\")\n        if not self._seekable:\n            raise io.UnsupportedOperation(\"underlying stream is not seekable\")\n        filepos = self._orig_file_size - self._left - len(self._readbuffer) + self._offset\n        return filepos\n\n\nclass _ZipWriteFile(io.BufferedIOBase):\n    def __init__(self, zf, zinfo, zip64):\n        self._zinfo = zinfo\n        self._zip64 = zip64\n        self._zipfile = zf\n        self._compressor = _get_compressor(zinfo.compress_type,\n                                           zinfo._compresslevel)\n        self._file_size = 0\n        self._compress_size = 0\n        self._crc = 0\n\n    @property\n    def _fileobj(self):\n        return self._zipfile.fp\n\n    def writable(self):\n        return True\n\n    def write(self, data):\n        if self.closed:\n            raise ValueError('I/O operation on closed file.')\n\n        # Accept any data that supports the buffer protocol\n        if isinstance(data, (bytes, bytearray)):\n            nbytes = len(data)\n        else:\n            data = memoryview(data)\n            nbytes = data.nbytes\n        self._file_size += nbytes\n\n        self._crc = crc32(data, self._crc)\n        if self._compressor:\n            data = self._compressor.compress(data)\n            self._compress_size += len(data)\n        self._fileobj.write(data)\n        return nbytes\n\n    def close(self):\n        if self.closed:\n            return\n        try:\n            super().close()\n            # Flush any data from the compressor, and update header info\n            if self._compressor:\n                buf = self._compressor.flush()\n                self._compress_size += len(buf)\n                self._fileobj.write(buf)\n                self._zinfo.compress_size = self._compress_size\n            else:\n                self._zinfo.compress_size = self._file_size\n            self._zinfo.CRC = self._crc\n            self._zinfo.file_size = self._file_size\n\n            if not self._zip64:\n                if self._file_size > ZIP64_LIMIT:\n                    raise RuntimeError(\"File size too large, try using force_zip64\")\n                if self._compress_size > ZIP64_LIMIT:\n                    raise RuntimeError(\"Compressed size too large, try using force_zip64\")\n\n            # Write updated header info\n            if self._zinfo.flag_bits & _MASK_USE_DATA_DESCRIPTOR:\n                # Write CRC and file sizes after the file data\n                fmt = '<LLQQ' if self._zip64 else '<LLLL'\n                self._fileobj.write(struct.pack(fmt, _DD_SIGNATURE, self._zinfo.CRC,\n                    self._zinfo.compress_size, self._zinfo.file_size))\n                self._zipfile.start_dir = self._fileobj.tell()\n            else:\n                # Seek backwards and write file header (which will now include\n                # correct CRC and file sizes)\n\n                # Preserve current position in file\n                self._zipfile.start_dir = self._fileobj.tell()\n                self._fileobj.seek(self._zinfo.header_offset)\n                self._fileobj.write(self._zinfo.FileHeader(self._zip64))\n                self._fileobj.seek(self._zipfile.start_dir)\n\n            # Successfully written: Add file to our caches\n            self._zipfile.filelist.append(self._zinfo)\n            self._zipfile.NameToInfo[self._zinfo.filename] = self._zinfo\n        finally:\n            self._zipfile._writing = False\n\n\n\nclass ZipFile:\n    \"\"\" Class with methods to open, read, write, close, list zip files.\n\n    z = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n                compresslevel=None)\n\n    file: Either the path to the file, or a file-like object.\n          If it is a path, the file will be opened and closed by ZipFile.\n    mode: The mode can be either read 'r', write 'w', exclusive create 'x',\n          or append 'a'.\n    compression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n                 ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\n    allowZip64: if True ZipFile will create files with ZIP64 extensions when\n                needed, otherwise it will raise an exception when this would\n                be necessary.\n    compresslevel: None (default for the given compression type) or an integer\n                   specifying the level to pass to the compressor.\n                   When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n                   When using ZIP_DEFLATED integers 0 through 9 are accepted.\n                   When using ZIP_BZIP2 integers 1 through 9 are accepted.\n\n    \"\"\"\n\n    fp = None                   # Set here since __del__ checks it\n    _windows_illegal_name_trans_table = None\n\n    def __init__(self, file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n                 compresslevel=None, *, strict_timestamps=True, metadata_encoding=None):\n        \"\"\"Open the ZIP file with mode read 'r', write 'w', exclusive create 'x',\n        or append 'a'.\"\"\"\n        if mode not in ('r', 'w', 'x', 'a'):\n            raise ValueError(\"ZipFile requires mode 'r', 'w', 'x', or 'a'\")\n\n        _check_compression(compression)\n\n        self._allowZip64 = allowZip64\n        self._didModify = False\n        self.debug = 0  # Level of printing: 0 through 3\n        self.NameToInfo = {}    # Find file info given name\n        self.filelist = []      # List of ZipInfo instances for archive\n        self.compression = compression  # Method of compression\n        self.compresslevel = compresslevel\n        self.mode = mode\n        self.pwd = None\n        self._comment = b''\n        self._strict_timestamps = strict_timestamps\n        self.metadata_encoding = metadata_encoding\n\n        # Check that we don't try to write with nonconforming codecs\n        if self.metadata_encoding and mode != 'r':\n            raise ValueError(\n                \"metadata_encoding is only supported for reading files\")\n\n        # Check if we were passed a file-like object\n        if isinstance(file, os.PathLike):\n            file = os.fspath(file)\n        if isinstance(file, str):\n            # No, it's a filename\n            self._filePassed = 0\n            self.filename = file\n            modeDict = {'r' : 'rb', 'w': 'w+b', 'x': 'x+b', 'a' : 'r+b',\n                        'r+b': 'w+b', 'w+b': 'wb', 'x+b': 'xb'}\n            filemode = modeDict[mode]\n            while True:\n                try:\n                    self.fp = io.open(file, filemode)\n                except OSError:\n                    if filemode in modeDict:\n                        filemode = modeDict[filemode]\n                        continue\n                    raise\n                break\n        else:\n            self._filePassed = 1\n            self.fp = file\n            self.filename = getattr(file, 'name', None)\n        self._fileRefCnt = 1\n        self._lock = threading.RLock()\n        self._seekable = True\n        self._writing = False\n\n        try:\n            if mode == 'r':\n                self._RealGetContents()\n            elif mode in ('w', 'x'):\n                # set the modified flag so central directory gets written\n                # even if no files are added to the archive\n                self._didModify = True\n                try:\n                    self.start_dir = self.fp.tell()\n                except (AttributeError, OSError):\n                    self.fp = _Tellable(self.fp)\n                    self.start_dir = 0\n                    self._seekable = False\n                else:\n                    # Some file-like objects can provide tell() but not seek()\n                    try:\n                        self.fp.seek(self.start_dir)\n                    except (AttributeError, OSError):\n                        self._seekable = False\n            elif mode == 'a':\n                try:\n                    # See if file is a zip file\n                    self._RealGetContents()\n                    # seek to start of directory and overwrite\n                    self.fp.seek(self.start_dir)\n                except BadZipFile:\n                    # file is not a zip file, just append\n                    self.fp.seek(0, 2)\n\n                    # set the modified flag so central directory gets written\n                    # even if no files are added to the archive\n                    self._didModify = True\n                    self.start_dir = self.fp.tell()\n            else:\n                raise ValueError(\"Mode must be 'r', 'w', 'x', or 'a'\")\n        except:\n            fp = self.fp\n            self.fp = None\n            self._fpclose(fp)\n            raise\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def __repr__(self):\n        result = ['<%s.%s' % (self.__class__.__module__,\n                              self.__class__.__qualname__)]\n        if self.fp is not None:\n            if self._filePassed:\n                result.append(' file=%r' % self.fp)\n            elif self.filename is not None:\n                result.append(' filename=%r' % self.filename)\n            result.append(' mode=%r' % self.mode)\n        else:\n            result.append(' [closed]')\n        result.append('>')\n        return ''.join(result)\n\n    def _RealGetContents(self):\n        \"\"\"Read in the table of contents for the ZIP file.\"\"\"\n        fp = self.fp\n        try:\n            endrec = _EndRecData(fp)\n        except OSError:\n            raise BadZipFile(\"File is not a zip file\")\n        if not endrec:\n            raise BadZipFile(\"File is not a zip file\")\n        if self.debug > 1:\n            print(endrec)\n        size_cd = endrec[_ECD_SIZE]             # bytes in central directory\n        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory\n        self._comment = endrec[_ECD_COMMENT]    # archive comment\n\n        # \"concat\" is zero, unless zip was concatenated to another file\n        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd\n\n        if self.debug > 2:\n            inferred = concat + offset_cd\n            print(\"given, inferred, offset\", offset_cd, inferred, concat)\n        # self.start_dir:  Position of start of central directory\n        self.start_dir = offset_cd + concat\n        if self.start_dir < 0:\n            raise BadZipFile(\"Bad offset for central directory\")\n        fp.seek(self.start_dir, 0)\n        data = fp.read(size_cd)\n        fp = io.BytesIO(data)\n        total = 0\n        while total < size_cd:\n            centdir = fp.read(sizeCentralDir)\n            if len(centdir) != sizeCentralDir:\n                raise BadZipFile(\"Truncated central directory\")\n            centdir = struct.unpack(structCentralDir, centdir)\n            if centdir[_CD_SIGNATURE] != stringCentralDir:\n                raise BadZipFile(\"Bad magic number for central directory\")\n            if self.debug > 2:\n                print(centdir)\n            filename = fp.read(centdir[_CD_FILENAME_LENGTH])\n            orig_filename_crc = crc32(filename)\n            flags = centdir[_CD_FLAG_BITS]\n            if flags & _MASK_UTF_FILENAME:\n                # UTF-8 file names extension\n                filename = filename.decode('utf-8')\n            else:\n                # Historical ZIP filename encoding\n                filename = filename.decode(self.metadata_encoding or 'cp437')\n            # Create ZipInfo instance to store file information\n            x = ZipInfo(filename)\n            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])\n            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])\n            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]\n            (x.create_version, x.create_system, x.extract_version, x.reserved,\n             x.flag_bits, x.compress_type, t, d,\n             x.CRC, x.compress_size, x.file_size) = centdir[1:12]\n            if x.extract_version > MAX_EXTRACT_VERSION:\n                raise NotImplementedError(\"zip file version %.1f\" %\n                                          (x.extract_version / 10))\n            x.volume, x.internal_attr, x.external_attr = centdir[15:18]\n            # Convert date/time code to (year, month, day, hour, min, sec)\n            x._raw_time = t\n            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,\n                            t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )\n            x._decodeExtra(orig_filename_crc)\n            x.header_offset = x.header_offset + concat\n            self.filelist.append(x)\n            self.NameToInfo[x.filename] = x\n\n            # update total bytes read from central directory\n            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]\n                     + centdir[_CD_EXTRA_FIELD_LENGTH]\n                     + centdir[_CD_COMMENT_LENGTH])\n\n            if self.debug > 2:\n                print(\"total\", total)\n\n        end_offset = self.start_dir\n        for zinfo in reversed(sorted(self.filelist,\n                                     key=lambda zinfo: zinfo.header_offset)):\n            zinfo._end_offset = end_offset\n            end_offset = zinfo.header_offset\n\n    def namelist(self):\n        \"\"\"Return a list of file names in the archive.\"\"\"\n        return [data.filename for data in self.filelist]\n\n    def infolist(self):\n        \"\"\"Return a list of class ZipInfo instances for files in the\n        archive.\"\"\"\n        return self.filelist\n\n    def printdir(self, file=None):\n        \"\"\"Print a table of contents for the zip file.\"\"\"\n        print(\"%-46s %19s %12s\" % (\"File Name\", \"Modified    \", \"Size\"),\n              file=file)\n        for zinfo in self.filelist:\n            date = \"%d-%02d-%02d %02d:%02d:%02d\" % zinfo.date_time[:6]\n            print(\"%-46s %s %12d\" % (zinfo.filename, date, zinfo.file_size),\n                  file=file)\n\n    def testzip(self):\n        \"\"\"Read all the files and check the CRC.\n\n        Return None if all files could be read successfully, or the name\n        of the offending file otherwise.\"\"\"\n        chunk_size = 2 ** 20\n        for zinfo in self.filelist:\n            try:\n                # Read by chunks, to avoid an OverflowError or a\n                # MemoryError with very large embedded files.\n                with self.open(zinfo.filename, \"r\") as f:\n                    while f.read(chunk_size):     # Check CRC-32\n                        pass\n            except BadZipFile:\n                return zinfo.filename\n\n    def getinfo(self, name):\n        \"\"\"Return the instance of ZipInfo given 'name'.\"\"\"\n        info = self.NameToInfo.get(name)\n        if info is None:\n            raise KeyError(\n                'There is no item named %r in the archive' % name)\n\n        return info\n\n    def setpassword(self, pwd):\n        \"\"\"Set default password for encrypted files.\"\"\"\n        if pwd and not isinstance(pwd, bytes):\n            raise TypeError(\"pwd: expected bytes, got %s\" % type(pwd).__name__)\n        if pwd:\n            self.pwd = pwd\n        else:\n            self.pwd = None\n\n    @property\n    def comment(self):\n        \"\"\"The comment text associated with the ZIP file.\"\"\"\n        return self._comment\n\n    @comment.setter\n    def comment(self, comment):\n        if not isinstance(comment, bytes):\n            raise TypeError(\"comment: expected bytes, got %s\" % type(comment).__name__)\n        # check for valid comment length\n        if len(comment) > ZIP_MAX_COMMENT:\n            import warnings\n            warnings.warn('Archive comment is too long; truncating to %d bytes'\n                          % ZIP_MAX_COMMENT, stacklevel=2)\n            comment = comment[:ZIP_MAX_COMMENT]\n        self._comment = comment\n        self._didModify = True\n\n    def read(self, name, pwd=None):\n        \"\"\"Return file bytes for name. 'pwd' is the password to decrypt\n        encrypted files.\"\"\"\n        with self.open(name, \"r\", pwd) as fp:\n            return fp.read()\n\n    def open(self, name, mode=\"r\", pwd=None, *, force_zip64=False):\n        \"\"\"Return file-like object for 'name'.\n\n        name is a string for the file name within the ZIP file, or a ZipInfo\n        object.\n\n        mode should be 'r' to read a file already in the ZIP file, or 'w' to\n        write to a file newly added to the archive.\n\n        pwd is the password to decrypt files (only used for reading).\n\n        When writing, if the file size is not known in advance but may exceed\n        2 GiB, pass force_zip64 to use the ZIP64 format, which can handle large\n        files.  If the size is known in advance, it is best to pass a ZipInfo\n        instance for name, with zinfo.file_size set.\n        \"\"\"\n        if mode not in {\"r\", \"w\"}:\n            raise ValueError('open() requires mode \"r\" or \"w\"')\n        if pwd and (mode == \"w\"):\n            raise ValueError(\"pwd is only supported for reading files\")\n        if not self.fp:\n            raise ValueError(\n                \"Attempt to use ZIP archive that was already closed\")\n\n        # Make sure we have an info object\n        if isinstance(name, ZipInfo):\n            # 'name' is already an info object\n            zinfo = name\n        elif mode == 'w':\n            zinfo = ZipInfo(name)\n            zinfo.compress_type = self.compression\n            zinfo._compresslevel = self.compresslevel\n        else:\n            # Get info object for name\n            zinfo = self.getinfo(name)\n\n        if mode == 'w':\n            return self._open_to_write(zinfo, force_zip64=force_zip64)\n\n        if self._writing:\n            raise ValueError(\"Can't read from the ZIP file while there \"\n                    \"is an open writing handle on it. \"\n                    \"Close the writing handle before trying to read.\")\n\n        # Open for reading:\n        self._fileRefCnt += 1\n        zef_file = _SharedFile(self.fp, zinfo.header_offset,\n                               self._fpclose, self._lock, lambda: self._writing)\n        try:\n            # Skip the file header:\n            fheader = zef_file.read(sizeFileHeader)\n            if len(fheader) != sizeFileHeader:\n                raise BadZipFile(\"Truncated file header\")\n            fheader = struct.unpack(structFileHeader, fheader)\n            if fheader[_FH_SIGNATURE] != stringFileHeader:\n                raise BadZipFile(\"Bad magic number for file header\")\n\n            fname = zef_file.read(fheader[_FH_FILENAME_LENGTH])\n            if fheader[_FH_EXTRA_FIELD_LENGTH]:\n                zef_file.seek(fheader[_FH_EXTRA_FIELD_LENGTH], whence=1)\n\n            if zinfo.flag_bits & _MASK_COMPRESSED_PATCH:\n                # Zip 2.7: compressed patched data\n                raise NotImplementedError(\"compressed patched data (flag bit 5)\")\n\n            if zinfo.flag_bits & _MASK_STRONG_ENCRYPTION:\n                # strong encryption\n                raise NotImplementedError(\"strong encryption (flag bit 6)\")\n\n            if fheader[_FH_GENERAL_PURPOSE_FLAG_BITS] & _MASK_UTF_FILENAME:\n                # UTF-8 filename\n                fname_str = fname.decode(\"utf-8\")\n            else:\n                fname_str = fname.decode(self.metadata_encoding or \"cp437\")\n\n            if fname_str != zinfo.orig_filename:\n                raise BadZipFile(\n                    'File name in directory %r and header %r differ.'\n                    % (zinfo.orig_filename, fname))\n\n            if (zinfo._end_offset is not None and\n                zef_file.tell() + zinfo.compress_size > zinfo._end_offset):\n                if zinfo._end_offset == zinfo.header_offset:\n                    import warnings\n                    warnings.warn(\n                        f\"Overlapped entries: {zinfo.orig_filename!r} \"\n                        f\"(possible zip bomb)\",\n                        skip_file_prefixes=(os.path.dirname(__file__),))\n                else:\n                    raise BadZipFile(\n                        f\"Overlapped entries: {zinfo.orig_filename!r} \"\n                        f\"(possible zip bomb)\")\n\n            # check for encrypted flag & handle password\n            is_encrypted = zinfo.flag_bits & _MASK_ENCRYPTED\n            if is_encrypted:\n                if not pwd:\n                    pwd = self.pwd\n                if pwd and not isinstance(pwd, bytes):\n                    raise TypeError(\"pwd: expected bytes, got %s\" % type(pwd).__name__)\n                if not pwd:\n                    raise RuntimeError(\"File %r is encrypted, password \"\n                                       \"required for extraction\" % name)\n            else:\n                pwd = None\n\n            return ZipExtFile(zef_file, mode, zinfo, pwd, True)\n        except:\n            zef_file.close()\n            raise\n\n    def _open_to_write(self, zinfo, force_zip64=False):\n        if force_zip64 and not self._allowZip64:\n            raise ValueError(\n                \"force_zip64 is True, but allowZip64 was False when opening \"\n                \"the ZIP file.\"\n            )\n        if self._writing:\n            raise ValueError(\"Can't write to the ZIP file while there is \"\n                             \"another write handle open on it. \"\n                             \"Close the first handle before opening another.\")\n\n        # Size and CRC are overwritten with correct data after processing the file\n        zinfo.compress_size = 0\n        zinfo.CRC = 0\n\n        zinfo.flag_bits = 0x00\n        if zinfo.compress_type == ZIP_LZMA:\n            # Compressed data includes an end-of-stream (EOS) marker\n            zinfo.flag_bits |= _MASK_COMPRESS_OPTION_1\n        if not self._seekable:\n            zinfo.flag_bits |= _MASK_USE_DATA_DESCRIPTOR\n\n        if not zinfo.external_attr:\n            zinfo.external_attr = 0o600 << 16  # permissions: ?rw-------\n\n        # Compressed size can be larger than uncompressed size\n        zip64 = force_zip64 or (zinfo.file_size * 1.05 > ZIP64_LIMIT)\n        if not self._allowZip64 and zip64:\n            raise LargeZipFile(\"Filesize would require ZIP64 extensions\")\n\n        if self._seekable:\n            self.fp.seek(self.start_dir)\n        zinfo.header_offset = self.fp.tell()\n\n        self._writecheck(zinfo)\n        self._didModify = True\n\n        self.fp.write(zinfo.FileHeader(zip64))\n\n        self._writing = True\n        return _ZipWriteFile(self, zinfo, zip64)\n\n    def extract(self, member, path=None, pwd=None):\n        \"\"\"Extract a member from the archive to the current working directory,\n           using its full name. Its file information is extracted as accurately\n           as possible. `member' may be a filename or a ZipInfo object. You can\n           specify a different directory using `path'. You can specify the\n           password to decrypt the file using 'pwd'.\n        \"\"\"\n        if path is None:\n            path = os.getcwd()\n        else:\n            path = os.fspath(path)\n\n        return self._extract_member(member, path, pwd)\n\n    def extractall(self, path=None, members=None, pwd=None):\n        \"\"\"Extract all members from the archive to the current working\n           directory. `path' specifies a different directory to extract to.\n           `members' is optional and must be a subset of the list returned\n           by namelist(). You can specify the password to decrypt all files\n           using 'pwd'.\n        \"\"\"\n        if members is None:\n            members = self.namelist()\n\n        if path is None:\n            path = os.getcwd()\n        else:\n            path = os.fspath(path)\n\n        for zipinfo in members:\n            self._extract_member(zipinfo, path, pwd)\n\n    @classmethod\n    def _sanitize_windows_name(cls, arcname, pathsep):\n        \"\"\"Replace bad characters and remove trailing dots from parts.\"\"\"\n        table = cls._windows_illegal_name_trans_table\n        if not table:\n            illegal = ':<>|\"?*'\n            table = str.maketrans(illegal, '_' * len(illegal))\n            cls._windows_illegal_name_trans_table = table\n        arcname = arcname.translate(table)\n        # remove trailing dots and spaces\n        arcname = (x.rstrip(' .') for x in arcname.split(pathsep))\n        # rejoin, removing empty parts.\n        arcname = pathsep.join(x for x in arcname if x)\n        return arcname\n\n    def _extract_member(self, member, targetpath, pwd):\n        \"\"\"Extract the ZipInfo object 'member' to a physical\n           file on the path targetpath.\n        \"\"\"\n        if not isinstance(member, ZipInfo):\n            member = self.getinfo(member)\n\n        # build the destination pathname, replacing\n        # forward slashes to platform specific separators.\n        arcname = member.filename.replace('/', os.path.sep)\n\n        if os.path.altsep:\n            arcname = arcname.replace(os.path.altsep, os.path.sep)\n        # interpret absolute pathname as relative, remove drive letter or\n        # UNC path, redundant separators, \".\" and \"..\" components.\n        arcname = os.path.splitdrive(arcname)[1]\n        invalid_path_parts = ('', os.path.curdir, os.path.pardir)\n        arcname = os.path.sep.join(x for x in arcname.split(os.path.sep)\n                                   if x not in invalid_path_parts)\n        if os.path.sep == '\\\\':\n            # filter illegal characters on Windows\n            arcname = self._sanitize_windows_name(arcname, os.path.sep)\n\n        if not arcname and not member.is_dir():\n            raise ValueError(\"Empty filename.\")\n\n        targetpath = os.path.join(targetpath, arcname)\n        targetpath = os.path.normpath(targetpath)\n\n        # Create all upper directories if necessary.\n        upperdirs = os.path.dirname(targetpath)\n        if upperdirs and not os.path.exists(upperdirs):\n            os.makedirs(upperdirs)\n\n        if member.is_dir():\n            if not os.path.isdir(targetpath):\n                os.mkdir(targetpath)\n            return targetpath\n\n        with self.open(member, pwd=pwd) as source, \\\n             open(targetpath, \"wb\") as target:\n            shutil.copyfileobj(source, target)\n\n        return targetpath\n\n    def _writecheck(self, zinfo):\n        \"\"\"Check for errors before writing a file to the archive.\"\"\"\n        if zinfo.filename in self.NameToInfo:\n            import warnings\n            warnings.warn('Duplicate name: %r' % zinfo.filename, stacklevel=3)\n        if self.mode not in ('w', 'x', 'a'):\n            raise ValueError(\"write() requires mode 'w', 'x', or 'a'\")\n        if not self.fp:\n            raise ValueError(\n                \"Attempt to write ZIP archive that was already closed\")\n        _check_compression(zinfo.compress_type)\n        if not self._allowZip64:\n            requires_zip64 = None\n            if len(self.filelist) >= ZIP_FILECOUNT_LIMIT:\n                requires_zip64 = \"Files count\"\n            elif zinfo.file_size > ZIP64_LIMIT:\n                requires_zip64 = \"Filesize\"\n            elif zinfo.header_offset > ZIP64_LIMIT:\n                requires_zip64 = \"Zipfile size\"\n            if requires_zip64:\n                raise LargeZipFile(requires_zip64 +\n                                   \" would require ZIP64 extensions\")\n\n    def write(self, filename, arcname=None,\n              compress_type=None, compresslevel=None):\n        \"\"\"Put the bytes from filename into the archive under the name\n        arcname.\"\"\"\n        if not self.fp:\n            raise ValueError(\n                \"Attempt to write to ZIP archive that was already closed\")\n        if self._writing:\n            raise ValueError(\n                \"Can't write to ZIP archive while an open writing handle exists\"\n            )\n\n        zinfo = ZipInfo.from_file(filename, arcname,\n                                  strict_timestamps=self._strict_timestamps)\n\n        if zinfo.is_dir():\n            zinfo.compress_size = 0\n            zinfo.CRC = 0\n            self.mkdir(zinfo)\n        else:\n            if compress_type is not None:\n                zinfo.compress_type = compress_type\n            else:\n                zinfo.compress_type = self.compression\n\n            if compresslevel is not None:\n                zinfo._compresslevel = compresslevel\n            else:\n                zinfo._compresslevel = self.compresslevel\n\n            with open(filename, \"rb\") as src, self.open(zinfo, 'w') as dest:\n                shutil.copyfileobj(src, dest, 1024*8)\n\n    def writestr(self, zinfo_or_arcname, data,\n                 compress_type=None, compresslevel=None):\n        \"\"\"Write a file into the archive.  The contents is 'data', which\n        may be either a 'str' or a 'bytes' instance; if it is a 'str',\n        it is encoded as UTF-8 first.\n        'zinfo_or_arcname' is either a ZipInfo instance or\n        the name of the file in the archive.\"\"\"\n        if isinstance(data, str):\n            data = data.encode(\"utf-8\")\n        if not isinstance(zinfo_or_arcname, ZipInfo):\n            zinfo = ZipInfo(filename=zinfo_or_arcname,\n                            date_time=time.localtime(time.time())[:6])\n            zinfo.compress_type = self.compression\n            zinfo._compresslevel = self.compresslevel\n            if zinfo.filename.endswith('/'):\n                zinfo.external_attr = 0o40775 << 16   # drwxrwxr-x\n                zinfo.external_attr |= 0x10           # MS-DOS directory flag\n            else:\n                zinfo.external_attr = 0o600 << 16     # ?rw-------\n        else:\n            zinfo = zinfo_or_arcname\n\n        if not self.fp:\n            raise ValueError(\n                \"Attempt to write to ZIP archive that was already closed\")\n        if self._writing:\n            raise ValueError(\n                \"Can't write to ZIP archive while an open writing handle exists.\"\n            )\n\n        if compress_type is not None:\n            zinfo.compress_type = compress_type\n\n        if compresslevel is not None:\n            zinfo._compresslevel = compresslevel\n\n        zinfo.file_size = len(data)            # Uncompressed size\n        with self._lock:\n            with self.open(zinfo, mode='w') as dest:\n                dest.write(data)\n\n    def mkdir(self, zinfo_or_directory_name, mode=511):\n        \"\"\"Creates a directory inside the zip archive.\"\"\"\n        if isinstance(zinfo_or_directory_name, ZipInfo):\n            zinfo = zinfo_or_directory_name\n            if not zinfo.is_dir():\n                raise ValueError(\"The given ZipInfo does not describe a directory\")\n        elif isinstance(zinfo_or_directory_name, str):\n            directory_name = zinfo_or_directory_name\n            if not directory_name.endswith(\"/\"):\n                directory_name += \"/\"\n            zinfo = ZipInfo(directory_name)\n            zinfo.compress_size = 0\n            zinfo.CRC = 0\n            zinfo.external_attr = ((0o40000 | mode) & 0xFFFF) << 16\n            zinfo.file_size = 0\n            zinfo.external_attr |= 0x10\n        else:\n            raise TypeError(\"Expected type str or ZipInfo\")\n\n        with self._lock:\n            if self._seekable:\n                self.fp.seek(self.start_dir)\n            zinfo.header_offset = self.fp.tell()  # Start of header bytes\n            if zinfo.compress_type == ZIP_LZMA:\n            # Compressed data includes an end-of-stream (EOS) marker\n                zinfo.flag_bits |= _MASK_COMPRESS_OPTION_1\n\n            self._writecheck(zinfo)\n            self._didModify = True\n\n            self.filelist.append(zinfo)\n            self.NameToInfo[zinfo.filename] = zinfo\n            self.fp.write(zinfo.FileHeader(False))\n            self.start_dir = self.fp.tell()\n\n    def __del__(self):\n        \"\"\"Call the \"close()\" method in case the user forgot.\"\"\"\n        self.close()\n\n    def close(self):\n        \"\"\"Close the file, and for mode 'w', 'x' and 'a' write the ending\n        records.\"\"\"\n        if self.fp is None:\n            return\n\n        if self._writing:\n            raise ValueError(\"Can't close the ZIP file while there is \"\n                             \"an open writing handle on it. \"\n                             \"Close the writing handle before closing the zip.\")\n\n        try:\n            if self.mode in ('w', 'x', 'a') and self._didModify: # write ending records\n                with self._lock:\n                    if self._seekable:\n                        self.fp.seek(self.start_dir)\n                    self._write_end_record()\n        finally:\n            fp = self.fp\n            self.fp = None\n            self._fpclose(fp)\n\n    def _write_end_record(self):\n        for zinfo in self.filelist:         # write central directory\n            dt = zinfo.date_time\n            dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n            dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n            extra = []\n            if zinfo.file_size > ZIP64_LIMIT \\\n               or zinfo.compress_size > ZIP64_LIMIT:\n                extra.append(zinfo.file_size)\n                extra.append(zinfo.compress_size)\n                file_size = 0xffffffff\n                compress_size = 0xffffffff\n            else:\n                file_size = zinfo.file_size\n                compress_size = zinfo.compress_size\n\n            if zinfo.header_offset > ZIP64_LIMIT:\n                extra.append(zinfo.header_offset)\n                header_offset = 0xffffffff\n            else:\n                header_offset = zinfo.header_offset\n\n            extra_data = zinfo.extra\n            min_version = 0\n            if extra:\n                # Append a ZIP64 field to the extra's\n                extra_data = _strip_extra(extra_data, (1,))\n                extra_data = struct.pack(\n                    '<HH' + 'Q'*len(extra),\n                    1, 8*len(extra), *extra) + extra_data\n\n                min_version = ZIP64_VERSION\n\n            if zinfo.compress_type == ZIP_BZIP2:\n                min_version = max(BZIP2_VERSION, min_version)\n            elif zinfo.compress_type == ZIP_LZMA:\n                min_version = max(LZMA_VERSION, min_version)\n\n            extract_version = max(min_version, zinfo.extract_version)\n            create_version = max(min_version, zinfo.create_version)\n            filename, flag_bits = zinfo._encodeFilenameFlags()\n            centdir = struct.pack(structCentralDir,\n                                  stringCentralDir, create_version,\n                                  zinfo.create_system, extract_version, zinfo.reserved,\n                                  flag_bits, zinfo.compress_type, dostime, dosdate,\n                                  zinfo.CRC, compress_size, file_size,\n                                  len(filename), len(extra_data), len(zinfo.comment),\n                                  0, zinfo.internal_attr, zinfo.external_attr,\n                                  header_offset)\n            self.fp.write(centdir)\n            self.fp.write(filename)\n            self.fp.write(extra_data)\n            self.fp.write(zinfo.comment)\n\n        pos2 = self.fp.tell()\n        # Write end-of-zip-archive record\n        centDirCount = len(self.filelist)\n        centDirSize = pos2 - self.start_dir\n        centDirOffset = self.start_dir\n        requires_zip64 = None\n        if centDirCount > ZIP_FILECOUNT_LIMIT:\n            requires_zip64 = \"Files count\"\n        elif centDirOffset > ZIP64_LIMIT:\n            requires_zip64 = \"Central directory offset\"\n        elif centDirSize > ZIP64_LIMIT:\n            requires_zip64 = \"Central directory size\"\n        if requires_zip64:\n            # Need to write the ZIP64 end-of-archive records\n            if not self._allowZip64:\n                raise LargeZipFile(requires_zip64 +\n                                   \" would require ZIP64 extensions\")\n            zip64endrec = struct.pack(\n                structEndArchive64, stringEndArchive64,\n                sizeEndCentDir64 - 12, 45, 45, 0, 0, centDirCount, centDirCount,\n                centDirSize, centDirOffset)\n            self.fp.write(zip64endrec)\n\n            zip64locrec = struct.pack(\n                structEndArchive64Locator,\n                stringEndArchive64Locator, 0, pos2, 1)\n            self.fp.write(zip64locrec)\n            centDirCount = min(centDirCount, 0xFFFF)\n            centDirSize = min(centDirSize, 0xFFFFFFFF)\n            centDirOffset = min(centDirOffset, 0xFFFFFFFF)\n\n        endrec = struct.pack(structEndArchive, stringEndArchive,\n                             0, 0, centDirCount, centDirCount,\n                             centDirSize, centDirOffset, len(self._comment))\n        self.fp.write(endrec)\n        self.fp.write(self._comment)\n        if self.mode == \"a\":\n            self.fp.truncate()\n        self.fp.flush()\n\n    def _fpclose(self, fp):\n        assert self._fileRefCnt > 0\n        self._fileRefCnt -= 1\n        if not self._fileRefCnt and not self._filePassed:\n            fp.close()\n\n\nclass PyZipFile(ZipFile):\n    \"\"\"Class to create ZIP archives with Python library files and packages.\"\"\"\n\n    def __init__(self, file, mode=\"r\", compression=ZIP_STORED,\n                 allowZip64=True, optimize=-1):\n        ZipFile.__init__(self, file, mode=mode, compression=compression,\n                         allowZip64=allowZip64)\n        self._optimize = optimize\n\n    def writepy(self, pathname, basename=\"\", filterfunc=None):\n        \"\"\"Add all files from \"pathname\" to the ZIP archive.\n\n        If pathname is a package directory, search the directory and\n        all package subdirectories recursively for all *.py and enter\n        the modules into the archive.  If pathname is a plain\n        directory, listdir *.py and enter all modules.  Else, pathname\n        must be a Python *.py file and the module will be put into the\n        archive.  Added modules are always module.pyc.\n        This method will compile the module.py into module.pyc if\n        necessary.\n        If filterfunc(pathname) is given, it is called with every argument.\n        When it is False, the file or directory is skipped.\n        \"\"\"\n        pathname = os.fspath(pathname)\n        if filterfunc and not filterfunc(pathname):\n            if self.debug:\n                label = 'path' if os.path.isdir(pathname) else 'file'\n                print('%s %r skipped by filterfunc' % (label, pathname))\n            return\n        dir, name = os.path.split(pathname)\n        if os.path.isdir(pathname):\n            initname = os.path.join(pathname, \"__init__.py\")\n            if os.path.isfile(initname):\n                # This is a package directory, add it\n                if basename:\n                    basename = \"%s/%s\" % (basename, name)\n                else:\n                    basename = name\n                if self.debug:\n                    print(\"Adding package in\", pathname, \"as\", basename)\n                fname, arcname = self._get_codename(initname[0:-3], basename)\n                if self.debug:\n                    print(\"Adding\", arcname)\n                self.write(fname, arcname)\n                dirlist = sorted(os.listdir(pathname))\n                dirlist.remove(\"__init__.py\")\n                # Add all *.py files and package subdirectories\n                for filename in dirlist:\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if os.path.isdir(path):\n                        if os.path.isfile(os.path.join(path, \"__init__.py\")):\n                            # This is a package directory, add it\n                            self.writepy(path, basename,\n                                         filterfunc=filterfunc)  # Recursive call\n                    elif ext == \".py\":\n                        if filterfunc and not filterfunc(path):\n                            if self.debug:\n                                print('file %r skipped by filterfunc' % path)\n                            continue\n                        fname, arcname = self._get_codename(path[0:-3],\n                                                            basename)\n                        if self.debug:\n                            print(\"Adding\", arcname)\n                        self.write(fname, arcname)\n            else:\n                # This is NOT a package directory, add its files at top level\n                if self.debug:\n                    print(\"Adding files from directory\", pathname)\n                for filename in sorted(os.listdir(pathname)):\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if ext == \".py\":\n                        if filterfunc and not filterfunc(path):\n                            if self.debug:\n                                print('file %r skipped by filterfunc' % path)\n                            continue\n                        fname, arcname = self._get_codename(path[0:-3],\n                                                            basename)\n                        if self.debug:\n                            print(\"Adding\", arcname)\n                        self.write(fname, arcname)\n        else:\n            if pathname[-3:] != \".py\":\n                raise RuntimeError(\n                    'Files added with writepy() must end with \".py\"')\n            fname, arcname = self._get_codename(pathname[0:-3], basename)\n            if self.debug:\n                print(\"Adding file\", arcname)\n            self.write(fname, arcname)\n\n    def _get_codename(self, pathname, basename):\n        \"\"\"Return (filename, archivename) for the path.\n\n        Given a module name path, return the correct file path and\n        archive name, compiling if necessary.  For example, given\n        /python/lib/string, return (/python/lib/string.pyc, string).\n        \"\"\"\n        def _compile(file, optimize=-1):\n            import py_compile\n            if self.debug:\n                print(\"Compiling\", file)\n            try:\n                py_compile.compile(file, doraise=True, optimize=optimize)\n            except py_compile.PyCompileError as err:\n                print(err.msg)\n                return False\n            return True\n\n        file_py  = pathname + \".py\"\n        file_pyc = pathname + \".pyc\"\n        pycache_opt0 = importlib.util.cache_from_source(file_py, optimization='')\n        pycache_opt1 = importlib.util.cache_from_source(file_py, optimization=1)\n        pycache_opt2 = importlib.util.cache_from_source(file_py, optimization=2)\n        if self._optimize == -1:\n            # legacy mode: use whatever file is present\n            if (os.path.isfile(file_pyc) and\n                  os.stat(file_pyc).st_mtime >= os.stat(file_py).st_mtime):\n                # Use .pyc file.\n                arcname = fname = file_pyc\n            elif (os.path.isfile(pycache_opt0) and\n                  os.stat(pycache_opt0).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt0\n                arcname = file_pyc\n            elif (os.path.isfile(pycache_opt1) and\n                  os.stat(pycache_opt1).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt1\n                arcname = file_pyc\n            elif (os.path.isfile(pycache_opt2) and\n                  os.stat(pycache_opt2).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt2\n                arcname = file_pyc\n            else:\n                # Compile py into PEP 3147 pyc file.\n                if _compile(file_py):\n                    if sys.flags.optimize == 0:\n                        fname = pycache_opt0\n                    elif sys.flags.optimize == 1:\n                        fname = pycache_opt1\n                    else:\n                        fname = pycache_opt2\n                    arcname = file_pyc\n                else:\n                    fname = arcname = file_py\n        else:\n            # new mode: use given optimization level\n            if self._optimize == 0:\n                fname = pycache_opt0\n                arcname = file_pyc\n            else:\n                arcname = file_pyc\n                if self._optimize == 1:\n                    fname = pycache_opt1\n                elif self._optimize == 2:\n                    fname = pycache_opt2\n                else:\n                    msg = \"invalid value for 'optimize': {!r}\".format(self._optimize)\n                    raise ValueError(msg)\n            if not (os.path.isfile(fname) and\n                    os.stat(fname).st_mtime >= os.stat(file_py).st_mtime):\n                if not _compile(file_py, optimize=self._optimize):\n                    fname = arcname = file_py\n        archivename = os.path.split(arcname)[1]\n        if basename:\n            archivename = \"%s/%s\" % (basename, archivename)\n        return (fname, archivename)\n\n\ndef main(args=None):\n    import argparse\n\n    description = 'A simple command-line interface for zipfile module.'\n    parser = argparse.ArgumentParser(description=description)\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('-l', '--list', metavar='<zipfile>',\n                       help='Show listing of a zipfile')\n    group.add_argument('-e', '--extract', nargs=2,\n                       metavar=('<zipfile>', '<output_dir>'),\n                       help='Extract zipfile into target dir')\n    group.add_argument('-c', '--create', nargs='+',\n                       metavar=('<name>', '<file>'),\n                       help='Create zipfile from sources')\n    group.add_argument('-t', '--test', metavar='<zipfile>',\n                       help='Test if a zipfile is valid')\n    parser.add_argument('--metadata-encoding', metavar='<encoding>',\n                        help='Specify encoding of member names for -l, -e and -t')\n    args = parser.parse_args(args)\n\n    encoding = args.metadata_encoding\n\n    if args.test is not None:\n        src = args.test\n        with ZipFile(src, 'r', metadata_encoding=encoding) as zf:\n            badfile = zf.testzip()\n        if badfile:\n            print(\"The following enclosed file is corrupted: {!r}\".format(badfile))\n        print(\"Done testing\")\n\n    elif args.list is not None:\n        src = args.list\n        with ZipFile(src, 'r', metadata_encoding=encoding) as zf:\n            zf.printdir()\n\n    elif args.extract is not None:\n        src, curdir = args.extract\n        with ZipFile(src, 'r', metadata_encoding=encoding) as zf:\n            zf.extractall(curdir)\n\n    elif args.create is not None:\n        if encoding:\n            print(\"Non-conforming encodings not supported with -c.\",\n                  file=sys.stderr)\n            sys.exit(1)\n\n        zip_name = args.create.pop(0)\n        files = args.create\n\n        def addToZip(zf, path, zippath):\n            if os.path.isfile(path):\n                zf.write(path, zippath, ZIP_DEFLATED)\n            elif os.path.isdir(path):\n                if zippath:\n                    zf.write(path, zippath)\n                for nm in sorted(os.listdir(path)):\n                    addToZip(zf,\n                             os.path.join(path, nm), os.path.join(zippath, nm))\n            # else: ignore\n\n        with ZipFile(zip_name, 'w') as zf:\n            for path in files:\n                zippath = os.path.basename(path)\n                if not zippath:\n                    zippath = os.path.basename(os.path.dirname(path))\n                if zippath in ('', os.curdir, os.pardir):\n                    zippath = ''\n                addToZip(zf, path, zippath)\n\n\nfrom ._path import (  # noqa: E402\n    Path,\n\n    # used privately for tests\n    CompleteDirs,  # noqa: F401\n)\n", 2340], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_parseaddr.py": ["# Copyright (C) 2002-2007 Python Software Foundation\n# Contact: email-sig@python.org\n\n\"\"\"Email address parsing code.\n\nLifted directly from rfc822.py.  This should eventually be rewritten.\n\"\"\"\n\n__all__ = [\n    'mktime_tz',\n    'parsedate',\n    'parsedate_tz',\n    'quote',\n    ]\n\nimport time, calendar\n\nSPACE = ' '\nEMPTYSTRING = ''\nCOMMASPACE = ', '\n\n# Parse a date field\n_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n               'aug', 'sep', 'oct', 'nov', 'dec',\n               'january', 'february', 'march', 'april', 'may', 'june', 'july',\n               'august', 'september', 'october', 'november', 'december']\n\n_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n\n# The timezone table does not include the military time zones defined\n# in RFC822, other than Z.  According to RFC1123, the description in\n# RFC822 gets the signs wrong, so we can't rely on any such time\n# zones.  RFC1123 recommends that numeric timezone indicators be used\n# instead of timezone names.\n\n_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,\n              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)\n              'EST': -500, 'EDT': -400,  # Eastern\n              'CST': -600, 'CDT': -500,  # Central\n              'MST': -700, 'MDT': -600,  # Mountain\n              'PST': -800, 'PDT': -700   # Pacific\n              }\n\n\ndef parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n    res = _parsedate_tz(data)\n    if not res:\n        return\n    if res[9] is None:\n        res[9] = 0\n    return tuple(res)\n\ndef _parsedate_tz(data):\n    \"\"\"Convert date to extended time tuple.\n\n    The last (additional) element is the time zone offset in seconds, except if\n    the timezone was specified as -0000.  In that case the last element is\n    None.  This indicates a UTC timestamp that explicitly declaims knowledge of\n    the source timezone, as opposed to a +0000 timestamp that indicates the\n    source timezone really was UTC.\n\n    \"\"\"\n    if not data:\n        return None\n    data = data.split()\n    if not data:  # This happens for whitespace-only input.\n        return None\n    # The FWS after the comma after the day-of-week is optional, so search and\n    # adjust for this.\n    if data[0].endswith(',') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i == -1:\n            i = s.find('-')\n        if i > 0:\n            data[3:] = [s[:i], s[i:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    if not (dd and mm and yy):\n        return None\n    mm = mm.lower()\n    if mm not in _monthnames:\n        dd, mm = mm, dd.lower()\n        if mm not in _monthnames:\n            return None\n    mm = _monthnames.index(mm) + 1\n    if mm > 12:\n        mm -= 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n        if not yy:\n            return None\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    elif len(tm) == 1 and '.' in tm[0]:\n        # Some non-compliant MUAs use '.' to separate time elements.\n        tm = tm[0].split('.')\n        if len(tm) == 2:\n            [thh, tmm] = tm\n            tss = 0\n        elif len(tm) == 3:\n            [thh, tmm, tss] = tm\n        else:\n            return None\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    # Check for a yy specified in two-digit format, then convert it to the\n    # appropriate four-digit format, according to the POSIX standard. RFC 822\n    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)\n    # mandates a 4-digit yy. For more information, see the documentation for\n    # the time module.\n    if yy < 100:\n        # The year is between 1969 and 1999 (inclusive).\n        if yy > 68:\n            yy += 1900\n        # The year is between 2000 and 2068 (inclusive).\n        else:\n            yy += 2000\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n        if tzoffset==0 and tz.startswith('-'):\n            tzoffset = None\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    # Daylight Saving Time flag is set to -1, since DST is unknown.\n    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]\n\n\ndef parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if isinstance(t, tuple):\n        return t[:9]\n    else:\n        return t\n\n\ndef mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]\n\n\ndef quote(str):\n    \"\"\"Prepare string to be used in a quoted string.\n\n    Turns backslash and double quote characters into quoted pairs.  These\n    are the only characters that need to be quoted inside a quoted string.\n    Does not add the surrounding double quotes.\n    \"\"\"\n    return str.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n\nclass AddrlistClass:\n    \"\"\"Address parser class by Ben Escoto.\n\n    To understand what this class does, it helps to have a copy of RFC 2822 in\n    front of you.\n\n    Note: this class interface is deprecated and may be removed in the future.\n    Use email.utils.AddressList instead.\n    \"\"\"\n\n    def __init__(self, field):\n        \"\"\"Initialize a new instance.\n\n        `field' is an unparsed address header field, containing\n        one or more addresses.\n        \"\"\"\n        self.specials = '()<>@,:;.\\\"[]'\n        self.pos = 0\n        self.LWS = ' \\t'\n        self.CR = '\\r\\n'\n        self.FWS = self.LWS + self.CR\n        self.atomends = self.specials + self.LWS + self.CR\n        # Note that RFC 2822 now specifies `.' as obs-phrase, meaning that it\n        # is obsolete syntax.  RFC 2822 requires that we recognize obsolete\n        # syntax, so allow dots in phrases.\n        self.phraseends = self.atomends.replace('.', '')\n        self.field = field\n        self.commentlist = []\n\n    def gotonext(self):\n        \"\"\"Skip white space and extract comments.\"\"\"\n        wslist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                if self.field[self.pos] not in '\\n\\r':\n                    wslist.append(self.field[self.pos])\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break\n        return EMPTYSTRING.join(wslist)\n\n    def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        while self.pos < len(self.field):\n            ad = self.getaddress()\n            if ad:\n                result += ad\n            else:\n                result.append(('', ''))\n        return result\n\n    def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(SPACE.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(SPACE.join(plist) + ' (' +\n                               ' '.join(self.commentlist) + ')', routeaddr)]\n            else:\n                returnlist = [(SPACE.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist\n\n    def getrouteaddr(self):\n        \"\"\"Parse a route address (Return-path value).\n\n        This method just skips all the route stuff and returns the addrspec.\n        \"\"\"\n        if self.field[self.pos] != '<':\n            return\n\n        expectroute = False\n        self.pos += 1\n        self.gotonext()\n        adlist = ''\n        while self.pos < len(self.field):\n            if expectroute:\n                self.getdomain()\n                expectroute = False\n            elif self.field[self.pos] == '>':\n                self.pos += 1\n                break\n            elif self.field[self.pos] == '@':\n                self.pos += 1\n                expectroute = True\n            elif self.field[self.pos] == ':':\n                self.pos += 1\n            else:\n                adlist = self.getaddrspec()\n                self.pos += 1\n                break\n            self.gotonext()\n\n        return adlist\n\n    def getaddrspec(self):\n        \"\"\"Parse an RFC 2822 addr-spec.\"\"\"\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            preserve_ws = True\n            if self.field[self.pos] == '.':\n                if aslist and not aslist[-1].strip():\n                    aslist.pop()\n                aslist.append('.')\n                self.pos += 1\n                preserve_ws = False\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % quote(self.getquote()))\n            elif self.field[self.pos] in self.atomends:\n                if aslist and not aslist[-1].strip():\n                    aslist.pop()\n                break\n            else:\n                aslist.append(self.getatom())\n            ws = self.gotonext()\n            if preserve_ws and ws:\n                aslist.append(ws)\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return EMPTYSTRING.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        domain = self.getdomain()\n        if not domain:\n            # Invalid domain, return an empty address instead of returning a\n            # local part to denote failed parsing.\n            return EMPTYSTRING\n        return EMPTYSTRING.join(aslist) + domain\n\n    def getdomain(self):\n        \"\"\"Get the complete domain name from an address.\"\"\"\n        sdlist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] == '[':\n                sdlist.append(self.getdomainliteral())\n            elif self.field[self.pos] == '.':\n                self.pos += 1\n                sdlist.append('.')\n            elif self.field[self.pos] == '@':\n                # bpo-34155: Don't parse domains with two `@` like\n                # `a@malicious.org@important.com`.\n                return EMPTYSTRING\n            elif self.field[self.pos] in self.atomends:\n                break\n            else:\n                sdlist.append(self.getatom())\n        return EMPTYSTRING.join(sdlist)\n\n    def getdelimited(self, beginchar, endchars, allowcomments=True):\n        \"\"\"Parse a header fragment delimited by special characters.\n\n        `beginchar' is the start character for the fragment.\n        If self is not looking at an instance of `beginchar' then\n        getdelimited returns the empty string.\n\n        `endchars' is a sequence of allowable end-delimiting characters.\n        Parsing stops when one of these is encountered.\n\n        If `allowcomments' is non-zero, embedded RFC 2822 comments are allowed\n        within the parsed fragment.\n        \"\"\"\n        if self.field[self.pos] != beginchar:\n            return ''\n\n        slist = ['']\n        quote = False\n        self.pos += 1\n        while self.pos < len(self.field):\n            if quote:\n                slist.append(self.field[self.pos])\n                quote = False\n            elif self.field[self.pos] in endchars:\n                self.pos += 1\n                break\n            elif allowcomments and self.field[self.pos] == '(':\n                slist.append(self.getcomment())\n                continue        # have already advanced pos from getcomment\n            elif self.field[self.pos] == '\\\\':\n                quote = True\n            else:\n                slist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(slist)\n\n    def getquote(self):\n        \"\"\"Get a quote-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('\"', '\"\\r', False)\n\n    def getcomment(self):\n        \"\"\"Get a parenthesis-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('(', ')\\r', True)\n\n    def getdomainliteral(self):\n        \"\"\"Parse an RFC 2822 domain-literal.\"\"\"\n        return '[%s]' % self.getdelimited('[', ']\\r', False)\n\n    def getatom(self, atomends=None):\n        \"\"\"Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).\"\"\"\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else:\n                atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(atomlist)\n\n    def getphraselist(self):\n        \"\"\"Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.\n        \"\"\"\n        plist = []\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.FWS:\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                plist.append(self.getquote())\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] in self.phraseends:\n                break\n            else:\n                plist.append(self.getatom(self.phraseends))\n\n        return plist\n\nclass AddressList(AddrlistClass):\n    \"\"\"An AddressList encapsulates a list of parsed RFC 2822 addresses.\"\"\"\n    def __init__(self, field):\n        AddrlistClass.__init__(self, field)\n        if field:\n            self.addresslist = self.getaddrlist()\n        else:\n            self.addresslist = []\n\n    def __len__(self):\n        return len(self.addresslist)\n\n    def __add__(self, other):\n        # Set union\n        newaddr = AddressList(None)\n        newaddr.addresslist = self.addresslist[:]\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __iadd__(self, other):\n        # Set union, in-place\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                self.addresslist.append(x)\n        return self\n\n    def __sub__(self, other):\n        # Set difference\n        newaddr = AddressList(None)\n        for x in self.addresslist:\n            if not x in other.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __isub__(self, other):\n        # Set difference, in-place\n        for x in other.addresslist:\n            if x in self.addresslist:\n                self.addresslist.remove(x)\n        return self\n\n    def __getitem__(self, index):\n        # Make indexing, slices, and 'in' work\n        return self.addresslist[index]\n", 557], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\charset.py": ["# Copyright (C) 2001-2007 Python Software Foundation\n# Author: Ben Gertzfield, Barry Warsaw\n# Contact: email-sig@python.org\n\n__all__ = [\n    'Charset',\n    'add_alias',\n    'add_charset',\n    'add_codec',\n    ]\n\nfrom functools import partial\n\nimport email.base64mime\nimport email.quoprimime\n\nfrom email import errors\nfrom email.encoders import encode_7or8bit\n\n\n# Flags for types of header encodings\nQP          = 1 # Quoted-Printable\nBASE64      = 2 # Base64\nSHORTEST    = 3 # the shorter of QP and base64, but only for headers\n\n# In \"=?charset?q?hello_world?=\", the =?, ?q?, and ?= add up to 7\nRFC2047_CHROME_LEN = 7\n\nDEFAULT_CHARSET = 'us-ascii'\nUNKNOWN8BIT = 'unknown-8bit'\nEMPTYSTRING = ''\n\n\n# Defaults\nCHARSETS = {\n    # input        header enc  body enc output conv\n    'iso-8859-1':  (QP,        QP,      None),\n    'iso-8859-2':  (QP,        QP,      None),\n    'iso-8859-3':  (QP,        QP,      None),\n    'iso-8859-4':  (QP,        QP,      None),\n    # iso-8859-5 is Cyrillic, and not especially used\n    # iso-8859-6 is Arabic, also not particularly used\n    # iso-8859-7 is Greek, QP will not make it readable\n    # iso-8859-8 is Hebrew, QP will not make it readable\n    'iso-8859-9':  (QP,        QP,      None),\n    'iso-8859-10': (QP,        QP,      None),\n    # iso-8859-11 is Thai, QP will not make it readable\n    'iso-8859-13': (QP,        QP,      None),\n    'iso-8859-14': (QP,        QP,      None),\n    'iso-8859-15': (QP,        QP,      None),\n    'iso-8859-16': (QP,        QP,      None),\n    'windows-1252':(QP,        QP,      None),\n    'viscii':      (QP,        QP,      None),\n    'us-ascii':    (None,      None,    None),\n    'big5':        (BASE64,    BASE64,  None),\n    'gb2312':      (BASE64,    BASE64,  None),\n    'euc-jp':      (BASE64,    None,    'iso-2022-jp'),\n    'shift_jis':   (BASE64,    None,    'iso-2022-jp'),\n    'iso-2022-jp': (BASE64,    None,    None),\n    'koi8-r':      (BASE64,    BASE64,  None),\n    'utf-8':       (SHORTEST,  BASE64, 'utf-8'),\n    }\n\n# Aliases for other commonly-used names for character sets.  Map\n# them to the real ones used in email.\nALIASES = {\n    'latin_1': 'iso-8859-1',\n    'latin-1': 'iso-8859-1',\n    'latin_2': 'iso-8859-2',\n    'latin-2': 'iso-8859-2',\n    'latin_3': 'iso-8859-3',\n    'latin-3': 'iso-8859-3',\n    'latin_4': 'iso-8859-4',\n    'latin-4': 'iso-8859-4',\n    'latin_5': 'iso-8859-9',\n    'latin-5': 'iso-8859-9',\n    'latin_6': 'iso-8859-10',\n    'latin-6': 'iso-8859-10',\n    'latin_7': 'iso-8859-13',\n    'latin-7': 'iso-8859-13',\n    'latin_8': 'iso-8859-14',\n    'latin-8': 'iso-8859-14',\n    'latin_9': 'iso-8859-15',\n    'latin-9': 'iso-8859-15',\n    'latin_10':'iso-8859-16',\n    'latin-10':'iso-8859-16',\n    'cp949':   'ks_c_5601-1987',\n    'euc_jp':  'euc-jp',\n    'euc_kr':  'euc-kr',\n    'ascii':   'us-ascii',\n    }\n\n\n# Map charsets to their Unicode codec strings.\nCODEC_MAP = {\n    'gb2312':      'eucgb2312_cn',\n    'big5':        'big5_tw',\n    # Hack: We don't want *any* conversion for stuff marked us-ascii, as all\n    # sorts of garbage might be sent to us in the guise of 7-bit us-ascii.\n    # Let that stuff pass through without conversion to/from Unicode.\n    'us-ascii':    None,\n    }\n\n\n# Convenience functions for extending the above mappings\ndef add_charset(charset, header_enc=None, body_enc=None, output_charset=None):\n    \"\"\"Add character set properties to the global registry.\n\n    charset is the input character set, and must be the canonical name of a\n    character set.\n\n    Optional header_enc and body_enc is either charset.QP for\n    quoted-printable, charset.BASE64 for base64 encoding, charset.SHORTEST for\n    the shortest of qp or base64 encoding, or None for no encoding.  SHORTEST\n    is only valid for header_enc.  It describes how message headers and\n    message bodies in the input charset are to be encoded.  Default is no\n    encoding.\n\n    Optional output_charset is the character set that the output should be\n    in.  Conversions will proceed from input charset, to Unicode, to the\n    output charset when the method Charset.convert() is called.  The default\n    is to output in the same character set as the input.\n\n    Both input_charset and output_charset must have Unicode codec entries in\n    the module's charset-to-codec mapping; use add_codec(charset, codecname)\n    to add codecs the module does not know about.  See the codecs module's\n    documentation for more information.\n    \"\"\"\n    if body_enc == SHORTEST:\n        raise ValueError('SHORTEST not allowed for body_enc')\n    CHARSETS[charset] = (header_enc, body_enc, output_charset)\n\n\ndef add_alias(alias, canonical):\n    \"\"\"Add a character set alias.\n\n    alias is the alias name, e.g. latin-1\n    canonical is the character set's canonical name, e.g. iso-8859-1\n    \"\"\"\n    ALIASES[alias] = canonical\n\n\ndef add_codec(charset, codecname):\n    \"\"\"Add a codec that map characters in the given charset to/from Unicode.\n\n    charset is the canonical name of a character set.  codecname is the name\n    of a Python codec, as appropriate for the second argument to the unicode()\n    built-in, or to the encode() method of a Unicode string.\n    \"\"\"\n    CODEC_MAP[charset] = codecname\n\n\n# Convenience function for encoding strings, taking into account\n# that they might be unknown-8bit (ie: have surrogate-escaped bytes)\ndef _encode(string, codec):\n    if codec == UNKNOWN8BIT:\n        return string.encode('ascii', 'surrogateescape')\n    else:\n        return string.encode(codec)\n\n\nclass Charset:\n    \"\"\"Map character sets to their email properties.\n\n    This class provides information about the requirements imposed on email\n    for a specific character set.  It also provides convenience routines for\n    converting between character sets, given the availability of the\n    applicable codecs.  Given a character set, it will do its best to provide\n    information on how to use that character set in an email in an\n    RFC-compliant way.\n\n    Certain character sets must be encoded with quoted-printable or base64\n    when used in email headers or bodies.  Certain character sets must be\n    converted outright, and are not allowed in email.  Instances of this\n    module expose the following information about a character set:\n\n    input_charset: The initial character set specified.  Common aliases\n                   are converted to their `official' email names (e.g. latin_1\n                   is converted to iso-8859-1).  Defaults to 7-bit us-ascii.\n\n    header_encoding: If the character set must be encoded before it can be\n                     used in an email header, this attribute will be set to\n                     charset.QP (for quoted-printable), charset.BASE64 (for\n                     base64 encoding), or charset.SHORTEST for the shortest of\n                     QP or BASE64 encoding.  Otherwise, it will be None.\n\n    body_encoding: Same as header_encoding, but describes the encoding for the\n                   mail message's body, which indeed may be different than the\n                   header encoding.  charset.SHORTEST is not allowed for\n                   body_encoding.\n\n    output_charset: Some character sets must be converted before they can be\n                    used in email headers or bodies.  If the input_charset is\n                    one of them, this attribute will contain the name of the\n                    charset output will be converted to.  Otherwise, it will\n                    be None.\n\n    input_codec: The name of the Python codec used to convert the\n                 input_charset to Unicode.  If no conversion codec is\n                 necessary, this attribute will be None.\n\n    output_codec: The name of the Python codec used to convert Unicode\n                  to the output_charset.  If no conversion codec is necessary,\n                  this attribute will have the same value as the input_codec.\n    \"\"\"\n    def __init__(self, input_charset=DEFAULT_CHARSET):\n        # RFC 2046, $4.1.2 says charsets are not case sensitive.  We coerce to\n        # unicode because its .lower() is locale insensitive.  If the argument\n        # is already a unicode, we leave it at that, but ensure that the\n        # charset is ASCII, as the standard (RFC XXX) requires.\n        try:\n            if isinstance(input_charset, str):\n                input_charset.encode('ascii')\n            else:\n                input_charset = str(input_charset, 'ascii')\n        except UnicodeError:\n            raise errors.CharsetError(input_charset)\n        input_charset = input_charset.lower()\n        # Set the input charset after filtering through the aliases\n        self.input_charset = ALIASES.get(input_charset, input_charset)\n        # We can try to guess which encoding and conversion to use by the\n        # charset_map dictionary.  Try that first, but let the user override\n        # it.\n        henc, benc, conv = CHARSETS.get(self.input_charset,\n                                        (SHORTEST, BASE64, None))\n        if not conv:\n            conv = self.input_charset\n        # Set the attributes, allowing the arguments to override the default.\n        self.header_encoding = henc\n        self.body_encoding = benc\n        self.output_charset = ALIASES.get(conv, conv)\n        # Now set the codecs.  If one isn't defined for input_charset,\n        # guess and try a Unicode codec with the same name as input_codec.\n        self.input_codec = CODEC_MAP.get(self.input_charset,\n                                         self.input_charset)\n        self.output_codec = CODEC_MAP.get(self.output_charset,\n                                          self.output_charset)\n\n    def __repr__(self):\n        return self.input_charset.lower()\n\n    def __eq__(self, other):\n        return str(self) == str(other).lower()\n\n    def get_body_encoding(self):\n        \"\"\"Return the content-transfer-encoding used for body encoding.\n\n        This is either the string `quoted-printable' or `base64' depending on\n        the encoding used, or it is a function in which case you should call\n        the function with a single argument, the Message object being\n        encoded.  The function should then set the Content-Transfer-Encoding\n        header itself to whatever is appropriate.\n\n        Returns \"quoted-printable\" if self.body_encoding is QP.\n        Returns \"base64\" if self.body_encoding is BASE64.\n        Returns conversion function otherwise.\n        \"\"\"\n        assert self.body_encoding != SHORTEST\n        if self.body_encoding == QP:\n            return 'quoted-printable'\n        elif self.body_encoding == BASE64:\n            return 'base64'\n        else:\n            return encode_7or8bit\n\n    def get_output_charset(self):\n        \"\"\"Return the output character set.\n\n        This is self.output_charset if that is not None, otherwise it is\n        self.input_charset.\n        \"\"\"\n        return self.output_charset or self.input_charset\n\n    def header_encode(self, string):\n        \"\"\"Header-encode a string by converting it first to bytes.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        this charset's `header_encoding`.\n\n        :param string: A unicode string for the header.  It must be possible\n            to encode this string to bytes using the character set's\n            output codec.\n        :return: The encoded string, with RFC 2047 chrome.\n        \"\"\"\n        codec = self.output_codec or 'us-ascii'\n        header_bytes = _encode(string, codec)\n        # 7bit/8bit encodings return the string unchanged (modulo conversions)\n        encoder_module = self._get_encoder(header_bytes)\n        if encoder_module is None:\n            return string\n        return encoder_module.header_encode(header_bytes, codec)\n\n    def header_encode_lines(self, string, maxlengths):\n        \"\"\"Header-encode a string by converting it first to bytes.\n\n        This is similar to `header_encode()` except that the string is fit\n        into maximum line lengths as given by the argument.\n\n        :param string: A unicode string for the header.  It must be possible\n            to encode this string to bytes using the character set's\n            output codec.\n        :param maxlengths: Maximum line length iterator.  Each element\n            returned from this iterator will provide the next maximum line\n            length.  This parameter is used as an argument to built-in next()\n            and should never be exhausted.  The maximum line lengths should\n            not count the RFC 2047 chrome.  These line lengths are only a\n            hint; the splitter does the best it can.\n        :return: Lines of encoded strings, each with RFC 2047 chrome.\n        \"\"\"\n        # See which encoding we should use.\n        codec = self.output_codec or 'us-ascii'\n        header_bytes = _encode(string, codec)\n        encoder_module = self._get_encoder(header_bytes)\n        encoder = partial(encoder_module.header_encode, charset=codec)\n        # Calculate the number of characters that the RFC 2047 chrome will\n        # contribute to each line.\n        charset = self.get_output_charset()\n        extra = len(charset) + RFC2047_CHROME_LEN\n        # Now comes the hard part.  We must encode bytes but we can't split on\n        # bytes because some character sets are variable length and each\n        # encoded word must stand on its own.  So the problem is you have to\n        # encode to bytes to figure out this word's length, but you must split\n        # on characters.  This causes two problems: first, we don't know how\n        # many octets a specific substring of unicode characters will get\n        # encoded to, and second, we don't know how many ASCII characters\n        # those octets will get encoded to.  Unless we try it.  Which seems\n        # inefficient.  In the interest of being correct rather than fast (and\n        # in the hope that there will be few encoded headers in any such\n        # message), brute force it. :(\n        lines = []\n        current_line = []\n        maxlen = next(maxlengths) - extra\n        for character in string:\n            current_line.append(character)\n            this_line = EMPTYSTRING.join(current_line)\n            length = encoder_module.header_length(_encode(this_line, charset))\n            if length > maxlen:\n                # This last character doesn't fit so pop it off.\n                current_line.pop()\n                # Does nothing fit on the first line?\n                if not lines and not current_line:\n                    lines.append(None)\n                else:\n                    joined_line = EMPTYSTRING.join(current_line)\n                    header_bytes = _encode(joined_line, codec)\n                    lines.append(encoder(header_bytes))\n                current_line = [character]\n                maxlen = next(maxlengths) - extra\n        joined_line = EMPTYSTRING.join(current_line)\n        header_bytes = _encode(joined_line, codec)\n        lines.append(encoder(header_bytes))\n        return lines\n\n    def _get_encoder(self, header_bytes):\n        if self.header_encoding == BASE64:\n            return email.base64mime\n        elif self.header_encoding == QP:\n            return email.quoprimime\n        elif self.header_encoding == SHORTEST:\n            len64 = email.base64mime.header_length(header_bytes)\n            lenqp = email.quoprimime.header_length(header_bytes)\n            if len64 < lenqp:\n                return email.base64mime\n            else:\n                return email.quoprimime\n        else:\n            return None\n\n    def body_encode(self, string):\n        \"\"\"Body-encode a string by converting it first to bytes.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        self.body_encoding.  If body_encoding is None, we assume the\n        output charset is a 7bit encoding, so re-encoding the decoded\n        string using the ascii codec produces the correct string version\n        of the content.\n        \"\"\"\n        if not string:\n            return string\n        if self.body_encoding is BASE64:\n            if isinstance(string, str):\n                string = string.encode(self.output_charset)\n            return email.base64mime.body_encode(string)\n        elif self.body_encoding is QP:\n            # quopromime.body_encode takes a string, but operates on it as if\n            # it were a list of byte codes.  For a (minimal) history on why\n            # this is so, see changeset 0cf700464177.  To correctly encode a\n            # character set, then, we must turn it into pseudo bytes via the\n            # latin1 charset, which will encode any byte as a single code point\n            # between 0 and 255, which is what body_encode is expecting.\n            if isinstance(string, str):\n                string = string.encode(self.output_charset)\n            string = string.decode('latin1')\n            return email.quoprimime.body_encode(string)\n        else:\n            if isinstance(string, str):\n                string = string.encode(self.output_charset).decode('ascii')\n            return string\n", 398], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\utils.py": ["# Copyright (C) 2001-2010 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Miscellaneous utilities.\"\"\"\n\n__all__ = [\n    'collapse_rfc2231_value',\n    'decode_params',\n    'decode_rfc2231',\n    'encode_rfc2231',\n    'formataddr',\n    'formatdate',\n    'format_datetime',\n    'getaddresses',\n    'make_msgid',\n    'mktime_tz',\n    'parseaddr',\n    'parsedate',\n    'parsedate_tz',\n    'parsedate_to_datetime',\n    'unquote',\n    ]\n\nimport os\nimport re\nimport time\nimport random\nimport socket\nimport datetime\nimport urllib.parse\n\nfrom email._parseaddr import quote\nfrom email._parseaddr import AddressList as _AddressList\nfrom email._parseaddr import mktime_tz\n\nfrom email._parseaddr import parsedate, parsedate_tz, _parsedate_tz\n\n# Intrapackage imports\nfrom email.charset import Charset\n\nCOMMASPACE = ', '\nEMPTYSTRING = ''\nUEMPTYSTRING = ''\nCRLF = '\\r\\n'\nTICK = \"'\"\n\nspecialsre = re.compile(r'[][\\\\()<>@,:;\".]')\nescapesre = re.compile(r'[\\\\\"]')\n\n\ndef _has_surrogates(s):\n    \"\"\"Return True if s may contain surrogate-escaped binary data.\"\"\"\n    # This check is based on the fact that unless there are surrogates, utf8\n    # (Python's default encoding) can encode any string.  This is the fastest\n    # way to check for surrogates, see bpo-11454 (moved to gh-55663) for timings.\n    try:\n        s.encode()\n        return False\n    except UnicodeEncodeError:\n        return True\n\n# How to deal with a string containing bytes before handing it to the\n# application through the 'normal' interface.\ndef _sanitize(string):\n    # Turn any escaped bytes into unicode 'unknown' char.  If the escaped\n    # bytes happen to be utf-8 they will instead get decoded, even if they\n    # were invalid in the charset the source was supposed to be in.  This\n    # seems like it is not a bad thing; a defect was still registered.\n    original_bytes = string.encode('utf-8', 'surrogateescape')\n    return original_bytes.decode('utf-8', 'replace')\n\n\n\n# Helpers\n\ndef formataddr(pair, charset='utf-8'):\n    \"\"\"The inverse of parseaddr(), this takes a 2-tuple of the form\n    (realname, email_address) and returns the string value suitable\n    for an RFC 2822 From, To or Cc header.\n\n    If the first element of pair is false, then the second element is\n    returned unmodified.\n\n    The optional charset is the character set that is used to encode\n    realname in case realname is not ASCII safe.  Can be an instance of str or\n    a Charset-like object which has a header_encode method.  Default is\n    'utf-8'.\n    \"\"\"\n    name, address = pair\n    # The address MUST (per RFC) be ascii, so raise a UnicodeError if it isn't.\n    address.encode('ascii')\n    if name:\n        try:\n            name.encode('ascii')\n        except UnicodeEncodeError:\n            if isinstance(charset, str):\n                charset = Charset(charset)\n            encoded_name = charset.header_encode(name)\n            return \"%s <%s>\" % (encoded_name, address)\n        else:\n            quotes = ''\n            if specialsre.search(name):\n                quotes = '\"'\n            name = escapesre.sub(r'\\\\\\g<0>', name)\n            return '%s%s%s <%s>' % (quotes, name, quotes, address)\n    return address\n\n\ndef _iter_escaped_chars(addr):\n    pos = 0\n    escape = False\n    for pos, ch in enumerate(addr):\n        if escape:\n            yield (pos, '\\\\' + ch)\n            escape = False\n        elif ch == '\\\\':\n            escape = True\n        else:\n            yield (pos, ch)\n    if escape:\n        yield (pos, '\\\\')\n\n\ndef _strip_quoted_realnames(addr):\n    \"\"\"Strip real names between quotes.\"\"\"\n    if '\"' not in addr:\n        # Fast path\n        return addr\n\n    start = 0\n    open_pos = None\n    result = []\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '\"':\n            if open_pos is None:\n                open_pos = pos\n            else:\n                if start != open_pos:\n                    result.append(addr[start:open_pos])\n                start = pos + 1\n                open_pos = None\n\n    if start < len(addr):\n        result.append(addr[start:])\n\n    return ''.join(result)\n\n\nsupports_strict_parsing = True\n\ndef getaddresses(fieldvalues, *, strict=True):\n    \"\"\"Return a list of (REALNAME, EMAIL) or ('','') for each fieldvalue.\n\n    When parsing fails for a fieldvalue, a 2-tuple of ('', '') is returned in\n    its place.\n\n    If strict is true, use a strict parser which rejects malformed inputs.\n    \"\"\"\n\n    # If strict is true, if the resulting list of parsed addresses is greater\n    # than the number of fieldvalues in the input list, a parsing error has\n    # occurred and consequently a list containing a single empty 2-tuple [('',\n    # '')] is returned in its place. This is done to avoid invalid output.\n    #\n    # Malformed input: getaddresses(['alice@example.com <bob@example.com>'])\n    # Invalid output: [('', 'alice@example.com'), ('', 'bob@example.com')]\n    # Safe output: [('', '')]\n\n    if not strict:\n        all = COMMASPACE.join(str(v) for v in fieldvalues)\n        a = _AddressList(all)\n        return a.addresslist\n\n    fieldvalues = [str(v) for v in fieldvalues]\n    fieldvalues = _pre_parse_validation(fieldvalues)\n    addr = COMMASPACE.join(fieldvalues)\n    a = _AddressList(addr)\n    result = _post_parse_validation(a.addresslist)\n\n    # Treat output as invalid if the number of addresses is not equal to the\n    # expected number of addresses.\n    n = 0\n    for v in fieldvalues:\n        # When a comma is used in the Real Name part it is not a deliminator.\n        # So strip those out before counting the commas.\n        v = _strip_quoted_realnames(v)\n        # Expected number of addresses: 1 + number of commas\n        n += 1 + v.count(',')\n    if len(result) != n:\n        return [('', '')]\n\n    return result\n\n\ndef _check_parenthesis(addr):\n    # Ignore parenthesis in quoted real names.\n    addr = _strip_quoted_realnames(addr)\n\n    opens = 0\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '(':\n            opens += 1\n        elif ch == ')':\n            opens -= 1\n            if opens < 0:\n                return False\n    return (opens == 0)\n\n\ndef _pre_parse_validation(email_header_fields):\n    accepted_values = []\n    for v in email_header_fields:\n        if not _check_parenthesis(v):\n            v = \"('', '')\"\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _post_parse_validation(parsed_email_header_tuples):\n    accepted_values = []\n    # The parser would have parsed a correctly formatted domain-literal\n    # The existence of an [ after parsing indicates a parsing failure\n    for v in parsed_email_header_tuples:\n        if '[' in v[1]:\n            v = ('', '')\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _format_timetuple_and_zone(timetuple, zone):\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],\n        timetuple[2],\n        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],\n        timetuple[0], timetuple[3], timetuple[4], timetuple[5],\n        zone)\n\ndef formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating-point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    dt = datetime.datetime.fromtimestamp(timeval, datetime.timezone.utc)\n\n    if localtime:\n        dt = dt.astimezone()\n        usegmt = False\n    elif not usegmt:\n        dt = dt.replace(tzinfo=None)\n    return format_datetime(dt, usegmt)\n\ndef format_datetime(dt, usegmt=False):\n    \"\"\"Turn a datetime into a date string as specified in RFC 2822.\n\n    If usegmt is True, dt must be an aware datetime with an offset of zero.  In\n    this case 'GMT' will be rendered instead of the normal +0000 required by\n    RFC2822.  This is to support HTTP headers involving date stamps.\n    \"\"\"\n    now = dt.timetuple()\n    if usegmt:\n        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:\n            raise ValueError(\"usegmt option requires a UTC datetime\")\n        zone = 'GMT'\n    elif dt.tzinfo is None:\n        zone = '-0000'\n    else:\n        zone = dt.strftime(\"%z\")\n    return _format_timetuple_and_zone(now, zone)\n\n\ndef make_msgid(idstring=None, domain=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <142480216486.20800.16526388040877946887@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.  Optional domain if given provides the\n    portion of the message id after the '@'.  It defaults to the locally\n    defined hostname.\n    \"\"\"\n    timeval = int(time.time()*100)\n    pid = os.getpid()\n    randint = random.getrandbits(64)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    if domain is None:\n        domain = socket.getfqdn()\n    msgid = '<%d.%d.%d%s@%s>' % (timeval, pid, randint, idstring, domain)\n    return msgid\n\n\ndef parsedate_to_datetime(data):\n    parsed_date_tz = _parsedate_tz(data)\n    if parsed_date_tz is None:\n        raise ValueError('Invalid date value or format \"%s\"' % str(data))\n    *dtuple, tz = parsed_date_tz\n    if tz is None:\n        return datetime.datetime(*dtuple[:6])\n    return datetime.datetime(*dtuple[:6],\n            tzinfo=datetime.timezone(datetime.timedelta(seconds=tz)))\n\n\ndef parseaddr(addr, *, strict=True):\n    \"\"\"\n    Parse addr into its constituent realname and email address parts.\n\n    Return a tuple of realname and email address, unless the parse fails, in\n    which case return a 2-tuple of ('', '').\n\n    If strict is True, use a strict parser which rejects malformed inputs.\n    \"\"\"\n    if not strict:\n        addrs = _AddressList(addr).addresslist\n        if not addrs:\n            return ('', '')\n        return addrs[0]\n\n    if isinstance(addr, list):\n        addr = addr[0]\n\n    if not isinstance(addr, str):\n        return ('', '')\n\n    addr = _pre_parse_validation([addr])[0]\n    addrs = _post_parse_validation(_AddressList(addr).addresslist)\n\n    if not addrs or len(addrs) > 1:\n        return ('', '')\n\n    return addrs[0]\n\n\n# rfc822.unquote() doesn't properly de-backslash-ify in Python pre-2.3.\ndef unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str\n\n\n\n# RFC2231-related functions - parameter encoding and decoding\ndef decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts\n\n\ndef encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    s = urllib.parse.quote(s, safe='', encoding=charset or 'ascii')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)\n\n\nrfc2231_continuation = re.compile(r'^(?P<name>\\w+)\\*((?P<num>[0-9]+)\\*?)?$',\n    re.ASCII)\n\ndef decode_params(params):\n    \"\"\"Decode parameters list according to RFC 2231.\n\n    params is a sequence of 2-tuples containing (param name, string value).\n    \"\"\"\n    new_params = [params[0]]\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    for name, value in params[1:]:\n        encoded = name.endswith('*')\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    # Decode as \"latin-1\", so the characters in s directly\n                    # represent the percent-encoded octet values.\n                    # collapse_rfc2231_value treats this as an octet sequence.\n                    s = urllib.parse.unquote(s, encoding=\"latin-1\")\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params\n\ndef collapse_rfc2231_value(value, errors='replace',\n                           fallback_charset='us-ascii'):\n    if not isinstance(value, tuple) or len(value) != 3:\n        return unquote(value)\n    # While value comes to us as a unicode string, we need it to be a bytes\n    # object.  We do not want bytes() normal utf-8 decoder, we want a straight\n    # interpretation of the string as character bytes.\n    charset, language, text = value\n    if charset is None:\n        # Issue 17369: if charset/lang is None, decode_rfc2231 couldn't parse\n        # the value, so use the fallback_charset.\n        charset = fallback_charset\n    rawbytes = bytes(text, 'raw-unicode-escape')\n    try:\n        return str(rawbytes, charset, errors)\n    except LookupError:\n        # charset is not a known codec.\n        return unquote(text)\n\n\n#\n# datetime doesn't provide a localtime function yet, so provide one.  Code\n# adapted from the patch in issue 9527.  This may not be perfect, but it is\n# better than not having it.\n#\n\ndef localtime(dt=None, isdst=None):\n    \"\"\"Return local time as an aware datetime object.\n\n    If called without arguments, return current time.  Otherwise *dt*\n    argument should be a datetime instance, and it is converted to the\n    local time zone according to the system time zone database.  If *dt* is\n    naive (that is, dt.tzinfo is None), it is assumed to be in local time.\n    The isdst parameter is ignored.\n\n    \"\"\"\n    if isdst is not None:\n        import warnings\n        warnings._deprecated(\n            \"The 'isdst' parameter to 'localtime'\",\n            message='{name} is deprecated and slated for removal in Python {remove}',\n            remove=(3, 14),\n            )\n    if dt is None:\n        dt = datetime.datetime.now()\n    return dt.astimezone()\n", 486], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_policybase.py": ["\"\"\"Policy framework for the email package.\n\nAllows fine grained feature control of how the package parses and emits data.\n\"\"\"\n\nimport abc\nfrom email import header\nfrom email import charset as _charset\nfrom email.utils import _has_surrogates\n\n__all__ = [\n    'Policy',\n    'Compat32',\n    'compat32',\n    ]\n\n\nclass _PolicyBase:\n\n    \"\"\"Policy Object basic framework.\n\n    This class is useless unless subclassed.  A subclass should define\n    class attributes with defaults for any values that are to be\n    managed by the Policy object.  The constructor will then allow\n    non-default values to be set for these attributes at instance\n    creation time.  The instance will be callable, taking these same\n    attributes keyword arguments, and returning a new instance\n    identical to the called instance except for those values changed\n    by the keyword arguments.  Instances may be added, yielding new\n    instances with any non-default values from the right hand\n    operand overriding those in the left hand operand.  That is,\n\n        A + B == A(<non-default values of B>)\n\n    The repr of an instance can be used to reconstruct the object\n    if and only if the repr of the values can be used to reconstruct\n    those values.\n\n    \"\"\"\n\n    def __init__(self, **kw):\n        \"\"\"Create new Policy, possibly overriding some defaults.\n\n        See class docstring for a list of overridable attributes.\n\n        \"\"\"\n        for name, value in kw.items():\n            if hasattr(self, name):\n                super(_PolicyBase,self).__setattr__(name, value)\n            else:\n                raise TypeError(\n                    \"{!r} is an invalid keyword argument for {}\".format(\n                        name, self.__class__.__name__))\n\n    def __repr__(self):\n        args = [ \"{}={!r}\".format(name, value)\n                 for name, value in self.__dict__.items() ]\n        return \"{}({})\".format(self.__class__.__name__, ', '.join(args))\n\n    def clone(self, **kw):\n        \"\"\"Return a new instance with specified attributes changed.\n\n        The new instance has the same attribute values as the current object,\n        except for the changes passed in as keyword arguments.\n\n        \"\"\"\n        newpolicy = self.__class__.__new__(self.__class__)\n        for attr, value in self.__dict__.items():\n            object.__setattr__(newpolicy, attr, value)\n        for attr, value in kw.items():\n            if not hasattr(self, attr):\n                raise TypeError(\n                    \"{!r} is an invalid keyword argument for {}\".format(\n                        attr, self.__class__.__name__))\n            object.__setattr__(newpolicy, attr, value)\n        return newpolicy\n\n    def __setattr__(self, name, value):\n        if hasattr(self, name):\n            msg = \"{!r} object attribute {!r} is read-only\"\n        else:\n            msg = \"{!r} object has no attribute {!r}\"\n        raise AttributeError(msg.format(self.__class__.__name__, name))\n\n    def __add__(self, other):\n        \"\"\"Non-default values from right operand override those from left.\n\n        The object returned is a new instance of the subclass.\n\n        \"\"\"\n        return self.clone(**other.__dict__)\n\n\ndef _append_doc(doc, added_doc):\n    doc = doc.rsplit('\\n', 1)[0]\n    added_doc = added_doc.split('\\n', 1)[1]\n    return doc + '\\n' + added_doc\n\ndef _extend_docstrings(cls):\n    if cls.__doc__ and cls.__doc__.startswith('+'):\n        cls.__doc__ = _append_doc(cls.__bases__[0].__doc__, cls.__doc__)\n    for name, attr in cls.__dict__.items():\n        if attr.__doc__ and attr.__doc__.startswith('+'):\n            for c in (c for base in cls.__bases__ for c in base.mro()):\n                doc = getattr(getattr(c, name), '__doc__')\n                if doc:\n                    attr.__doc__ = _append_doc(doc, attr.__doc__)\n                    break\n    return cls\n\n\nclass Policy(_PolicyBase, metaclass=abc.ABCMeta):\n\n    r\"\"\"Controls for how messages are interpreted and formatted.\n\n    Most of the classes and many of the methods in the email package accept\n    Policy objects as parameters.  A Policy object contains a set of values and\n    functions that control how input is interpreted and how output is rendered.\n    For example, the parameter 'raise_on_defect' controls whether or not an RFC\n    violation results in an error being raised or not, while 'max_line_length'\n    controls the maximum length of output lines when a Message is serialized.\n\n    Any valid attribute may be overridden when a Policy is created by passing\n    it as a keyword argument to the constructor.  Policy objects are immutable,\n    but a new Policy object can be created with only certain values changed by\n    calling the Policy instance with keyword arguments.  Policy objects can\n    also be added, producing a new Policy object in which the non-default\n    attributes set in the right hand operand overwrite those specified in the\n    left operand.\n\n    Settable attributes:\n\n    raise_on_defect     -- If true, then defects should be raised as errors.\n                           Default: False.\n\n    linesep             -- string containing the value to use as separation\n                           between output lines.  Default '\\n'.\n\n    cte_type            -- Type of allowed content transfer encodings\n\n                           7bit  -- ASCII only\n                           8bit  -- Content-Transfer-Encoding: 8bit is allowed\n\n                           Default: 8bit.  Also controls the disposition of\n                           (RFC invalid) binary data in headers; see the\n                           documentation of the binary_fold method.\n\n    max_line_length     -- maximum length of lines, excluding 'linesep',\n                           during serialization.  None or 0 means no line\n                           wrapping is done.  Default is 78.\n\n    mangle_from_        -- a flag that, when True escapes From_ lines in the\n                           body of the message by putting a `>' in front of\n                           them. This is used when the message is being\n                           serialized by a generator. Default: False.\n\n    message_factory     -- the class to use to create new message objects.\n                           If the value is None, the default is Message.\n\n    verify_generated_headers\n                        -- if true, the generator verifies that each header\n                           they are properly folded, so that a parser won't\n                           treat it as multiple headers, start-of-body, or\n                           part of another header.\n                           This is a check against custom Header & fold()\n                           implementations.\n    \"\"\"\n\n    raise_on_defect = False\n    linesep = '\\n'\n    cte_type = '8bit'\n    max_line_length = 78\n    mangle_from_ = False\n    message_factory = None\n    verify_generated_headers = True\n\n    def handle_defect(self, obj, defect):\n        \"\"\"Based on policy, either raise defect or call register_defect.\n\n            handle_defect(obj, defect)\n\n        defect should be a Defect subclass, but in any case must be an\n        Exception subclass.  obj is the object on which the defect should be\n        registered if it is not raised.  If the raise_on_defect is True, the\n        defect is raised as an error, otherwise the object and the defect are\n        passed to register_defect.\n\n        This method is intended to be called by parsers that discover defects.\n        The email package parsers always call it with Defect instances.\n\n        \"\"\"\n        if self.raise_on_defect:\n            raise defect\n        self.register_defect(obj, defect)\n\n    def register_defect(self, obj, defect):\n        \"\"\"Record 'defect' on 'obj'.\n\n        Called by handle_defect if raise_on_defect is False.  This method is\n        part of the Policy API so that Policy subclasses can implement custom\n        defect handling.  The default implementation calls the append method of\n        the defects attribute of obj.  The objects used by the email package by\n        default that get passed to this method will always have a defects\n        attribute with an append method.\n\n        \"\"\"\n        obj.defects.append(defect)\n\n    def header_max_count(self, name):\n        \"\"\"Return the maximum allowed number of headers named 'name'.\n\n        Called when a header is added to a Message object.  If the returned\n        value is not 0 or None, and there are already a number of headers with\n        the name 'name' equal to the value returned, a ValueError is raised.\n\n        Because the default behavior of Message's __setitem__ is to append the\n        value to the list of headers, it is easy to create duplicate headers\n        without realizing it.  This method allows certain headers to be limited\n        in the number of instances of that header that may be added to a\n        Message programmatically.  (The limit is not observed by the parser,\n        which will faithfully produce as many headers as exist in the message\n        being parsed.)\n\n        The default implementation returns None for all header names.\n        \"\"\"\n        return None\n\n    @abc.abstractmethod\n    def header_source_parse(self, sourcelines):\n        \"\"\"Given a list of linesep terminated strings constituting the lines of\n        a single header, return the (name, value) tuple that should be stored\n        in the model.  The input lines should retain their terminating linesep\n        characters.  The lines passed in by the email package may contain\n        surrogateescaped binary data.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def header_store_parse(self, name, value):\n        \"\"\"Given the header name and the value provided by the application\n        program, return the (name, value) that should be stored in the model.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def header_fetch_parse(self, name, value):\n        \"\"\"Given the header name and the value from the model, return the value\n        to be returned to the application program that is requesting that\n        header.  The value passed in by the email package may contain\n        surrogateescaped binary data if the lines were parsed by a BytesParser.\n        The returned value should not contain any surrogateescaped data.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def fold(self, name, value):\n        \"\"\"Given the header name and the value from the model, return a string\n        containing linesep characters that implement the folding of the header\n        according to the policy controls.  The value passed in by the email\n        package may contain surrogateescaped binary data if the lines were\n        parsed by a BytesParser.  The returned value should not contain any\n        surrogateescaped data.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def fold_binary(self, name, value):\n        \"\"\"Given the header name and the value from the model, return binary\n        data containing linesep characters that implement the folding of the\n        header according to the policy controls.  The value passed in by the\n        email package may contain surrogateescaped binary data.\n\n        \"\"\"\n        raise NotImplementedError\n\n\n@_extend_docstrings\nclass Compat32(Policy):\n\n    \"\"\"+\n    This particular policy is the backward compatibility Policy.  It\n    replicates the behavior of the email package version 5.1.\n    \"\"\"\n\n    mangle_from_ = True\n\n    def _sanitize_header(self, name, value):\n        # If the header value contains surrogates, return a Header using\n        # the unknown-8bit charset to encode the bytes as encoded words.\n        if not isinstance(value, str):\n            # Assume it is already a header object\n            return value\n        if _has_surrogates(value):\n            return header.Header(value, charset=_charset.UNKNOWN8BIT,\n                                 header_name=name)\n        else:\n            return value\n\n    def header_source_parse(self, sourcelines):\n        \"\"\"+\n        The name is parsed as everything up to the ':' and returned unmodified.\n        The value is determined by stripping leading whitespace off the\n        remainder of the first line joined with all subsequent lines, and\n        stripping any trailing carriage return or linefeed characters.\n\n        \"\"\"\n        name, value = sourcelines[0].split(':', 1)\n        value = ''.join((value, *sourcelines[1:])).lstrip(' \\t\\r\\n')\n        return (name, value.rstrip('\\r\\n'))\n\n    def header_store_parse(self, name, value):\n        \"\"\"+\n        The name and value are returned unmodified.\n        \"\"\"\n        return (name, value)\n\n    def header_fetch_parse(self, name, value):\n        \"\"\"+\n        If the value contains binary data, it is converted into a Header object\n        using the unknown-8bit charset.  Otherwise it is returned unmodified.\n        \"\"\"\n        return self._sanitize_header(name, value)\n\n    def fold(self, name, value):\n        \"\"\"+\n        Headers are folded using the Header folding algorithm, which preserves\n        existing line breaks in the value, and wraps each resulting line to the\n        max_line_length.  Non-ASCII binary data are CTE encoded using the\n        unknown-8bit charset.\n\n        \"\"\"\n        return self._fold(name, value, sanitize=True)\n\n    def fold_binary(self, name, value):\n        \"\"\"+\n        Headers are folded using the Header folding algorithm, which preserves\n        existing line breaks in the value, and wraps each resulting line to the\n        max_line_length.  If cte_type is 7bit, non-ascii binary data is CTE\n        encoded using the unknown-8bit charset.  Otherwise the original source\n        header is used, with its existing line breaks and/or binary data.\n\n        \"\"\"\n        folded = self._fold(name, value, sanitize=self.cte_type=='7bit')\n        return folded.encode('ascii', 'surrogateescape')\n\n    def _fold(self, name, value, sanitize):\n        parts = []\n        parts.append('%s: ' % name)\n        if isinstance(value, str):\n            if _has_surrogates(value):\n                if sanitize:\n                    h = header.Header(value,\n                                      charset=_charset.UNKNOWN8BIT,\n                                      header_name=name)\n                else:\n                    # If we have raw 8bit data in a byte string, we have no idea\n                    # what the encoding is.  There is no safe way to split this\n                    # string.  If it's ascii-subset, then we could do a normal\n                    # ascii split, but if it's multibyte then we could break the\n                    # string.  There's no way to know so the least harm seems to\n                    # be to not split the string and risk it being too long.\n                    parts.append(value)\n                    h = None\n            else:\n                h = header.Header(value, header_name=name)\n        else:\n            # Assume it is a Header-like object.\n            h = value\n        if h is not None:\n            # The Header class interprets a value of None for maxlinelen as the\n            # default value of 78, as recommended by RFC 2822.\n            maxlinelen = 0\n            if self.max_line_length is not None:\n                maxlinelen = self.max_line_length\n            parts.append(h.encode(linesep=self.linesep, maxlinelen=maxlinelen))\n        parts.append(self.linesep)\n        return ''.join(parts)\n\n\ncompat32 = Compat32()\n", 382], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\message.py": ["# Copyright (C) 2001-2007 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Basic message object for the email package object model.\"\"\"\n\n__all__ = ['Message', 'EmailMessage']\n\nimport binascii\nimport re\nimport quopri\nfrom io import BytesIO, StringIO\n\n# Intrapackage imports\nfrom email import utils\nfrom email import errors\nfrom email._policybase import compat32\nfrom email import charset as _charset\nfrom email._encoded_words import decode_b\nCharset = _charset.Charset\n\nSEMISPACE = '; '\n\n# Regular expression that matches `special' characters in parameters, the\n# existence of which force quoting of the parameter value.\ntspecials = re.compile(r'[ \\(\\)<>@,;:\\\\\"/\\[\\]\\?=]')\n\n\ndef _splitparam(param):\n    # Split header parameters.  BAW: this may be too simple.  It isn't\n    # strictly RFC 2045 (section 5.1) compliant, but it catches most headers\n    # found in the wild.  We may eventually need a full fledged parser.\n    # RDM: we might have a Header here; for now just stringify it.\n    a, sep, b = str(param).partition(';')\n    if not sep:\n        return a.strip(), None\n    return a.strip(), b.strip()\n\ndef _formatparam(param, value=None, quote=True):\n    \"\"\"Convenience function to format and return a key=value pair.\n\n    This will quote the value if needed or if quote is true.  If value is a\n    three tuple (charset, language, value), it will be encoded according\n    to RFC2231 rules.  If it contains non-ascii characters it will likewise\n    be encoded according to RFC2231 rules, using the utf-8 charset and\n    a null language.\n    \"\"\"\n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.  RFC 2231 encoded values are never quoted, per RFC.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n            return '%s=%s' % (param, value)\n        else:\n            try:\n                value.encode('ascii')\n            except UnicodeEncodeError:\n                param += '*'\n                value = utils.encode_rfc2231(value, 'utf-8', '')\n                return '%s=%s' % (param, value)\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param\n\ndef _parseparam(s):\n    # RDM This might be a Header, so for now stringify it.\n    s = ';' + str(s)\n    plist = []\n    while s[:1] == ';':\n        s = s[1:]\n        end = s.find(';')\n        while end > 0 and (s.count('\"', 0, end) - s.count('\\\\\"', 0, end)) % 2:\n            end = s.find(';', end + 1)\n        if end < 0:\n            end = len(s)\n        f = s[:end]\n        if '=' in f:\n            i = f.index('=')\n            f = f[:i].strip().lower() + '=' + f[i+1:].strip()\n        plist.append(f.strip())\n        s = s[end:]\n    return plist\n\n\ndef _unquotevalue(value):\n    # This is different than utils.collapse_rfc2231_value() because it doesn't\n    # try to convert the value to a unicode.  Message.get_param() and\n    # Message.get_params() are both currently defined to return the tuple in\n    # the face of RFC 2231 parameters.\n    if isinstance(value, tuple):\n        return value[0], value[1], utils.unquote(value[2])\n    else:\n        return utils.unquote(value)\n\n\ndef _decode_uu(encoded):\n    \"\"\"Decode uuencoded data.\"\"\"\n    decoded_lines = []\n    encoded_lines_iter = iter(encoded.splitlines())\n    for line in encoded_lines_iter:\n        if line.startswith(b\"begin \"):\n            mode, _, path = line.removeprefix(b\"begin \").partition(b\" \")\n            try:\n                int(mode, base=8)\n            except ValueError:\n                continue\n            else:\n                break\n    else:\n        raise ValueError(\"`begin` line not found\")\n    for line in encoded_lines_iter:\n        if not line:\n            raise ValueError(\"Truncated input\")\n        elif line.strip(b' \\t\\r\\n\\f') == b'end':\n            break\n        try:\n            decoded_line = binascii.a2b_uu(line)\n        except binascii.Error:\n            # Workaround for broken uuencoders by /Fredrik Lundh\n            nbytes = (((line[0]-32) & 63) * 4 + 5) // 3\n            decoded_line = binascii.a2b_uu(line[:nbytes])\n        decoded_lines.append(decoded_line)\n\n    return b''.join(decoded_lines)\n\n\nclass Message:\n    \"\"\"Basic message object.\n\n    A message object is defined as something that has a bunch of RFC 2822\n    headers and a payload.  It may optionally have an envelope header\n    (a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\n    multipart or a message/rfc822), then the payload is a list of Message\n    objects, otherwise it is a string.\n\n    Message objects implement part of the `mapping' interface, which assumes\n    there is exactly one occurrence of the header per message.  Some headers\n    do in fact appear multiple times (e.g. Received) and for those headers,\n    you must use the explicit API to set or get all the headers.  Not all of\n    the mapping methods are implemented.\n    \"\"\"\n    def __init__(self, policy=compat32):\n        self.policy = policy\n        self._headers = []\n        self._unixfrom = None\n        self._payload = None\n        self._charset = None\n        # Defaults for multipart messages\n        self.preamble = self.epilogue = None\n        self.defects = []\n        # Default content type\n        self._default_type = 'text/plain'\n\n    def __str__(self):\n        \"\"\"Return the entire formatted message as a string.\n        \"\"\"\n        return self.as_string()\n\n    def as_string(self, unixfrom=False, maxheaderlen=0, policy=None):\n        \"\"\"Return the entire formatted message as a string.\n\n        Optional 'unixfrom', when true, means include the Unix From_ envelope\n        header.  For backward compatibility reasons, if maxheaderlen is\n        not specified it defaults to 0, so you must override it explicitly\n        if you want a different maxheaderlen.  'policy' is passed to the\n        Generator instance used to serialize the message; if it is not\n        specified the policy associated with the message instance is used.\n\n        If the message object contains binary data that is not encoded\n        according to RFC standards, the non-compliant data will be replaced by\n        unicode \"unknown character\" code points.\n        \"\"\"\n        from email.generator import Generator\n        policy = self.policy if policy is None else policy\n        fp = StringIO()\n        g = Generator(fp,\n                      mangle_from_=False,\n                      maxheaderlen=maxheaderlen,\n                      policy=policy)\n        g.flatten(self, unixfrom=unixfrom)\n        return fp.getvalue()\n\n    def __bytes__(self):\n        \"\"\"Return the entire formatted message as a bytes object.\n        \"\"\"\n        return self.as_bytes()\n\n    def as_bytes(self, unixfrom=False, policy=None):\n        \"\"\"Return the entire formatted message as a bytes object.\n\n        Optional 'unixfrom', when true, means include the Unix From_ envelope\n        header.  'policy' is passed to the BytesGenerator instance used to\n        serialize the message; if not specified the policy associated with\n        the message instance is used.\n        \"\"\"\n        from email.generator import BytesGenerator\n        policy = self.policy if policy is None else policy\n        fp = BytesIO()\n        g = BytesGenerator(fp, mangle_from_=False, policy=policy)\n        g.flatten(self, unixfrom=unixfrom)\n        return fp.getvalue()\n\n    def is_multipart(self):\n        \"\"\"Return True if the message consists of multiple parts.\"\"\"\n        return isinstance(self._payload, list)\n\n    #\n    # Unix From_ line\n    #\n    def set_unixfrom(self, unixfrom):\n        self._unixfrom = unixfrom\n\n    def get_unixfrom(self):\n        return self._unixfrom\n\n    #\n    # Payload manipulation.\n    #\n    def attach(self, payload):\n        \"\"\"Add the given payload to the current payload.\n\n        The current payload will always be a list of objects after this method\n        is called.  If you want to set the payload to a scalar object, use\n        set_payload() instead.\n        \"\"\"\n        if self._payload is None:\n            self._payload = [payload]\n        else:\n            try:\n                self._payload.append(payload)\n            except AttributeError:\n                raise TypeError(\"Attach is not valid on a message with a\"\n                                \" non-multipart payload\")\n\n    def get_payload(self, i=None, decode=False):\n        \"\"\"Return a reference to the payload.\n\n        The payload will either be a list object or a string.  If you mutate\n        the list object, you modify the message's payload in place.  Optional\n        i returns that index into the payload.\n\n        Optional decode is a flag indicating whether the payload should be\n        decoded or not, according to the Content-Transfer-Encoding header\n        (default is False).\n\n        When True and the message is not a multipart, the payload will be\n        decoded if this header's value is `quoted-printable' or `base64'.  If\n        some other encoding is used, or the header is missing, or if the\n        payload has bogus data (i.e. bogus base64 or uuencoded data), the\n        payload is returned as-is.\n\n        If the message is a multipart and the decode flag is True, then None\n        is returned.\n        \"\"\"\n        # Here is the logic table for this code, based on the email5.0.0 code:\n        #   i     decode  is_multipart  result\n        # ------  ------  ------------  ------------------------------\n        #  None   True    True          None\n        #   i     True    True          None\n        #  None   False   True          _payload (a list)\n        #   i     False   True          _payload element i (a Message)\n        #   i     False   False         error (not a list)\n        #   i     True    False         error (not a list)\n        #  None   False   False         _payload\n        #  None   True    False         _payload decoded (bytes)\n        # Note that Barry planned to factor out the 'decode' case, but that\n        # isn't so easy now that we handle the 8 bit data, which needs to be\n        # converted in both the decode and non-decode path.\n        if self.is_multipart():\n            if decode:\n                return None\n            if i is None:\n                return self._payload\n            else:\n                return self._payload[i]\n        # For backward compatibility, Use isinstance and this error message\n        # instead of the more logical is_multipart test.\n        if i is not None and not isinstance(self._payload, list):\n            raise TypeError('Expected list, got %s' % type(self._payload))\n        payload = self._payload\n        cte = self.get('content-transfer-encoding', '')\n        if hasattr(cte, 'cte'):\n            cte = cte.cte\n        else:\n            # cte might be a Header, so for now stringify it.\n            cte = str(cte).strip().lower()\n        # payload may be bytes here.\n        if not decode:\n            if isinstance(payload, str) and utils._has_surrogates(payload):\n                try:\n                    bpayload = payload.encode('ascii', 'surrogateescape')\n                    try:\n                        payload = bpayload.decode(self.get_content_charset('ascii'), 'replace')\n                    except LookupError:\n                        payload = bpayload.decode('ascii', 'replace')\n                except UnicodeEncodeError:\n                    pass\n            return payload\n        if isinstance(payload, str):\n            try:\n                bpayload = payload.encode('ascii', 'surrogateescape')\n            except UnicodeEncodeError:\n                # This won't happen for RFC compliant messages (messages\n                # containing only ASCII code points in the unicode input).\n                # If it does happen, turn the string into bytes in a way\n                # guaranteed not to fail.\n                bpayload = payload.encode('raw-unicode-escape')\n        if cte == 'quoted-printable':\n            return quopri.decodestring(bpayload)\n        elif cte == 'base64':\n            # XXX: this is a bit of a hack; decode_b should probably be factored\n            # out somewhere, but I haven't figured out where yet.\n            value, defects = decode_b(b''.join(bpayload.splitlines()))\n            for defect in defects:\n                self.policy.handle_defect(self, defect)\n            return value\n        elif cte in ('x-uuencode', 'uuencode', 'uue', 'x-uue'):\n            try:\n                return _decode_uu(bpayload)\n            except ValueError:\n                # Some decoding problem.\n                return bpayload\n        if isinstance(payload, str):\n            return bpayload\n        return payload\n\n    def set_payload(self, payload, charset=None):\n        \"\"\"Set the payload to the given value.\n\n        Optional charset sets the message's default character set.  See\n        set_charset() for details.\n        \"\"\"\n        if hasattr(payload, 'encode'):\n            if charset is None:\n                self._payload = payload\n                return\n            if not isinstance(charset, Charset):\n                charset = Charset(charset)\n            payload = payload.encode(charset.output_charset, 'surrogateescape')\n        if hasattr(payload, 'decode'):\n            self._payload = payload.decode('ascii', 'surrogateescape')\n        else:\n            self._payload = payload\n        if charset is not None:\n            self.set_charset(charset)\n\n    def set_charset(self, charset):\n        \"\"\"Set the charset of the payload to a given character set.\n\n        charset can be a Charset instance, a string naming a character set, or\n        None.  If it is a string it will be converted to a Charset instance.\n        If charset is None, the charset parameter will be removed from the\n        Content-Type field.  Anything else will generate a TypeError.\n\n        The message will be assumed to be of type text/* encoded with\n        charset.input_charset.  It will be converted to charset.output_charset\n        and encoded properly, if needed, when generating the plain text\n        representation of the message.  MIME headers (MIME-Version,\n        Content-Type, Content-Transfer-Encoding) will be added as needed.\n        \"\"\"\n        if charset is None:\n            self.del_param('charset')\n            self._charset = None\n            return\n        if not isinstance(charset, Charset):\n            charset = Charset(charset)\n        self._charset = charset\n        if 'MIME-Version' not in self:\n            self.add_header('MIME-Version', '1.0')\n        if 'Content-Type' not in self:\n            self.add_header('Content-Type', 'text/plain',\n                            charset=charset.get_output_charset())\n        else:\n            self.set_param('charset', charset.get_output_charset())\n        if charset != charset.get_output_charset():\n            self._payload = charset.body_encode(self._payload)\n        if 'Content-Transfer-Encoding' not in self:\n            cte = charset.get_body_encoding()\n            try:\n                cte(self)\n            except TypeError:\n                # This 'if' is for backward compatibility, it allows unicode\n                # through even though that won't work correctly if the\n                # message is serialized.\n                payload = self._payload\n                if payload:\n                    try:\n                        payload = payload.encode('ascii', 'surrogateescape')\n                    except UnicodeError:\n                        payload = payload.encode(charset.output_charset)\n                self._payload = charset.body_encode(payload)\n                self.add_header('Content-Transfer-Encoding', cte)\n\n    def get_charset(self):\n        \"\"\"Return the Charset instance associated with the message's payload.\n        \"\"\"\n        return self._charset\n\n    #\n    # MAPPING INTERFACE (partial)\n    #\n    def __len__(self):\n        \"\"\"Return the total number of headers, including duplicates.\"\"\"\n        return len(self._headers)\n\n    def __getitem__(self, name):\n        \"\"\"Get a header value.\n\n        Return None if the header is missing instead of raising an exception.\n\n        Note that if the header appeared multiple times, exactly which\n        occurrence gets returned is undefined.  Use get_all() to get all\n        the values matching a header field name.\n        \"\"\"\n        return self.get(name)\n\n    def __setitem__(self, name, val):\n        \"\"\"Set the value of a header.\n\n        Note: this does not overwrite an existing header with the same field\n        name.  Use __delitem__() first to delete any existing headers.\n        \"\"\"\n        max_count = self.policy.header_max_count(name)\n        if max_count:\n            lname = name.lower()\n            found = 0\n            for k, v in self._headers:\n                if k.lower() == lname:\n                    found += 1\n                    if found >= max_count:\n                        raise ValueError(\"There may be at most {} {} headers \"\n                                         \"in a message\".format(max_count, name))\n        self._headers.append(self.policy.header_store_parse(name, val))\n\n    def __delitem__(self, name):\n        \"\"\"Delete all occurrences of a header, if present.\n\n        Does not raise an exception if the header is missing.\n        \"\"\"\n        name = name.lower()\n        newheaders = []\n        for k, v in self._headers:\n            if k.lower() != name:\n                newheaders.append((k, v))\n        self._headers = newheaders\n\n    def __contains__(self, name):\n        name_lower = name.lower()\n        for k, v in self._headers:\n            if name_lower == k.lower():\n                return True\n        return False\n\n    def __iter__(self):\n        for field, value in self._headers:\n            yield field\n\n    def keys(self):\n        \"\"\"Return a list of all the message's header field names.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [k for k, v in self._headers]\n\n    def values(self):\n        \"\"\"Return a list of all the message's header values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [self.policy.header_fetch_parse(k, v)\n                for k, v in self._headers]\n\n    def items(self):\n        \"\"\"Get all the message's header fields and values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [(k, self.policy.header_fetch_parse(k, v))\n                for k, v in self._headers]\n\n    def get(self, name, failobj=None):\n        \"\"\"Get a header value.\n\n        Like __getitem__() but return failobj instead of None when the field\n        is missing.\n        \"\"\"\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                return self.policy.header_fetch_parse(k, v)\n        return failobj\n\n    #\n    # \"Internal\" methods (public API, but only intended for use by a parser\n    # or generator, not normal application code.\n    #\n\n    def set_raw(self, name, value):\n        \"\"\"Store name and value in the model without modification.\n\n        This is an \"internal\" API, intended only for use by a parser.\n        \"\"\"\n        self._headers.append((name, value))\n\n    def raw_items(self):\n        \"\"\"Return the (name, value) header pairs without modification.\n\n        This is an \"internal\" API, intended only for use by a generator.\n        \"\"\"\n        return iter(self._headers.copy())\n\n    #\n    # Additional useful stuff\n    #\n\n    def get_all(self, name, failobj=None):\n        \"\"\"Return a list of all the values for the named field.\n\n        These will be sorted in the order they appeared in the original\n        message, and may contain duplicates.  Any fields deleted and\n        re-inserted are always appended to the header list.\n\n        If no such fields exist, failobj is returned (defaults to None).\n        \"\"\"\n        values = []\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                values.append(self.policy.header_fetch_parse(k, v))\n        if not values:\n            return failobj\n        return values\n\n    def add_header(self, _name, _value, **_params):\n        \"\"\"Extended header setting.\n\n        name is the header field to add.  keyword arguments can be used to set\n        additional parameters for the header field, with underscores converted\n        to dashes.  Normally the parameter will be added as key=\"value\" unless\n        value is None, in which case only the key will be added.  If a\n        parameter value contains non-ASCII characters it can be specified as a\n        three-tuple of (charset, language, value), in which case it will be\n        encoded according to RFC2231 rules.  Otherwise it will be encoded using\n        the utf-8 charset and a language of ''.\n\n        Examples:\n\n        msg.add_header('content-disposition', 'attachment', filename='bud.gif')\n        msg.add_header('content-disposition', 'attachment',\n                       filename=('utf-8', '', Fu\u00dfballer.ppt'))\n        msg.add_header('content-disposition', 'attachment',\n                       filename='Fu\u00dfballer.ppt'))\n        \"\"\"\n        parts = []\n        for k, v in _params.items():\n            if v is None:\n                parts.append(k.replace('_', '-'))\n            else:\n                parts.append(_formatparam(k.replace('_', '-'), v))\n        if _value is not None:\n            parts.insert(0, _value)\n        self[_name] = SEMISPACE.join(parts)\n\n    def replace_header(self, _name, _value):\n        \"\"\"Replace a header.\n\n        Replace the first matching header found in the message, retaining\n        header order and case.  If no matching header was found, a KeyError is\n        raised.\n        \"\"\"\n        _name = _name.lower()\n        for i, (k, v) in zip(range(len(self._headers)), self._headers):\n            if k.lower() == _name:\n                self._headers[i] = self.policy.header_store_parse(k, _value)\n                break\n        else:\n            raise KeyError(_name)\n\n    #\n    # Use these three methods instead of the three above.\n    #\n\n    def get_content_type(self):\n        \"\"\"Return the message's content type.\n\n        The returned string is coerced to lower case of the form\n        `maintype/subtype'.  If there was no Content-Type header in the\n        message, the default type as given by get_default_type() will be\n        returned.  Since according to RFC 2045, messages always have a default\n        type this will always return a value.\n\n        RFC 2045 defines a message's default type to be text/plain unless it\n        appears inside a multipart/digest container, in which case it would be\n        message/rfc822.\n        \"\"\"\n        missing = object()\n        value = self.get('content-type', missing)\n        if value is missing:\n            # This should have no parameters\n            return self.get_default_type()\n        ctype = _splitparam(value)[0].lower()\n        # RFC 2045, section 5.2 says if its invalid, use text/plain\n        if ctype.count('/') != 1:\n            return 'text/plain'\n        return ctype\n\n    def get_content_maintype(self):\n        \"\"\"Return the message's main content type.\n\n        This is the `maintype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[0]\n\n    def get_content_subtype(self):\n        \"\"\"Returns the message's sub-content type.\n\n        This is the `subtype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[1]\n\n    def get_default_type(self):\n        \"\"\"Return the `default' content type.\n\n        Most messages have a default content type of text/plain, except for\n        messages that are subparts of multipart/digest containers.  Such\n        subparts have a default content type of message/rfc822.\n        \"\"\"\n        return self._default_type\n\n    def set_default_type(self, ctype):\n        \"\"\"Set the `default' content type.\n\n        ctype should be either \"text/plain\" or \"message/rfc822\", although this\n        is not enforced.  The default content type is not stored in the\n        Content-Type header.\n        \"\"\"\n        self._default_type = ctype\n\n    def _get_params_preserve(self, failobj, header):\n        # Like get_params() but preserves the quoting of values.  BAW:\n        # should this be part of the public interface?\n        missing = object()\n        value = self.get(header, missing)\n        if value is missing:\n            return failobj\n        params = []\n        for p in _parseparam(value):\n            try:\n                name, val = p.split('=', 1)\n                name = name.strip()\n                val = val.strip()\n            except ValueError:\n                # Must have been a bare attribute\n                name = p.strip()\n                val = ''\n            params.append((name, val))\n        params = utils.decode_params(params)\n        return params\n\n    def get_params(self, failobj=None, header='content-type', unquote=True):\n        \"\"\"Return the message's Content-Type parameters, as a list.\n\n        The elements of the returned list are 2-tuples of key/value pairs, as\n        split on the `=' sign.  The left hand side of the `=' is the key,\n        while the right hand side is the value.  If there is no `=' sign in\n        the parameter the value is the empty string.  The value is as\n        described in the get_param() method.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header.  Optional header is the header to search instead of\n        Content-Type.  If unquote is True, the value is unquoted.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, header)\n        if params is missing:\n            return failobj\n        if unquote:\n            return [(k, _unquotevalue(v)) for k, v in params]\n        else:\n            return params\n\n    def get_param(self, param, failobj=None, header='content-type',\n                  unquote=True):\n        \"\"\"Return the parameter value if found in the Content-Type header.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header, or the Content-Type header has no such parameter.  Optional\n        header is the header to search instead of Content-Type.\n\n        Parameter keys are always compared case insensitively.  The return\n        value can either be a string, or a 3-tuple if the parameter was RFC\n        2231 encoded.  When it's a 3-tuple, the elements of the value are of\n        the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and\n        LANGUAGE can be None, in which case you should consider VALUE to be\n        encoded in the us-ascii charset.  You can usually ignore LANGUAGE.\n        The parameter value (either the returned string, or the VALUE item in\n        the 3-tuple) is always unquoted, unless unquote is set to False.\n\n        If your application doesn't care whether the parameter was RFC 2231\n        encoded, it can turn the return value into a string as follows:\n\n            rawparam = msg.get_param('foo')\n            param = email.utils.collapse_rfc2231_value(rawparam)\n\n        \"\"\"\n        if header not in self:\n            return failobj\n        for k, v in self._get_params_preserve(failobj, header):\n            if k.lower() == param.lower():\n                if unquote:\n                    return _unquotevalue(v)\n                else:\n                    return v\n        return failobj\n\n    def set_param(self, param, value, header='Content-Type', requote=True,\n                  charset=None, language='', replace=False):\n        \"\"\"Set a parameter in the Content-Type header.\n\n        If the parameter already exists in the header, its value will be\n        replaced with the new value.\n\n        If header is Content-Type and has not yet been defined for this\n        message, it will be set to \"text/plain\" and the new parameter and\n        value will be appended as per RFC 2045.\n\n        An alternate header can be specified in the header argument, and all\n        parameters will be quoted as necessary unless requote is False.\n\n        If charset is specified, the parameter will be encoded according to RFC\n        2231.  Optional language specifies the RFC 2231 language, defaulting\n        to the empty string.  Both charset and language should be strings.\n        \"\"\"\n        if not isinstance(value, tuple) and charset:\n            value = (charset, language, value)\n\n        if header not in self and header.lower() == 'content-type':\n            ctype = 'text/plain'\n        else:\n            ctype = self.get(header)\n        if not self.get_param(param, header=header):\n            if not ctype:\n                ctype = _formatparam(param, value, requote)\n            else:\n                ctype = SEMISPACE.join(\n                    [ctype, _formatparam(param, value, requote)])\n        else:\n            ctype = ''\n            for old_param, old_value in self.get_params(header=header,\n                                                        unquote=requote):\n                append_param = ''\n                if old_param.lower() == param.lower():\n                    append_param = _formatparam(param, value, requote)\n                else:\n                    append_param = _formatparam(old_param, old_value, requote)\n                if not ctype:\n                    ctype = append_param\n                else:\n                    ctype = SEMISPACE.join([ctype, append_param])\n        if ctype != self.get(header):\n            if replace:\n                self.replace_header(header, ctype)\n            else:\n                del self[header]\n                self[header] = ctype\n\n    def del_param(self, param, header='content-type', requote=True):\n        \"\"\"Remove the given parameter completely from the Content-Type header.\n\n        The header will be re-written in place without the parameter or its\n        value. All values will be quoted as necessary unless requote is\n        False.  Optional header specifies an alternative to the Content-Type\n        header.\n        \"\"\"\n        if header not in self:\n            return\n        new_ctype = ''\n        for p, v in self.get_params(header=header, unquote=requote):\n            if p.lower() != param.lower():\n                if not new_ctype:\n                    new_ctype = _formatparam(p, v, requote)\n                else:\n                    new_ctype = SEMISPACE.join([new_ctype,\n                                                _formatparam(p, v, requote)])\n        if new_ctype != self.get(header):\n            del self[header]\n            self[header] = new_ctype\n\n    def set_type(self, type, header='Content-Type', requote=True):\n        \"\"\"Set the main type and subtype for the Content-Type header.\n\n        type must be a string in the form \"maintype/subtype\", otherwise a\n        ValueError is raised.\n\n        This method replaces the Content-Type header, keeping all the\n        parameters in place.  If requote is False, this leaves the existing\n        header's quoting as is.  Otherwise, the parameters will be quoted (the\n        default).\n\n        An alternative header can be specified in the header argument.  When\n        the Content-Type header is set, we'll always also add a MIME-Version\n        header.\n        \"\"\"\n        # BAW: should we be strict?\n        if not type.count('/') == 1:\n            raise ValueError\n        # Set the Content-Type, you get a MIME-Version\n        if header.lower() == 'content-type':\n            del self['mime-version']\n            self['MIME-Version'] = '1.0'\n        if header not in self:\n            self[header] = type\n            return\n        params = self.get_params(header=header, unquote=requote)\n        del self[header]\n        self[header] = type\n        # Skip the first param; it's the old type.\n        for p, v in params[1:]:\n            self.set_param(p, v, header, requote)\n\n    def get_filename(self, failobj=None):\n        \"\"\"Return the filename associated with the payload if present.\n\n        The filename is extracted from the Content-Disposition header's\n        `filename' parameter, and it is unquoted.  If that header is missing\n        the `filename' parameter, this method falls back to looking for the\n        `name' parameter.\n        \"\"\"\n        missing = object()\n        filename = self.get_param('filename', missing, 'content-disposition')\n        if filename is missing:\n            filename = self.get_param('name', missing, 'content-type')\n        if filename is missing:\n            return failobj\n        return utils.collapse_rfc2231_value(filename).strip()\n\n    def get_boundary(self, failobj=None):\n        \"\"\"Return the boundary associated with the payload if present.\n\n        The boundary is extracted from the Content-Type header's `boundary'\n        parameter, and it is unquoted.\n        \"\"\"\n        missing = object()\n        boundary = self.get_param('boundary', missing)\n        if boundary is missing:\n            return failobj\n        # RFC 2046 says that boundaries may begin but not end in w/s\n        return utils.collapse_rfc2231_value(boundary).rstrip()\n\n    def set_boundary(self, boundary):\n        \"\"\"Set the boundary parameter in Content-Type to 'boundary'.\n\n        This is subtly different than deleting the Content-Type header and\n        adding a new one with a new boundary parameter via add_header().  The\n        main difference is that using the set_boundary() method preserves the\n        order of the Content-Type header in the original message.\n\n        HeaderParseError is raised if the message has no Content-Type header.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, 'content-type')\n        if params is missing:\n            # There was no Content-Type header, and we don't know what type\n            # to set it to, so raise an exception.\n            raise errors.HeaderParseError('No Content-Type header found')\n        newparams = []\n        foundp = False\n        for pk, pv in params:\n            if pk.lower() == 'boundary':\n                newparams.append(('boundary', '\"%s\"' % boundary))\n                foundp = True\n            else:\n                newparams.append((pk, pv))\n        if not foundp:\n            # The original Content-Type header had no boundary attribute.\n            # Tack one on the end.  BAW: should we raise an exception\n            # instead???\n            newparams.append(('boundary', '\"%s\"' % boundary))\n        # Replace the existing Content-Type header with the new value\n        newheaders = []\n        for h, v in self._headers:\n            if h.lower() == 'content-type':\n                parts = []\n                for k, v in newparams:\n                    if v == '':\n                        parts.append(k)\n                    else:\n                        parts.append('%s=%s' % (k, v))\n                val = SEMISPACE.join(parts)\n                newheaders.append(self.policy.header_store_parse(h, val))\n\n            else:\n                newheaders.append((h, v))\n        self._headers = newheaders\n\n    def get_content_charset(self, failobj=None):\n        \"\"\"Return the charset parameter of the Content-Type header.\n\n        The returned string is always coerced to lower case.  If there is no\n        Content-Type header, or if that header has no charset parameter,\n        failobj is returned.\n        \"\"\"\n        missing = object()\n        charset = self.get_param('charset', missing)\n        if charset is missing:\n            return failobj\n        if isinstance(charset, tuple):\n            # RFC 2231 encoded, so decode it, and it better end up as ascii.\n            pcharset = charset[0] or 'us-ascii'\n            try:\n                # LookupError will be raised if the charset isn't known to\n                # Python.  UnicodeError will be raised if the encoded text\n                # contains a character not in the charset.\n                as_bytes = charset[2].encode('raw-unicode-escape')\n                charset = str(as_bytes, pcharset)\n            except (LookupError, UnicodeError):\n                charset = charset[2]\n        # charset characters must be in us-ascii range\n        try:\n            charset.encode('us-ascii')\n        except UnicodeError:\n            return failobj\n        # RFC 2046, $4.1.2 says charsets are not case sensitive\n        return charset.lower()\n\n    def get_charsets(self, failobj=None):\n        \"\"\"Return a list containing the charset(s) used in this message.\n\n        The returned list of items describes the Content-Type headers'\n        charset parameter for this message and all the subparts in its\n        payload.\n\n        Each item will either be a string (the value of the charset parameter\n        in the Content-Type header of that part) or the value of the\n        'failobj' parameter (defaults to None), if the part does not have a\n        main MIME type of \"text\", or the charset is not defined.\n\n        The list will contain one string for each part of the message, plus\n        one for the container message (i.e. self), so that a non-multipart\n        message will still return a list of length 1.\n        \"\"\"\n        return [part.get_content_charset(failobj) for part in self.walk()]\n\n    def get_content_disposition(self):\n        \"\"\"Return the message's content-disposition if it exists, or None.\n\n        The return values can be either 'inline', 'attachment' or None\n        according to the rfc2183.\n        \"\"\"\n        value = self.get('content-disposition')\n        if value is None:\n            return None\n        c_d = _splitparam(value)[0].lower()\n        return c_d\n\n    # I.e. def walk(self): ...\n    from email.iterators import walk\n\n\nclass MIMEPart(Message):\n\n    def __init__(self, policy=None):\n        if policy is None:\n            from email.policy import default\n            policy = default\n        super().__init__(policy)\n\n\n    def as_string(self, unixfrom=False, maxheaderlen=None, policy=None):\n        \"\"\"Return the entire formatted message as a string.\n\n        Optional 'unixfrom', when true, means include the Unix From_ envelope\n        header.  maxheaderlen is retained for backward compatibility with the\n        base Message class, but defaults to None, meaning that the policy value\n        for max_line_length controls the header maximum length.  'policy' is\n        passed to the Generator instance used to serialize the message; if it\n        is not specified the policy associated with the message instance is\n        used.\n        \"\"\"\n        policy = self.policy if policy is None else policy\n        if maxheaderlen is None:\n            maxheaderlen = policy.max_line_length\n        return super().as_string(unixfrom, maxheaderlen, policy)\n\n    def __str__(self):\n        return self.as_string(policy=self.policy.clone(utf8=True))\n\n    def is_attachment(self):\n        c_d = self.get('content-disposition')\n        return False if c_d is None else c_d.content_disposition == 'attachment'\n\n    def _find_body(self, part, preferencelist):\n        if part.is_attachment():\n            return\n        maintype, subtype = part.get_content_type().split('/')\n        if maintype == 'text':\n            if subtype in preferencelist:\n                yield (preferencelist.index(subtype), part)\n            return\n        if maintype != 'multipart' or not self.is_multipart():\n            return\n        if subtype != 'related':\n            for subpart in part.iter_parts():\n                yield from self._find_body(subpart, preferencelist)\n            return\n        if 'related' in preferencelist:\n            yield (preferencelist.index('related'), part)\n        candidate = None\n        start = part.get_param('start')\n        if start:\n            for subpart in part.iter_parts():\n                if subpart['content-id'] == start:\n                    candidate = subpart\n                    break\n        if candidate is None:\n            subparts = part.get_payload()\n            candidate = subparts[0] if subparts else None\n        if candidate is not None:\n            yield from self._find_body(candidate, preferencelist)\n\n    def get_body(self, preferencelist=('related', 'html', 'plain')):\n        \"\"\"Return best candidate mime part for display as 'body' of message.\n\n        Do a depth first search, starting with self, looking for the first part\n        matching each of the items in preferencelist, and return the part\n        corresponding to the first item that has a match, or None if no items\n        have a match.  If 'related' is not included in preferencelist, consider\n        the root part of any multipart/related encountered as a candidate\n        match.  Ignore parts with 'Content-Disposition: attachment'.\n        \"\"\"\n        best_prio = len(preferencelist)\n        body = None\n        for prio, part in self._find_body(self, preferencelist):\n            if prio < best_prio:\n                best_prio = prio\n                body = part\n                if prio == 0:\n                    break\n        return body\n\n    _body_types = {('text', 'plain'),\n                   ('text', 'html'),\n                   ('multipart', 'related'),\n                   ('multipart', 'alternative')}\n    def iter_attachments(self):\n        \"\"\"Return an iterator over the non-main parts of a multipart.\n\n        Skip the first of each occurrence of text/plain, text/html,\n        multipart/related, or multipart/alternative in the multipart (unless\n        they have a 'Content-Disposition: attachment' header) and include all\n        remaining subparts in the returned iterator.  When applied to a\n        multipart/related, return all parts except the root part.  Return an\n        empty iterator when applied to a multipart/alternative or a\n        non-multipart.\n        \"\"\"\n        maintype, subtype = self.get_content_type().split('/')\n        if maintype != 'multipart' or subtype == 'alternative':\n            return\n        payload = self.get_payload()\n        # Certain malformed messages can have content type set to `multipart/*`\n        # but still have single part body, in which case payload.copy() can\n        # fail with AttributeError.\n        try:\n            parts = payload.copy()\n        except AttributeError:\n            # payload is not a list, it is most probably a string.\n            return\n\n        if maintype == 'multipart' and subtype == 'related':\n            # For related, we treat everything but the root as an attachment.\n            # The root may be indicated by 'start'; if there's no start or we\n            # can't find the named start, treat the first subpart as the root.\n            start = self.get_param('start')\n            if start:\n                found = False\n                attachments = []\n                for part in parts:\n                    if part.get('content-id') == start:\n                        found = True\n                    else:\n                        attachments.append(part)\n                if found:\n                    yield from attachments\n                    return\n            parts.pop(0)\n            yield from parts\n            return\n        # Otherwise we more or less invert the remaining logic in get_body.\n        # This only really works in edge cases (ex: non-text related or\n        # alternatives) if the sending agent sets content-disposition.\n        seen = []   # Only skip the first example of each candidate type.\n        for part in parts:\n            maintype, subtype = part.get_content_type().split('/')\n            if ((maintype, subtype) in self._body_types and\n                    not part.is_attachment() and subtype not in seen):\n                seen.append(subtype)\n                continue\n            yield part\n\n    def iter_parts(self):\n        \"\"\"Return an iterator over all immediate subparts of a multipart.\n\n        Return an empty iterator for a non-multipart.\n        \"\"\"\n        if self.is_multipart():\n            yield from self.get_payload()\n\n    def get_content(self, *args, content_manager=None, **kw):\n        if content_manager is None:\n            content_manager = self.policy.content_manager\n        return content_manager.get_content(self, *args, **kw)\n\n    def set_content(self, *args, content_manager=None, **kw):\n        if content_manager is None:\n            content_manager = self.policy.content_manager\n        content_manager.set_content(self, *args, **kw)\n\n    def _make_multipart(self, subtype, disallowed_subtypes, boundary):\n        if self.get_content_maintype() == 'multipart':\n            existing_subtype = self.get_content_subtype()\n            disallowed_subtypes = disallowed_subtypes + (subtype,)\n            if existing_subtype in disallowed_subtypes:\n                raise ValueError(\"Cannot convert {} to {}\".format(\n                    existing_subtype, subtype))\n        keep_headers = []\n        part_headers = []\n        for name, value in self._headers:\n            if name.lower().startswith('content-'):\n                part_headers.append((name, value))\n            else:\n                keep_headers.append((name, value))\n        if part_headers:\n            # There is existing content, move it to the first subpart.\n            part = type(self)(policy=self.policy)\n            part._headers = part_headers\n            part._payload = self._payload\n            self._payload = [part]\n        else:\n            self._payload = []\n        self._headers = keep_headers\n        self['Content-Type'] = 'multipart/' + subtype\n        if boundary is not None:\n            self.set_param('boundary', boundary)\n\n    def make_related(self, boundary=None):\n        self._make_multipart('related', ('alternative', 'mixed'), boundary)\n\n    def make_alternative(self, boundary=None):\n        self._make_multipart('alternative', ('mixed',), boundary)\n\n    def make_mixed(self, boundary=None):\n        self._make_multipart('mixed', (), boundary)\n\n    def _add_multipart(self, _subtype, *args, _disp=None, **kw):\n        if (self.get_content_maintype() != 'multipart' or\n                self.get_content_subtype() != _subtype):\n            getattr(self, 'make_' + _subtype)()\n        part = type(self)(policy=self.policy)\n        part.set_content(*args, **kw)\n        if _disp and 'content-disposition' not in part:\n            part['Content-Disposition'] = _disp\n        self.attach(part)\n\n    def add_related(self, *args, **kw):\n        self._add_multipart('related', *args, _disp='inline', **kw)\n\n    def add_alternative(self, *args, **kw):\n        self._add_multipart('alternative', *args, **kw)\n\n    def add_attachment(self, *args, **kw):\n        self._add_multipart('mixed', *args, _disp='attachment', **kw)\n\n    def clear(self):\n        self._headers = []\n        self._payload = None\n\n    def clear_content(self):\n        self._headers = [(n, v) for n, v in self._headers\n                         if not n.lower().startswith('content-')]\n        self._payload = None\n\n\nclass EmailMessage(MIMEPart):\n\n    def set_content(self, *args, **kw):\n        super().set_content(*args, **kw)\n        if 'MIME-Version' not in self:\n            self['MIME-Version'] = '1.0'\n", 1209], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\_adapters.py": ["import functools\nimport warnings\nimport re\nimport textwrap\nimport email.message\n\nfrom ._text import FoldedCase\n\n\n# Do not remove prior to 2024-01-01 or Python 3.14\n_warn = functools.partial(\n    warnings.warn,\n    \"Implicit None on return values is deprecated and will raise KeyErrors.\",\n    DeprecationWarning,\n    stacklevel=2,\n)\n\n\nclass Message(email.message.Message):\n    multiple_use_keys = set(\n        map(\n            FoldedCase,\n            [\n                'Classifier',\n                'Obsoletes-Dist',\n                'Platform',\n                'Project-URL',\n                'Provides-Dist',\n                'Provides-Extra',\n                'Requires-Dist',\n                'Requires-External',\n                'Supported-Platform',\n                'Dynamic',\n            ],\n        )\n    )\n    \"\"\"\n    Keys that may be indicated multiple times per PEP 566.\n    \"\"\"\n\n    def __new__(cls, orig: email.message.Message):\n        res = super().__new__(cls)\n        vars(res).update(vars(orig))\n        return res\n\n    def __init__(self, *args, **kwargs):\n        self._headers = self._repair_headers()\n\n    # suppress spurious error from mypy\n    def __iter__(self):\n        return super().__iter__()\n\n    def __getitem__(self, item):\n        \"\"\"\n        Warn users that a ``KeyError`` can be expected when a\n        mising key is supplied. Ref python/importlib_metadata#371.\n        \"\"\"\n        res = super().__getitem__(item)\n        if res is None:\n            _warn()\n        return res\n\n    def _repair_headers(self):\n        def redent(value):\n            \"Correct for RFC822 indentation\"\n            if not value or '\\n' not in value:\n                return value\n            return textwrap.dedent(' ' * 8 + value)\n\n        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]\n        if self._payload:\n            headers.append(('Description', self.get_payload()))\n        return headers\n\n    @property\n    def json(self):\n        \"\"\"\n        Convert PackageMetadata to a JSON-compatible format\n        per PEP 0566.\n        \"\"\"\n\n        def transform(key):\n            value = self.get_all(key) if key in self.multiple_use_keys else self[key]\n            if key == 'Keywords':\n                value = re.split(r'\\s+', value)\n            tk = key.lower().replace('-', '_')\n            return tk, value\n\n        return dict(map(transform, map(FoldedCase, self)))\n", 89], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\_common.py": ["import os\nimport pathlib\nimport tempfile\nimport functools\nimport contextlib\nimport types\nimport importlib\nimport inspect\nimport warnings\nimport itertools\n\nfrom typing import Union, Optional, cast\nfrom .abc import ResourceReader, Traversable\n\nfrom ._adapters import wrap_spec\n\nPackage = Union[types.ModuleType, str]\nAnchor = Package\n\n\ndef package_to_anchor(func):\n    \"\"\"\n    Replace 'package' parameter as 'anchor' and warn about the change.\n\n    Other errors should fall through.\n\n    >>> files('a', 'b')\n    Traceback (most recent call last):\n    TypeError: files() takes from 0 to 1 positional arguments but 2 were given\n    \"\"\"\n    undefined = object()\n\n    @functools.wraps(func)\n    def wrapper(anchor=undefined, package=undefined):\n        if package is not undefined:\n            if anchor is not undefined:\n                return func(anchor, package)\n            warnings.warn(\n                \"First parameter to files is renamed to 'anchor'\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            return func(package)\n        elif anchor is undefined:\n            return func()\n        return func(anchor)\n\n    return wrapper\n\n\n@package_to_anchor\ndef files(anchor: Optional[Anchor] = None) -> Traversable:\n    \"\"\"\n    Get a Traversable resource for an anchor.\n    \"\"\"\n    return from_package(resolve(anchor))\n\n\ndef get_resource_reader(package: types.ModuleType) -> Optional[ResourceReader]:\n    \"\"\"\n    Return the package's loader if it's a ResourceReader.\n    \"\"\"\n    # We can't use\n    # a issubclass() check here because apparently abc.'s __subclasscheck__()\n    # hook wants to create a weak reference to the object, but\n    # zipimport.zipimporter does not support weak references, resulting in a\n    # TypeError.  That seems terrible.\n    spec = package.__spec__\n    reader = getattr(spec.loader, 'get_resource_reader', None)  # type: ignore\n    if reader is None:\n        return None\n    return reader(spec.name)  # type: ignore\n\n\n@functools.singledispatch\ndef resolve(cand: Optional[Anchor]) -> types.ModuleType:\n    return cast(types.ModuleType, cand)\n\n\n@resolve.register\ndef _(cand: str) -> types.ModuleType:\n    return importlib.import_module(cand)\n\n\n@resolve.register\ndef _(cand: None) -> types.ModuleType:\n    return resolve(_infer_caller().f_globals['__name__'])\n\n\ndef _infer_caller():\n    \"\"\"\n    Walk the stack and find the frame of the first caller not in this module.\n    \"\"\"\n\n    def is_this_file(frame_info):\n        return frame_info.filename == stack[0].filename\n\n    def is_wrapper(frame_info):\n        return frame_info.function == 'wrapper'\n\n    stack = inspect.stack()\n    not_this_file = itertools.filterfalse(is_this_file, stack)\n    # also exclude 'wrapper' due to singledispatch in the call stack\n    callers = itertools.filterfalse(is_wrapper, not_this_file)\n    return next(callers).frame\n\n\ndef from_package(package: types.ModuleType):\n    \"\"\"\n    Return a Traversable object for the given package.\n\n    \"\"\"\n    spec = wrap_spec(package)\n    reader = spec.loader.get_resource_reader(spec.name)\n    return reader.files()\n\n\n@contextlib.contextmanager\ndef _tempfile(\n    reader,\n    suffix='',\n    # gh-93353: Keep a reference to call os.remove() in late Python\n    # finalization.\n    *,\n    _os_remove=os.remove,\n):\n    # Not using tempfile.NamedTemporaryFile as it leads to deeper 'try'\n    # blocks due to the need to close the temporary file to work on Windows\n    # properly.\n    fd, raw_path = tempfile.mkstemp(suffix=suffix)\n    try:\n        try:\n            os.write(fd, reader())\n        finally:\n            os.close(fd)\n        del reader\n        yield pathlib.Path(raw_path)\n    finally:\n        try:\n            _os_remove(raw_path)\n        except FileNotFoundError:\n            pass\n\n\ndef _temp_file(path):\n    return _tempfile(path.read_bytes, suffix=path.name)\n\n\ndef _is_present_dir(path: Traversable) -> bool:\n    \"\"\"\n    Some Traversables implement ``is_dir()`` to raise an\n    exception (i.e. ``FileNotFoundError``) when the\n    directory doesn't exist. This function wraps that call\n    to always return a boolean and only return True\n    if there's a dir and it exists.\n    \"\"\"\n    with contextlib.suppress(FileNotFoundError):\n        return path.is_dir()\n    return False\n\n\n@functools.singledispatch\ndef as_file(path):\n    \"\"\"\n    Given a Traversable object, return that object as a\n    path on the local file system in a context manager.\n    \"\"\"\n    return _temp_dir(path) if _is_present_dir(path) else _temp_file(path)\n\n\n@as_file.register(pathlib.Path)\n@contextlib.contextmanager\ndef _(path):\n    \"\"\"\n    Degenerate behavior for pathlib.Path objects.\n    \"\"\"\n    yield path\n\n\n@contextlib.contextmanager\ndef _temp_path(dir: tempfile.TemporaryDirectory):\n    \"\"\"\n    Wrap tempfile.TemporyDirectory to return a pathlib object.\n    \"\"\"\n    with dir as result:\n        yield pathlib.Path(result)\n\n\n@contextlib.contextmanager\ndef _temp_dir(path):\n    \"\"\"\n    Given a traversable dir, recursively replicate the whole tree\n    to the file system in a context manager.\n    \"\"\"\n    assert path.is_dir()\n    with _temp_path(tempfile.TemporaryDirectory()) as temp_dir:\n        yield _write_contents(temp_dir, path)\n\n\ndef _write_contents(target, source):\n    child = target.joinpath(source.name)\n    if source.is_dir():\n        child.mkdir()\n        for item in source.iterdir():\n            _write_contents(child, item)\n    else:\n        child.write_bytes(source.read_bytes())\n    return child\n", 208], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\__init__.py": ["\"\"\"Read resources contained within a package.\"\"\"\n\nfrom ._common import (\n    as_file,\n    files,\n    Package,\n    Anchor,\n)\n\nfrom ._legacy import (\n    contents,\n    open_binary,\n    read_binary,\n    open_text,\n    read_text,\n    is_resource,\n    path,\n    Resource,\n)\n\nfrom .abc import ResourceReader\n\n\n__all__ = [\n    'Package',\n    'Anchor',\n    'Resource',\n    'ResourceReader',\n    'as_file',\n    'contents',\n    'files',\n    'is_resource',\n    'open_binary',\n    'open_text',\n    'path',\n    'read_binary',\n    'read_text',\n]\n", 38], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\abc.py": ["\"\"\"Abstract base classes related to import.\"\"\"\nfrom . import _bootstrap_external\nfrom . import machinery\ntry:\n    import _frozen_importlib\nexcept ImportError as exc:\n    if exc.name != '_frozen_importlib':\n        raise\n    _frozen_importlib = None\ntry:\n    import _frozen_importlib_external\nexcept ImportError:\n    _frozen_importlib_external = _bootstrap_external\nfrom ._abc import Loader\nimport abc\nimport warnings\n\nfrom .resources import abc as _resources_abc\n\n\n__all__ = [\n    'Loader', 'MetaPathFinder', 'PathEntryFinder',\n    'ResourceLoader', 'InspectLoader', 'ExecutionLoader',\n    'FileLoader', 'SourceLoader',\n]\n\n\ndef __getattr__(name):\n    \"\"\"\n    For backwards compatibility, continue to make names\n    from _resources_abc available through this module. #93963\n    \"\"\"\n    if name in _resources_abc.__all__:\n        obj = getattr(_resources_abc, name)\n        warnings._deprecated(f\"{__name__}.{name}\", remove=(3, 14))\n        globals()[name] = obj\n        return obj\n    raise AttributeError(f'module {__name__!r} has no attribute {name!r}')\n\n\ndef _register(abstract_cls, *classes):\n    for cls in classes:\n        abstract_cls.register(cls)\n        if _frozen_importlib is not None:\n            try:\n                frozen_cls = getattr(_frozen_importlib, cls.__name__)\n            except AttributeError:\n                frozen_cls = getattr(_frozen_importlib_external, cls.__name__)\n            abstract_cls.register(frozen_cls)\n\n\nclass MetaPathFinder(metaclass=abc.ABCMeta):\n\n    \"\"\"Abstract base class for import finders on sys.meta_path.\"\"\"\n\n    # We don't define find_spec() here since that would break\n    # hasattr checks we do to support backward compatibility.\n\n    def invalidate_caches(self):\n        \"\"\"An optional method for clearing the finder's cache, if any.\n        This method is used by importlib.invalidate_caches().\n        \"\"\"\n\n_register(MetaPathFinder, machinery.BuiltinImporter, machinery.FrozenImporter,\n          machinery.PathFinder, machinery.WindowsRegistryFinder)\n\n\nclass PathEntryFinder(metaclass=abc.ABCMeta):\n\n    \"\"\"Abstract base class for path entry finders used by PathFinder.\"\"\"\n\n    def invalidate_caches(self):\n        \"\"\"An optional method for clearing the finder's cache, if any.\n        This method is used by PathFinder.invalidate_caches().\n        \"\"\"\n\n_register(PathEntryFinder, machinery.FileFinder)\n\n\nclass ResourceLoader(Loader):\n\n    \"\"\"Abstract base class for loaders which can return data from their\n    back-end storage.\n\n    This ABC represents one of the optional protocols specified by PEP 302.\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_data(self, path):\n        \"\"\"Abstract method which when implemented should return the bytes for\n        the specified path.  The path must be a str.\"\"\"\n        raise OSError\n\n\nclass InspectLoader(Loader):\n\n    \"\"\"Abstract base class for loaders which support inspection about the\n    modules they can load.\n\n    This ABC represents one of the optional protocols specified by PEP 302.\n\n    \"\"\"\n\n    def is_package(self, fullname):\n        \"\"\"Optional method which when implemented should return whether the\n        module is a package.  The fullname is a str.  Returns a bool.\n\n        Raises ImportError if the module cannot be found.\n        \"\"\"\n        raise ImportError\n\n    def get_code(self, fullname):\n        \"\"\"Method which returns the code object for the module.\n\n        The fullname is a str.  Returns a types.CodeType if possible, else\n        returns None if a code object does not make sense\n        (e.g. built-in module). Raises ImportError if the module cannot be\n        found.\n        \"\"\"\n        source = self.get_source(fullname)\n        if source is None:\n            return None\n        return self.source_to_code(source)\n\n    @abc.abstractmethod\n    def get_source(self, fullname):\n        \"\"\"Abstract method which should return the source code for the\n        module.  The fullname is a str.  Returns a str.\n\n        Raises ImportError if the module cannot be found.\n        \"\"\"\n        raise ImportError\n\n    @staticmethod\n    def source_to_code(data, path='<string>'):\n        \"\"\"Compile 'data' into a code object.\n\n        The 'data' argument can be anything that compile() can handle. The'path'\n        argument should be where the data was retrieved (when applicable).\"\"\"\n        return compile(data, path, 'exec', dont_inherit=True)\n\n    exec_module = _bootstrap_external._LoaderBasics.exec_module\n    load_module = _bootstrap_external._LoaderBasics.load_module\n\n_register(InspectLoader, machinery.BuiltinImporter, machinery.FrozenImporter, machinery.NamespaceLoader)\n\n\nclass ExecutionLoader(InspectLoader):\n\n    \"\"\"Abstract base class for loaders that wish to support the execution of\n    modules as scripts.\n\n    This ABC represents one of the optional protocols specified in PEP 302.\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_filename(self, fullname):\n        \"\"\"Abstract method which should return the value that __file__ is to be\n        set to.\n\n        Raises ImportError if the module cannot be found.\n        \"\"\"\n        raise ImportError\n\n    def get_code(self, fullname):\n        \"\"\"Method to return the code object for fullname.\n\n        Should return None if not applicable (e.g. built-in module).\n        Raise ImportError if the module cannot be found.\n        \"\"\"\n        source = self.get_source(fullname)\n        if source is None:\n            return None\n        try:\n            path = self.get_filename(fullname)\n        except ImportError:\n            return self.source_to_code(source)\n        else:\n            return self.source_to_code(source, path)\n\n_register(ExecutionLoader, machinery.ExtensionFileLoader)\n\n\nclass FileLoader(_bootstrap_external.FileLoader, ResourceLoader, ExecutionLoader):\n\n    \"\"\"Abstract base class partially implementing the ResourceLoader and\n    ExecutionLoader ABCs.\"\"\"\n\n_register(FileLoader, machinery.SourceFileLoader,\n            machinery.SourcelessFileLoader)\n\n\nclass SourceLoader(_bootstrap_external.SourceLoader, ResourceLoader, ExecutionLoader):\n\n    \"\"\"Abstract base class for loading source code (and optionally any\n    corresponding bytecode).\n\n    To support loading from source code, the abstractmethods inherited from\n    ResourceLoader and ExecutionLoader need to be implemented. To also support\n    loading from bytecode, the optional methods specified directly by this ABC\n    is required.\n\n    Inherited abstractmethods not implemented in this ABC:\n\n        * ResourceLoader.get_data\n        * ExecutionLoader.get_filename\n\n    \"\"\"\n\n    def path_mtime(self, path):\n        \"\"\"Return the (int) modification time for the path (str).\"\"\"\n        if self.path_stats.__func__ is SourceLoader.path_stats:\n            raise OSError\n        return int(self.path_stats(path)['mtime'])\n\n    def path_stats(self, path):\n        \"\"\"Return a metadata dict for the source pointed to by the path (str).\n        Possible keys:\n        - 'mtime' (mandatory) is the numeric timestamp of last source\n          code modification;\n        - 'size' (optional) is the size in bytes of the source code.\n        \"\"\"\n        if self.path_mtime.__func__ is SourceLoader.path_mtime:\n            raise OSError\n        return {'mtime': self.path_mtime(path)}\n\n    def set_data(self, path, data):\n        \"\"\"Write the bytes to the path (if possible).\n\n        Accepts a str path and data as bytes.\n\n        Any needed intermediary directories are to be created. If for some\n        reason the file cannot be written because of permissions, fail\n        silently.\n        \"\"\"\n\n_register(SourceLoader, machinery.SourceFileLoader)\n", 239], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py": ["import os\nimport re\nimport abc\nimport csv\nimport sys\nimport email\nimport pathlib\nimport zipfile\nimport operator\nimport textwrap\nimport warnings\nimport functools\nimport itertools\nimport posixpath\nimport contextlib\nimport collections\nimport inspect\n\nfrom . import _adapters, _meta\nfrom ._collections import FreezableDefaultDict, Pair\nfrom ._functools import method_cache, pass_none\nfrom ._itertools import always_iterable, unique_everseen\nfrom ._meta import PackageMetadata, SimplePath\n\nfrom contextlib import suppress\nfrom importlib import import_module\nfrom importlib.abc import MetaPathFinder\nfrom itertools import starmap\nfrom typing import List, Mapping, Optional, cast\n\n\n__all__ = [\n    'Distribution',\n    'DistributionFinder',\n    'PackageMetadata',\n    'PackageNotFoundError',\n    'distribution',\n    'distributions',\n    'entry_points',\n    'files',\n    'metadata',\n    'packages_distributions',\n    'requires',\n    'version',\n]\n\n\nclass PackageNotFoundError(ModuleNotFoundError):\n    \"\"\"The package was not found.\"\"\"\n\n    def __str__(self):\n        return f\"No package metadata was found for {self.name}\"\n\n    @property\n    def name(self):\n        (name,) = self.args\n        return name\n\n\nclass Sectioned:\n    \"\"\"\n    A simple entry point config parser for performance\n\n    >>> for item in Sectioned.read(Sectioned._sample):\n    ...     print(item)\n    Pair(name='sec1', value='# comments ignored')\n    Pair(name='sec1', value='a = 1')\n    Pair(name='sec1', value='b = 2')\n    Pair(name='sec2', value='a = 2')\n\n    >>> res = Sectioned.section_pairs(Sectioned._sample)\n    >>> item = next(res)\n    >>> item.name\n    'sec1'\n    >>> item.value\n    Pair(name='a', value='1')\n    >>> item = next(res)\n    >>> item.value\n    Pair(name='b', value='2')\n    >>> item = next(res)\n    >>> item.name\n    'sec2'\n    >>> item.value\n    Pair(name='a', value='2')\n    >>> list(res)\n    []\n    \"\"\"\n\n    _sample = textwrap.dedent(\n        \"\"\"\n        [sec1]\n        # comments ignored\n        a = 1\n        b = 2\n\n        [sec2]\n        a = 2\n        \"\"\"\n    ).lstrip()\n\n    @classmethod\n    def section_pairs(cls, text):\n        return (\n            section._replace(value=Pair.parse(section.value))\n            for section in cls.read(text, filter_=cls.valid)\n            if section.name is not None\n        )\n\n    @staticmethod\n    def read(text, filter_=None):\n        lines = filter(filter_, map(str.strip, text.splitlines()))\n        name = None\n        for value in lines:\n            section_match = value.startswith('[') and value.endswith(']')\n            if section_match:\n                name = value.strip('[]')\n                continue\n            yield Pair(name, value)\n\n    @staticmethod\n    def valid(line):\n        return line and not line.startswith('#')\n\n\nclass DeprecatedTuple:\n    \"\"\"\n    Provide subscript item access for backward compatibility.\n\n    >>> recwarn = getfixture('recwarn')\n    >>> ep = EntryPoint(name='name', value='value', group='group')\n    >>> ep[:]\n    ('name', 'value', 'group')\n    >>> ep[0]\n    'name'\n    >>> len(recwarn)\n    1\n    \"\"\"\n\n    # Do not remove prior to 2023-05-01 or Python 3.13\n    _warn = functools.partial(\n        warnings.warn,\n        \"EntryPoint tuple interface is deprecated. Access members by name.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    def __getitem__(self, item):\n        self._warn()\n        return self._key()[item]\n\n\nclass EntryPoint(DeprecatedTuple):\n    \"\"\"An entry point as defined by Python packaging conventions.\n\n    See `the packaging docs on entry points\n    <https://packaging.python.org/specifications/entry-points/>`_\n    for more information.\n\n    >>> ep = EntryPoint(\n    ...     name=None, group=None, value='package.module:attr [extra1, extra2]')\n    >>> ep.module\n    'package.module'\n    >>> ep.attr\n    'attr'\n    >>> ep.extras\n    ['extra1', 'extra2']\n    \"\"\"\n\n    pattern = re.compile(\n        r'(?P<module>[\\w.]+)\\s*'\n        r'(:\\s*(?P<attr>[\\w.]+)\\s*)?'\n        r'((?P<extras>\\[.*\\])\\s*)?$'\n    )\n    \"\"\"\n    A regular expression describing the syntax for an entry point,\n    which might look like:\n\n        - module\n        - package.module\n        - package.module:attribute\n        - package.module:object.attribute\n        - package.module:attr [extra1, extra2]\n\n    Other combinations are possible as well.\n\n    The expression is lenient about whitespace around the ':',\n    following the attr, and following any extras.\n    \"\"\"\n\n    name: str\n    value: str\n    group: str\n\n    dist: Optional['Distribution'] = None\n\n    def __init__(self, name, value, group):\n        vars(self).update(name=name, value=value, group=group)\n\n    def load(self):\n        \"\"\"Load the entry point from its definition. If only a module\n        is indicated by the value, return that module. Otherwise,\n        return the named object.\n        \"\"\"\n        match = self.pattern.match(self.value)\n        module = import_module(match.group('module'))\n        attrs = filter(None, (match.group('attr') or '').split('.'))\n        return functools.reduce(getattr, attrs, module)\n\n    @property\n    def module(self):\n        match = self.pattern.match(self.value)\n        return match.group('module')\n\n    @property\n    def attr(self):\n        match = self.pattern.match(self.value)\n        return match.group('attr')\n\n    @property\n    def extras(self):\n        match = self.pattern.match(self.value)\n        return re.findall(r'\\w+', match.group('extras') or '')\n\n    def _for(self, dist):\n        vars(self).update(dist=dist)\n        return self\n\n    def matches(self, **params):\n        \"\"\"\n        EntryPoint matches the given parameters.\n\n        >>> ep = EntryPoint(group='foo', name='bar', value='bing:bong [extra1, extra2]')\n        >>> ep.matches(group='foo')\n        True\n        >>> ep.matches(name='bar', value='bing:bong [extra1, extra2]')\n        True\n        >>> ep.matches(group='foo', name='other')\n        False\n        >>> ep.matches()\n        True\n        >>> ep.matches(extras=['extra1', 'extra2'])\n        True\n        >>> ep.matches(module='bing')\n        True\n        >>> ep.matches(attr='bong')\n        True\n        \"\"\"\n        attrs = (getattr(self, param) for param in params)\n        return all(map(operator.eq, params.values(), attrs))\n\n    def _key(self):\n        return self.name, self.value, self.group\n\n    def __lt__(self, other):\n        return self._key() < other._key()\n\n    def __eq__(self, other):\n        return self._key() == other._key()\n\n    def __setattr__(self, name, value):\n        raise AttributeError(\"EntryPoint objects are immutable.\")\n\n    def __repr__(self):\n        return (\n            f'EntryPoint(name={self.name!r}, value={self.value!r}, '\n            f'group={self.group!r})'\n        )\n\n    def __hash__(self):\n        return hash(self._key())\n\n\nclass EntryPoints(tuple):\n    \"\"\"\n    An immutable collection of selectable EntryPoint objects.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __getitem__(self, name):  # -> EntryPoint:\n        \"\"\"\n        Get the EntryPoint in self matching name.\n        \"\"\"\n        try:\n            return next(iter(self.select(name=name)))\n        except StopIteration:\n            raise KeyError(name)\n\n    def select(self, **params):\n        \"\"\"\n        Select entry points from self that match the\n        given parameters (typically group and/or name).\n        \"\"\"\n        return EntryPoints(ep for ep in self if ep.matches(**params))\n\n    @property\n    def names(self):\n        \"\"\"\n        Return the set of all names of all entry points.\n        \"\"\"\n        return {ep.name for ep in self}\n\n    @property\n    def groups(self):\n        \"\"\"\n        Return the set of all groups of all entry points.\n        \"\"\"\n        return {ep.group for ep in self}\n\n    @classmethod\n    def _from_text_for(cls, text, dist):\n        return cls(ep._for(dist) for ep in cls._from_text(text))\n\n    @staticmethod\n    def _from_text(text):\n        return (\n            EntryPoint(name=item.value.name, value=item.value.value, group=item.name)\n            for item in Sectioned.section_pairs(text or '')\n        )\n\n\nclass PackagePath(pathlib.PurePosixPath):\n    \"\"\"A reference to a path in a package\"\"\"\n\n    def read_text(self, encoding='utf-8'):\n        with self.locate().open(encoding=encoding) as stream:\n            return stream.read()\n\n    def read_binary(self):\n        with self.locate().open('rb') as stream:\n            return stream.read()\n\n    def locate(self):\n        \"\"\"Return a path-like object for this path\"\"\"\n        return self.dist.locate_file(self)\n\n\nclass FileHash:\n    def __init__(self, spec):\n        self.mode, _, self.value = spec.partition('=')\n\n    def __repr__(self):\n        return f'<FileHash mode: {self.mode} value: {self.value}>'\n\n\nclass DeprecatedNonAbstract:\n    def __new__(cls, *args, **kwargs):\n        all_names = {\n            name for subclass in inspect.getmro(cls) for name in vars(subclass)\n        }\n        abstract = {\n            name\n            for name in all_names\n            if getattr(getattr(cls, name), '__isabstractmethod__', False)\n        }\n        if abstract:\n            warnings.warn(\n                f\"Unimplemented abstract methods {abstract}\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__new__(cls)\n\n\nclass Distribution(DeprecatedNonAbstract):\n    \"\"\"A Python distribution package.\"\"\"\n\n    @abc.abstractmethod\n    def read_text(self, filename) -> Optional[str]:\n        \"\"\"Attempt to load metadata file given by the name.\n\n        :param filename: The name of the file in the distribution info.\n        :return: The text if found, otherwise None.\n        \"\"\"\n\n    @abc.abstractmethod\n    def locate_file(self, path):\n        \"\"\"\n        Given a path to a file in this distribution, return a path\n        to it.\n        \"\"\"\n\n    @classmethod\n    def from_name(cls, name: str):\n        \"\"\"Return the Distribution for the given package name.\n\n        :param name: The name of the distribution package to search for.\n        :return: The Distribution instance (or subclass thereof) for the named\n            package, if found.\n        :raises PackageNotFoundError: When the named package's distribution\n            metadata cannot be found.\n        :raises ValueError: When an invalid value is supplied for name.\n        \"\"\"\n        if not name:\n            raise ValueError(\"A distribution name is required.\")\n        try:\n            return next(cls.discover(name=name))\n        except StopIteration:\n            raise PackageNotFoundError(name)\n\n    @classmethod\n    def discover(cls, **kwargs):\n        \"\"\"Return an iterable of Distribution objects for all packages.\n\n        Pass a ``context`` or pass keyword arguments for constructing\n        a context.\n\n        :context: A ``DistributionFinder.Context`` object.\n        :return: Iterable of Distribution objects for all packages.\n        \"\"\"\n        context = kwargs.pop('context', None)\n        if context and kwargs:\n            raise ValueError(\"cannot accept context and kwargs\")\n        context = context or DistributionFinder.Context(**kwargs)\n        return itertools.chain.from_iterable(\n            resolver(context) for resolver in cls._discover_resolvers()\n        )\n\n    @staticmethod\n    def at(path):\n        \"\"\"Return a Distribution for the indicated metadata path\n\n        :param path: a string or path-like object\n        :return: a concrete Distribution instance for the path\n        \"\"\"\n        return PathDistribution(pathlib.Path(path))\n\n    @staticmethod\n    def _discover_resolvers():\n        \"\"\"Search the meta_path for resolvers.\"\"\"\n        declared = (\n            getattr(finder, 'find_distributions', None) for finder in sys.meta_path\n        )\n        return filter(None, declared)\n\n    @property\n    def metadata(self) -> _meta.PackageMetadata:\n        \"\"\"Return the parsed metadata for this Distribution.\n\n        The returned object will have keys that name the various bits of\n        metadata.  See PEP 566 for details.\n        \"\"\"\n        opt_text = (\n            self.read_text('METADATA')\n            or self.read_text('PKG-INFO')\n            # This last clause is here to support old egg-info files.  Its\n            # effect is to just end up using the PathDistribution's self._path\n            # (which points to the egg-info file) attribute unchanged.\n            or self.read_text('')\n        )\n        text = cast(str, opt_text)\n        return _adapters.Message(email.message_from_string(text))\n\n    @property\n    def name(self):\n        \"\"\"Return the 'Name' metadata for the distribution package.\"\"\"\n        return self.metadata['Name']\n\n    @property\n    def _normalized_name(self):\n        \"\"\"Return a normalized version of the name.\"\"\"\n        return Prepared.normalize(self.name)\n\n    @property\n    def version(self):\n        \"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\n        return self.metadata['Version']\n\n    @property\n    def entry_points(self):\n        return EntryPoints._from_text_for(self.read_text('entry_points.txt'), self)\n\n    @property\n    def files(self):\n        \"\"\"Files in this distribution.\n\n        :return: List of PackagePath for this distribution or None\n\n        Result is `None` if the metadata file that enumerates files\n        (i.e. RECORD for dist-info, or installed-files.txt or\n        SOURCES.txt for egg-info) is missing.\n        Result may be empty if the metadata exists but is empty.\n        \"\"\"\n\n        def make_file(name, hash=None, size_str=None):\n            result = PackagePath(name)\n            result.hash = FileHash(hash) if hash else None\n            result.size = int(size_str) if size_str else None\n            result.dist = self\n            return result\n\n        @pass_none\n        def make_files(lines):\n            return starmap(make_file, csv.reader(lines))\n\n        @pass_none\n        def skip_missing_files(package_paths):\n            return list(filter(lambda path: path.locate().exists(), package_paths))\n\n        return skip_missing_files(\n            make_files(\n                self._read_files_distinfo()\n                or self._read_files_egginfo_installed()\n                or self._read_files_egginfo_sources()\n            )\n        )\n\n    def _read_files_distinfo(self):\n        \"\"\"\n        Read the lines of RECORD\n        \"\"\"\n        text = self.read_text('RECORD')\n        return text and text.splitlines()\n\n    def _read_files_egginfo_installed(self):\n        \"\"\"\n        Read installed-files.txt and return lines in a similar\n        CSV-parsable format as RECORD: each file must be placed\n        relative to the site-packages directory and must also be\n        quoted (since file names can contain literal commas).\n\n        This file is written when the package is installed by pip,\n        but it might not be written for other installation methods.\n        Assume the file is accurate if it exists.\n        \"\"\"\n        text = self.read_text('installed-files.txt')\n        # Prepend the .egg-info/ subdir to the lines in this file.\n        # But this subdir is only available from PathDistribution's\n        # self._path.\n        subdir = getattr(self, '_path', None)\n        if not text or not subdir:\n            return\n\n        paths = (\n            (subdir / name)\n            .resolve()\n            .relative_to(self.locate_file('').resolve(), walk_up=True)\n            .as_posix()\n            for name in text.splitlines()\n        )\n        return map('\"{}\"'.format, paths)\n\n    def _read_files_egginfo_sources(self):\n        \"\"\"\n        Read SOURCES.txt and return lines in a similar CSV-parsable\n        format as RECORD: each file name must be quoted (since it\n        might contain literal commas).\n\n        Note that SOURCES.txt is not a reliable source for what\n        files are installed by a package. This file is generated\n        for a source archive, and the files that are present\n        there (e.g. setup.py) may not correctly reflect the files\n        that are present after the package has been installed.\n        \"\"\"\n        text = self.read_text('SOURCES.txt')\n        return text and map('\"{}\"'.format, text.splitlines())\n\n    @property\n    def requires(self):\n        \"\"\"Generated requirements specified for this Distribution\"\"\"\n        reqs = self._read_dist_info_reqs() or self._read_egg_info_reqs()\n        return reqs and list(reqs)\n\n    def _read_dist_info_reqs(self):\n        return self.metadata.get_all('Requires-Dist')\n\n    def _read_egg_info_reqs(self):\n        source = self.read_text('requires.txt')\n        return pass_none(self._deps_from_requires_text)(source)\n\n    @classmethod\n    def _deps_from_requires_text(cls, source):\n        return cls._convert_egg_info_reqs_to_simple_reqs(Sectioned.read(source))\n\n    @staticmethod\n    def _convert_egg_info_reqs_to_simple_reqs(sections):\n        \"\"\"\n        Historically, setuptools would solicit and store 'extra'\n        requirements, including those with environment markers,\n        in separate sections. More modern tools expect each\n        dependency to be defined separately, with any relevant\n        extras and environment markers attached directly to that\n        requirement. This method converts the former to the\n        latter. See _test_deps_from_requires_text for an example.\n        \"\"\"\n\n        def make_condition(name):\n            return name and f'extra == \"{name}\"'\n\n        def quoted_marker(section):\n            section = section or ''\n            extra, sep, markers = section.partition(':')\n            if extra and markers:\n                markers = f'({markers})'\n            conditions = list(filter(None, [markers, make_condition(extra)]))\n            return '; ' + ' and '.join(conditions) if conditions else ''\n\n        def url_req_space(req):\n            \"\"\"\n            PEP 508 requires a space between the url_spec and the quoted_marker.\n            Ref python/importlib_metadata#357.\n            \"\"\"\n            # '@' is uniquely indicative of a url_req.\n            return ' ' * ('@' in req)\n\n        for section in sections:\n            space = url_req_space(section.value)\n            yield section.value + space + quoted_marker(section.name)\n\n\nclass DistributionFinder(MetaPathFinder):\n    \"\"\"\n    A MetaPathFinder capable of discovering installed distributions.\n    \"\"\"\n\n    class Context:\n        \"\"\"\n        Keyword arguments presented by the caller to\n        ``distributions()`` or ``Distribution.discover()``\n        to narrow the scope of a search for distributions\n        in all DistributionFinders.\n\n        Each DistributionFinder may expect any parameters\n        and should attempt to honor the canonical\n        parameters defined below when appropriate.\n        \"\"\"\n\n        name = None\n        \"\"\"\n        Specific name for which a distribution finder should match.\n        A name of ``None`` matches all distributions.\n        \"\"\"\n\n        def __init__(self, **kwargs):\n            vars(self).update(kwargs)\n\n        @property\n        def path(self):\n            \"\"\"\n            The sequence of directory path that a distribution finder\n            should search.\n\n            Typically refers to Python installed package paths such as\n            \"site-packages\" directories and defaults to ``sys.path``.\n            \"\"\"\n            return vars(self).get('path', sys.path)\n\n    @abc.abstractmethod\n    def find_distributions(self, context=Context()):\n        \"\"\"\n        Find distributions.\n\n        Return an iterable of all Distribution instances capable of\n        loading the metadata for packages matching the ``context``,\n        a DistributionFinder.Context instance.\n        \"\"\"\n\n\nclass FastPath:\n    \"\"\"\n    Micro-optimized class for searching a path for\n    children.\n\n    >>> FastPath('').children()\n    ['...']\n    \"\"\"\n\n    @functools.lru_cache()  # type: ignore\n    def __new__(cls, root):\n        return super().__new__(cls)\n\n    def __init__(self, root):\n        self.root = root\n\n    def joinpath(self, child):\n        return pathlib.Path(self.root, child)\n\n    def children(self):\n        with suppress(Exception):\n            return os.listdir(self.root or '.')\n        with suppress(Exception):\n            return self.zip_children()\n        return []\n\n    def zip_children(self):\n        zip_path = zipfile.Path(self.root)\n        names = zip_path.root.namelist()\n        self.joinpath = zip_path.joinpath\n\n        return dict.fromkeys(child.split(posixpath.sep, 1)[0] for child in names)\n\n    def search(self, name):\n        return self.lookup(self.mtime).search(name)\n\n    @property\n    def mtime(self):\n        with suppress(OSError):\n            return os.stat(self.root).st_mtime\n        self.lookup.cache_clear()\n\n    @method_cache\n    def lookup(self, mtime):\n        return Lookup(self)\n\n\nclass Lookup:\n    def __init__(self, path: FastPath):\n        base = os.path.basename(path.root).lower()\n        base_is_egg = base.endswith(\".egg\")\n        self.infos = FreezableDefaultDict(list)\n        self.eggs = FreezableDefaultDict(list)\n\n        for child in path.children():\n            low = child.lower()\n            if low.endswith((\".dist-info\", \".egg-info\")):\n                # rpartition is faster than splitext and suitable for this purpose.\n                name = low.rpartition(\".\")[0].partition(\"-\")[0]\n                normalized = Prepared.normalize(name)\n                self.infos[normalized].append(path.joinpath(child))\n            elif base_is_egg and low == \"egg-info\":\n                name = base.rpartition(\".\")[0].partition(\"-\")[0]\n                legacy_normalized = Prepared.legacy_normalize(name)\n                self.eggs[legacy_normalized].append(path.joinpath(child))\n\n        self.infos.freeze()\n        self.eggs.freeze()\n\n    def search(self, prepared):\n        infos = (\n            self.infos[prepared.normalized]\n            if prepared\n            else itertools.chain.from_iterable(self.infos.values())\n        )\n        eggs = (\n            self.eggs[prepared.legacy_normalized]\n            if prepared\n            else itertools.chain.from_iterable(self.eggs.values())\n        )\n        return itertools.chain(infos, eggs)\n\n\nclass Prepared:\n    \"\"\"\n    A prepared search for metadata on a possibly-named package.\n    \"\"\"\n\n    normalized = None\n    legacy_normalized = None\n\n    def __init__(self, name):\n        self.name = name\n        if name is None:\n            return\n        self.normalized = self.normalize(name)\n        self.legacy_normalized = self.legacy_normalize(name)\n\n    @staticmethod\n    def normalize(name):\n        \"\"\"\n        PEP 503 normalization plus dashes as underscores.\n        \"\"\"\n        return re.sub(r\"[-_.]+\", \"-\", name).lower().replace('-', '_')\n\n    @staticmethod\n    def legacy_normalize(name):\n        \"\"\"\n        Normalize the package name as found in the convention in\n        older packaging tools versions and specs.\n        \"\"\"\n        return name.lower().replace('-', '_')\n\n    def __bool__(self):\n        return bool(self.name)\n\n\nclass MetadataPathFinder(DistributionFinder):\n    @classmethod\n    def find_distributions(cls, context=DistributionFinder.Context()):\n        \"\"\"\n        Find distributions.\n\n        Return an iterable of all Distribution instances capable of\n        loading the metadata for packages matching ``context.name``\n        (or all names if ``None`` indicated) along the paths in the list\n        of directories ``context.path``.\n        \"\"\"\n        found = cls._search_paths(context.name, context.path)\n        return map(PathDistribution, found)\n\n    @classmethod\n    def _search_paths(cls, name, paths):\n        \"\"\"Find metadata directories in paths heuristically.\"\"\"\n        prepared = Prepared(name)\n        return itertools.chain.from_iterable(\n            path.search(prepared) for path in map(FastPath, paths)\n        )\n\n    @classmethod\n    def invalidate_caches(cls):\n        FastPath.__new__.cache_clear()\n\n\nclass PathDistribution(Distribution):\n    def __init__(self, path: SimplePath):\n        \"\"\"Construct a distribution.\n\n        :param path: SimplePath indicating the metadata directory.\n        \"\"\"\n        self._path = path\n\n    def read_text(self, filename):\n        with suppress(\n            FileNotFoundError,\n            IsADirectoryError,\n            KeyError,\n            NotADirectoryError,\n            PermissionError,\n        ):\n            return self._path.joinpath(filename).read_text(encoding='utf-8')\n\n    read_text.__doc__ = Distribution.read_text.__doc__\n\n    def locate_file(self, path):\n        return self._path.parent / path\n\n    @property\n    def _normalized_name(self):\n        \"\"\"\n        Performance optimization: where possible, resolve the\n        normalized name from the file system path.\n        \"\"\"\n        stem = os.path.basename(str(self._path))\n        return (\n            pass_none(Prepared.normalize)(self._name_from_stem(stem))\n            or super()._normalized_name\n        )\n\n    @staticmethod\n    def _name_from_stem(stem):\n        \"\"\"\n        >>> PathDistribution._name_from_stem('foo-3.0.egg-info')\n        'foo'\n        >>> PathDistribution._name_from_stem('CherryPy-3.0.dist-info')\n        'CherryPy'\n        >>> PathDistribution._name_from_stem('face.egg-info')\n        'face'\n        >>> PathDistribution._name_from_stem('foo.bar')\n        \"\"\"\n        filename, ext = os.path.splitext(stem)\n        if ext not in ('.dist-info', '.egg-info'):\n            return\n        name, sep, rest = filename.partition('-')\n        return name\n\n\ndef distribution(distribution_name):\n    \"\"\"Get the ``Distribution`` instance for the named package.\n\n    :param distribution_name: The name of the distribution package as a string.\n    :return: A ``Distribution`` instance (or subclass thereof).\n    \"\"\"\n    return Distribution.from_name(distribution_name)\n\n\ndef distributions(**kwargs):\n    \"\"\"Get all ``Distribution`` instances in the current environment.\n\n    :return: An iterable of ``Distribution`` instances.\n    \"\"\"\n    return Distribution.discover(**kwargs)\n\n\ndef metadata(distribution_name) -> _meta.PackageMetadata:\n    \"\"\"Get the metadata for the named package.\n\n    :param distribution_name: The name of the distribution package to query.\n    :return: A PackageMetadata containing the parsed metadata.\n    \"\"\"\n    return Distribution.from_name(distribution_name).metadata\n\n\ndef version(distribution_name):\n    \"\"\"Get the version string for the named package.\n\n    :param distribution_name: The name of the distribution package to query.\n    :return: The version string for the package as defined in the package's\n        \"Version\" metadata key.\n    \"\"\"\n    return distribution(distribution_name).version\n\n\n_unique = functools.partial(\n    unique_everseen,\n    key=operator.attrgetter('_normalized_name'),\n)\n\"\"\"\nWrapper for ``distributions`` to return unique distributions by name.\n\"\"\"\n\n\ndef entry_points(**params) -> EntryPoints:\n    \"\"\"Return EntryPoint objects for all installed packages.\n\n    Pass selection parameters (group or name) to filter the\n    result to entry points matching those properties (see\n    EntryPoints.select()).\n\n    :return: EntryPoints for all installed packages.\n    \"\"\"\n    eps = itertools.chain.from_iterable(\n        dist.entry_points for dist in _unique(distributions())\n    )\n    return EntryPoints(eps).select(**params)\n\n\ndef files(distribution_name):\n    \"\"\"Return a list of files for the named package.\n\n    :param distribution_name: The name of the distribution package to query.\n    :return: List of files composing the distribution.\n    \"\"\"\n    return distribution(distribution_name).files\n\n\ndef requires(distribution_name):\n    \"\"\"\n    Return a list of requirements for the named package.\n\n    :return: An iterator of requirements, suitable for\n        packaging.requirement.Requirement.\n    \"\"\"\n    return distribution(distribution_name).requires\n\n\ndef packages_distributions() -> Mapping[str, List[str]]:\n    \"\"\"\n    Return a mapping of top-level packages to their\n    distributions.\n\n    >>> import collections.abc\n    >>> pkgs = packages_distributions()\n    >>> all(isinstance(dist, collections.abc.Sequence) for dist in pkgs.values())\n    True\n    \"\"\"\n    pkg_to_dist = collections.defaultdict(list)\n    for dist in distributions():\n        for pkg in _top_level_declared(dist) or _top_level_inferred(dist):\n            pkg_to_dist[pkg].append(dist.metadata['Name'])\n    return dict(pkg_to_dist)\n\n\ndef _top_level_declared(dist):\n    return (dist.read_text('top_level.txt') or '').split()\n\n\ndef _top_level_inferred(dist):\n    opt_names = {\n        f.parts[0] if len(f.parts) > 1 else inspect.getmodulename(f)\n        for f in always_iterable(dist.files)\n    }\n\n    @pass_none\n    def importable_name(name):\n        return '.' not in name\n\n    return filter(importable_name, opt_names)\n", 966], "C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py": ["import heapq\nimport random\nimport time\nfrom dataclasses import dataclass, field\n\n\n@dataclass(frozen=True)\nclass GridConfig:\n    size: int\n    obstacle_ratio: float\n    seed: int\n    start: tuple[int, int] = field(init=False)\n    goal: tuple[int, int] = field(init=False)\n\n    def __post_init__(self) -> None:\n        object.__setattr__(self, \"start\", (0, 0))\n        object.__setattr__(self, \"goal\", (self.size - 1, self.size - 1))\n        if self.size < 10 or self.size > 5000:\n            raise ValueError(\"size must be between 10 and 5000\")\n        if not 0.0 <= self.obstacle_ratio < 0.5:\n            raise ValueError(\"obstacle_ratio must be between 0.0 and 0.5\")\n\n\n@dataclass\nclass ObstacleStore:\n    cells: set[tuple[int, int]]\n\n    @classmethod\n    def from_config(cls, config: GridConfig) -> \"ObstacleStore\":\n        rng = random.Random(config.seed)\n        num_obstacles = int(config.size * config.size * config.obstacle_ratio)\n        obstacles: set[tuple[int, int]] = set()\n\n        while len(obstacles) < num_obstacles:\n            x = rng.randint(0, config.size - 1)\n            y = rng.randint(0, config.size - 1)\n            if (x, y) != config.start and (x, y) != config.goal:\n                obstacles.add((x, y))\n\n        return cls(obstacles)\n\n\ndef to_native_args(\n    config: GridConfig, obstacles: ObstacleStore\n) -> tuple[int, float, int, list[tuple[int, int]]]:\n    return (\n        config.size,\n        config.obstacle_ratio,\n        config.seed,\n        list(obstacles.cells),\n    )\n\n\nclass AStarPathfinder:\n    def __init__(self, size, obstacle_ratio, seed):\n        self.config = GridConfig(size=size, obstacle_ratio=obstacle_ratio, seed=seed)\n        self.size = self.config.size\n        self.seed = self.config.seed\n        self.start = self.config.start\n        self.goal = self.config.goal\n        self.obstacles = ObstacleStore(set())\n        self._generate_grid()\n\n    def _generate_grid(self) -> None:\n        \"\"\"\u969c\u5bb3\u7269\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u751f\u6210\"\"\"\n        num_obstacles = int(\n            self.size * self.size * self.config.obstacle_ratio\n        )\n\n        print(\n            f\"[*] \u30b0\u30ea\u30c3\u30c9\u751f\u6210\u4e2d... ({self.size}x{self.size}, \"\n            f\"\u969c\u5bb3\u7269: {num_obstacles}\u500b)\"\n        )\n\n        self.obstacles = ObstacleStore.from_config(self.config)\n\n    def heuristic(self, a, b):\n        \"\"\"\u30de\u30f3\u30cf\u30c3\u30bf\u30f3\u8ddd\u96e2\"\"\"\n        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n    def get_neighbors(self, node):\n        \"\"\"\u4e0a\u4e0b\u5de6\u53f3\u306e\u79fb\u52d5\uff08\u7bc4\u56f2\u5185\u304b\u3064\u969c\u5bb3\u7269\u306a\u3057\uff09\"\"\"\n        x, y = node\n        candidates = [\n            (x + 1, y), (x - 1, y),\n            (x, y + 1), (x, y - 1)\n        ]\n        results = []\n        for nx, ny in candidates:\n            if 0 <= nx < self.size and 0 <= ny < self.size:\n                if (nx, ny) not in self.obstacles.cells:\n                    results.append((nx, ny))\n        return results\n\n    def find_path(self):\n        \"\"\"A* \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u884c\"\"\"\n        start_time = time.time()\n        \n        open_set = []\n        heapq.heappush(open_set, (0, self.start))\n        \n        came_from = {}\n        g_score = {self.start: 0}\n        f_score = {self.start: self.heuristic(self.start, self.goal)}\n        \n        nodes_evaluated = 0\n\n        print(\"[*] \u7d4c\u8def\u63a2\u7d22\u958b\u59cb...\")\n\n        while open_set:\n            _, current = heapq.heappop(open_set)\n            nodes_evaluated += 1\n\n            if current == self.goal:\n                end_time = time.time()\n                return self._reconstruct_path(came_from, current), end_time - start_time, nodes_evaluated\n\n            for neighbor in self.get_neighbors(current):\n                tentative_g_score = g_score[current] + 1\n\n                if tentative_g_score < g_score.get(neighbor, float('inf')):\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f = tentative_g_score + self.heuristic(neighbor, self.goal)\n                    f_score[neighbor] = f\n                    heapq.heappush(open_set, (f, neighbor))\n\n        return None, time.time() - start_time, nodes_evaluated\n\n    def _reconstruct_path(self, came_from, current):\n        total_path = [current]\n        while current in came_from:\n            current = came_from[current]\n            total_path.append(current)\n        return total_path[::-1]\n\n    def to_native_args(self) -> tuple[int, float, int, list[tuple[int, int]]]:\n        return to_native_args(self.config, self.obstacles)\n\n", 139], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\parser.py": ["# Copyright (C) 2001-2007 Python Software Foundation\n# Author: Barry Warsaw, Thomas Wouters, Anthony Baxter\n# Contact: email-sig@python.org\n\n\"\"\"A parser of RFC 2822 and MIME email messages.\"\"\"\n\n__all__ = ['Parser', 'HeaderParser', 'BytesParser', 'BytesHeaderParser',\n           'FeedParser', 'BytesFeedParser']\n\nfrom io import StringIO, TextIOWrapper\n\nfrom email.feedparser import FeedParser, BytesFeedParser\nfrom email._policybase import compat32\n\n\nclass Parser:\n    def __init__(self, _class=None, *, policy=compat32):\n        \"\"\"Parser of RFC 2822 and MIME email messages.\n\n        Creates an in-memory object tree representing the email message, which\n        can then be manipulated and turned over to a Generator to return the\n        textual representation of the message.\n\n        The string must be formatted as a block of RFC 2822 headers and header\n        continuation lines, optionally preceded by a `Unix-from' header.  The\n        header block is terminated either by the end of the string or by a\n        blank line.\n\n        _class is the class to instantiate for new message objects when they\n        must be created.  This class must have a constructor that can take\n        zero arguments.  Default is Message.Message.\n\n        The policy keyword specifies a policy object that controls a number of\n        aspects of the parser's operation.  The default policy maintains\n        backward compatibility.\n\n        \"\"\"\n        self._class = _class\n        self.policy = policy\n\n    def parse(self, fp, headersonly=False):\n        \"\"\"Create a message structure from the data in a file.\n\n        Reads all the data from the file and returns the root of the message\n        structure.  Optional headersonly is a flag specifying whether to stop\n        parsing after reading the headers or not.  The default is False,\n        meaning it parses the entire contents of the file.\n        \"\"\"\n        feedparser = FeedParser(self._class, policy=self.policy)\n        if headersonly:\n            feedparser._set_headersonly()\n        while data := fp.read(8192):\n            feedparser.feed(data)\n        return feedparser.close()\n\n    def parsestr(self, text, headersonly=False):\n        \"\"\"Create a message structure from a string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        return self.parse(StringIO(text), headersonly=headersonly)\n\n\nclass HeaderParser(Parser):\n    def parse(self, fp, headersonly=True):\n        return Parser.parse(self, fp, True)\n\n    def parsestr(self, text, headersonly=True):\n        return Parser.parsestr(self, text, True)\n\n\nclass BytesParser:\n\n    def __init__(self, *args, **kw):\n        \"\"\"Parser of binary RFC 2822 and MIME email messages.\n\n        Creates an in-memory object tree representing the email message, which\n        can then be manipulated and turned over to a Generator to return the\n        textual representation of the message.\n\n        The input must be formatted as a block of RFC 2822 headers and header\n        continuation lines, optionally preceded by a `Unix-from' header.  The\n        header block is terminated either by the end of the input or by a\n        blank line.\n\n        _class is the class to instantiate for new message objects when they\n        must be created.  This class must have a constructor that can take\n        zero arguments.  Default is Message.Message.\n        \"\"\"\n        self.parser = Parser(*args, **kw)\n\n    def parse(self, fp, headersonly=False):\n        \"\"\"Create a message structure from the data in a binary file.\n\n        Reads all the data from the file and returns the root of the message\n        structure.  Optional headersonly is a flag specifying whether to stop\n        parsing after reading the headers or not.  The default is False,\n        meaning it parses the entire contents of the file.\n        \"\"\"\n        fp = TextIOWrapper(fp, encoding='ascii', errors='surrogateescape')\n        try:\n            return self.parser.parse(fp, headersonly)\n        finally:\n            fp.detach()\n\n\n    def parsebytes(self, text, headersonly=False):\n        \"\"\"Create a message structure from a byte string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        text = text.decode('ASCII', errors='surrogateescape')\n        return self.parser.parsestr(text, headersonly)\n\n\nclass BytesHeaderParser(BytesParser):\n    def parse(self, fp, headersonly=True):\n        return BytesParser.parse(self, fp, headersonly=True)\n\n    def parsebytes(self, text, headersonly=True):\n        return BytesParser.parsebytes(self, text, headersonly=True)\n", 127], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\__init__.py": ["# Copyright (C) 2001-2007 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"A package for parsing, handling, and generating email messages.\"\"\"\n\n__all__ = [\n    'base64mime',\n    'charset',\n    'encoders',\n    'errors',\n    'feedparser',\n    'generator',\n    'header',\n    'iterators',\n    'message',\n    'message_from_file',\n    'message_from_binary_file',\n    'message_from_string',\n    'message_from_bytes',\n    'mime',\n    'parser',\n    'quoprimime',\n    'utils',\n    ]\n\n\n# Some convenience routines.  Don't import Parser and Message as side-effects\n# of importing email since those cascadingly import most of the rest of the\n# email package.\ndef message_from_string(s, *args, **kws):\n    \"\"\"Parse a string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parsestr(s)\n\ndef message_from_bytes(s, *args, **kws):\n    \"\"\"Parse a bytes string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import BytesParser\n    return BytesParser(*args, **kws).parsebytes(s)\n\ndef message_from_file(fp, *args, **kws):\n    \"\"\"Read a file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parse(fp)\n\ndef message_from_binary_file(fp, *args, **kws):\n    \"\"\"Read a binary file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import BytesParser\n    return BytesParser(*args, **kws).parse(fp)\n", 61], "C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py": ["\"\"\"a_star\n\nRust-backed Python package scaffold that re-exports functions from the native extension module.\n\nBuild the extension before importing in documentation or runtime contexts:\n`uv run maturin develop`.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom importlib import import_module\nfrom importlib.metadata import PackageNotFoundError, version\nfrom typing import Any, Optional\n\nfrom a_star.pure_python import AStarPathfinder as PurePythonAStarPathfinder\n\n__all__ = [\n    \"AStarPathfinder\",\n    \"SearchTelemetry\",\n    \"validate_name\",\n    \"native_available\",\n    \"__version__\",\n]\n\n_NATIVE_MODULE = \"a_star._native\"\n\n\ndef _load_native() -> Any:\n    try:\n        return import_module(_NATIVE_MODULE)\n    except ImportError:\n        return None\n\n\ndef _require_native() -> Any:\n    native = _load_native()\n    if native is None:\n        raise RuntimeError(\n            \"Native extension module is not built. Run `uv run maturin develop` and retry.\"\n        )\n    return native\n\n\ntry:\n    __version__ = version(\"a_star\")\nexcept PackageNotFoundError:  # pragma: no cover - fallback for editable installs\n    __version__ = \"0.1.0\"\n\n\ndef native_available() -> bool:\n    return _load_native() is not None\n\n\n\ndef validate_name(name: str) -> str:\n    \"\"\"Return the stripped name or raise ``ValueError`` when blank.\"\"\"\n\n    native = _require_native()\n    return str(native.validate_name(str(name)))\n\n\n@dataclass\nclass SearchTelemetry:\n    duration: float\n    nodes_evaluated: int\n    heap_pushes: int = 0\n    heap_pops: int = 0\n    neighbors_checked: int = 0\n\n\nclass AStarPathfinder:\n    def __init__(self, size: int, obstacle_ratio: float, seed: int, force_python: bool = False):\n        self._force_python = force_python\n        self._python_finder = PurePythonAStarPathfinder(size, obstacle_ratio, seed)\n        self._native = None if force_python else _load_native()\n        self._last_telemetry: Optional[SearchTelemetry] = None\n\n    def find_path(self):\n        if self._native is None:\n            path, duration, nodes = self._python_finder.find_path()\n            self._last_telemetry = SearchTelemetry(duration=duration, nodes_evaluated=nodes)\n            return path, duration, nodes\n\n        native_find_path = getattr(self._native, \"native_find_path\", None)\n        if native_find_path is None:\n            path, duration, nodes = self._python_finder.find_path()\n            self._last_telemetry = SearchTelemetry(duration=duration, nodes_evaluated=nodes)\n            return path, duration, nodes\n\n        print(\"[*] \u7d4c\u8def\u63a2\u7d22\u958b\u59cb...\")\n        result = native_find_path(*self._python_finder.to_native_args())\n        if isinstance(result, tuple) and len(result) == 4:\n            path, duration, nodes, telemetry = result\n            self._last_telemetry = telemetry\n            return path, duration, nodes\n\n        return result\n\n    @property\n    def telemetry(self) -> Optional[SearchTelemetry]:\n        \"\"\"Return telemetry from the most recent `find_path()` run.\"\"\"\n\n        return self._last_telemetry\n", 102], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py": ["\"\"\"Random variable generators.\n\n    bytes\n    -----\n           uniform bytes (values between 0 and 255)\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           pick weighted random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\n    discrete distributions\n    ----------------------\n           binomial\n\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom math import tau as TWOPI, floor as _floor, isfinite as _isfinite\nfrom math import lgamma as _lgamma, fabs as _fabs, log2 as _log2\nfrom os import urandom as _urandom\nfrom _collections_abc import Sequence as _Sequence\nfrom operator import index as _index\nfrom itertools import accumulate as _accumulate, repeat as _repeat\nfrom bisect import bisect as _bisect\nimport os as _os\nimport _random\n\ntry:\n    # hashlib is pretty heavy to load, try lean internal module first\n    from _sha2 import sha512 as _sha512\nexcept ImportError:\n    # fallback to official implementation\n    from hashlib import sha512 as _sha512\n\n__all__ = [\n    \"Random\",\n    \"SystemRandom\",\n    \"betavariate\",\n    \"binomialvariate\",\n    \"choice\",\n    \"choices\",\n    \"expovariate\",\n    \"gammavariate\",\n    \"gauss\",\n    \"getrandbits\",\n    \"getstate\",\n    \"lognormvariate\",\n    \"normalvariate\",\n    \"paretovariate\",\n    \"randbytes\",\n    \"randint\",\n    \"random\",\n    \"randrange\",\n    \"sample\",\n    \"seed\",\n    \"setstate\",\n    \"shuffle\",\n    \"triangular\",\n    \"uniform\",\n    \"vonmisesvariate\",\n    \"weibullvariate\",\n]\n\nNV_MAGICCONST = 4 * _exp(-0.5) / _sqrt(2.0)\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2 ** -BPF\n_ONE = 1\n\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods:  random(), seed(), getstate(), and setstate().\n    Optionally, implement a getrandbits() method so that randrange()\n    can cover arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None, version=2):\n        \"\"\"Initialize internal state from a seed.\n\n        The only supported seed types are None, int, float,\n        str, bytes, and bytearray.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If *a* is an int, all bits are used.\n\n        For version 2 (the default), all of the bits are used if *a* is a str,\n        bytes, or bytearray.  For version 1 (provided for reproducing random\n        sequences from older versions of Python), the algorithm for str and\n        bytes generates a narrower range of seeds.\n\n        \"\"\"\n\n        if version == 1 and isinstance(a, (str, bytes)):\n            a = a.decode('latin-1') if isinstance(a, bytes) else a\n            x = ord(a[0]) << 7 if a else 0\n            for c in map(ord, a):\n                x = ((1000003 * x) ^ c) & 0xFFFFFFFFFFFFFFFF\n            x ^= len(a)\n            a = -2 if x == -1 else x\n\n        elif version == 2 and isinstance(a, (str, bytes, bytearray)):\n            if isinstance(a, str):\n                a = a.encode()\n            a = int.from_bytes(a + _sha512(a).digest())\n\n        elif not isinstance(a, (type(None), int, float, str, bytes, bytearray)):\n            raise TypeError('The only supported seed types are: None,\\n'\n                            'int, float, str, bytes, and bytearray.')\n\n        super().seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super().getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super().setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple(x % (2 ** 32) for x in internalstate)\n            except ValueError as e:\n                raise TypeError from e\n            super().setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n\n    ## -------------------------------------------------------\n    ## ---- Methods below this point do not need to be overridden or extended\n    ## ---- when subclassing for the purpose of using a different core generator.\n\n\n    ## -------------------- pickle support  -------------------\n\n    # Issue 17489: Since __reduce__ was defined to fix #759889 this is no\n    # longer called; we leave it here because it has been here since random was\n    # rewritten back in 2001 and why risk breaking something.\n    def __getstate__(self):  # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n\n    ## ---- internal support method for evenly distributed integers ----\n\n    def __init_subclass__(cls, /, **kwargs):\n        \"\"\"Control how subclasses generate random integers.\n\n        The algorithm a subclass can use depends on the random() and/or\n        getrandbits() implementation available to it and determines\n        whether it can generate random integers from arbitrarily large\n        ranges.\n        \"\"\"\n\n        for c in cls.__mro__:\n            if '_randbelow' in c.__dict__:\n                # just inherit it\n                break\n            if 'getrandbits' in c.__dict__:\n                cls._randbelow = cls._randbelow_with_getrandbits\n                break\n            if 'random' in c.__dict__:\n                cls._randbelow = cls._randbelow_without_getrandbits\n                break\n\n    def _randbelow_with_getrandbits(self, n):\n        \"Return a random int in the range [0,n).  Defined for n > 0.\"\n\n        getrandbits = self.getrandbits\n        k = n.bit_length()\n        r = getrandbits(k)  # 0 <= r < 2**k\n        while r >= n:\n            r = getrandbits(k)\n        return r\n\n    def _randbelow_without_getrandbits(self, n, maxsize=1<<BPF):\n        \"\"\"Return a random int in the range [0,n).  Defined for n > 0.\n\n        The implementation does not use getrandbits, but only random.\n        \"\"\"\n\n        random = self.random\n        if n >= maxsize:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large.\\n\"\n                \"To remove the range limitation, add a getrandbits() method.\")\n            return _floor(random() * n)\n        rem = maxsize % n\n        limit = (maxsize - rem) / maxsize   # int(limit * maxsize) % n == 0\n        r = random()\n        while r >= limit:\n            r = random()\n        return _floor(r * maxsize) % n\n\n    _randbelow = _randbelow_with_getrandbits\n\n\n    ## --------------------------------------------------------\n    ## ---- Methods below this point generate custom distributions\n    ## ---- based on the methods defined above.  They do not\n    ## ---- directly touch the underlying generator and only\n    ## ---- access randomness through the methods:  random(),\n    ## ---- getrandbits(), or _randbelow().\n\n\n    ## -------------------- bytes methods ---------------------\n\n    def randbytes(self, n):\n        \"\"\"Generate n random bytes.\"\"\"\n        return self.getrandbits(n * 8).to_bytes(n, 'little')\n\n\n    ## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=_ONE):\n        \"\"\"Choose a random item from range(stop) or range(start, stop[, step]).\n\n        Roughly equivalent to ``choice(range(start, stop, step))`` but\n        supports arbitrarily large ranges and is optimized for common cases.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _index(start)\n        if stop is None:\n            # We don't check for \"step != 1\" because it hasn't been\n            # type checked and converted to an integer yet.\n            if step is not _ONE:\n                raise TypeError(\"Missing a non-None stop argument\")\n            if istart > 0:\n                return self._randbelow(istart)\n            raise ValueError(\"empty range for randrange()\")\n\n        # Stop argument supplied.\n        istop = _index(stop)\n        width = istop - istart\n        istep = _index(step)\n        # Fast path.\n        if istep == 1:\n            if width > 0:\n                return istart + self._randbelow(width)\n            raise ValueError(f\"empty range in randrange({start}, {stop})\")\n\n        # Non-unit step argument supplied.\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError(\"zero step for randrange()\")\n        if n <= 0:\n            raise ValueError(f\"empty range in randrange({start}, {stop}, {step})\")\n        return istart + istep * self._randbelow(n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n\n    ## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n\n        # As an accommodation for NumPy, we don't use \"if not seq\"\n        # because bool(numpy.array()) raises a ValueError.\n        if not len(seq):\n            raise IndexError('Cannot choose from an empty sequence')\n        return seq[self._randbelow(len(seq))]\n\n    def shuffle(self, x):\n        \"\"\"Shuffle list x in place, and return None.\"\"\"\n\n        randbelow = self._randbelow\n        for i in reversed(range(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = randbelow(i + 1)\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k, *, counts=None):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        Repeated elements can be specified one at a time or with the optional\n        counts parameter.  For example:\n\n            sample(['red', 'blue'], counts=[4, 2], k=5)\n\n        is equivalent to:\n\n            sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n\n        To choose a sample from a range of integers, use range() for the\n        population argument.  This is especially fast and space efficient\n        for sampling from a large population:\n\n            sample(range(10000000), 60)\n\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        # The number of calls to _randbelow() is kept at or near k, the\n        # theoretical minimum.  This is important because running time\n        # is dominated by _randbelow() and because it extracts the\n        # least entropy from the underlying random number generators.\n\n        # Memory requirements are kept to the smaller of a k-length\n        # set or an n-length list.\n\n        # There are other sampling algorithms that do not require\n        # auxiliary memory, but they were rejected because they made\n        # too many calls to _randbelow(), making them slower and\n        # causing them to eat more entropy than necessary.\n\n        if not isinstance(population, _Sequence):\n            raise TypeError(\"Population must be a sequence.  \"\n                            \"For dicts or sets, use sorted(d).\")\n        n = len(population)\n        if counts is not None:\n            cum_counts = list(_accumulate(counts))\n            if len(cum_counts) != n:\n                raise ValueError('The number of counts does not match the population')\n            total = cum_counts.pop() if cum_counts else 0\n            if not isinstance(total, int):\n                raise TypeError('Counts must be integers')\n            if total < 0:\n                raise ValueError('Counts must be non-negative')\n            selections = self.sample(range(total), k=k)\n            bisect = _bisect\n            return [population[bisect(cum_counts, s)] for s in selections]\n        randbelow = self._randbelow\n        if not 0 <= k <= n:\n            raise ValueError(\"Sample larger than population or is negative\")\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4))  # table size for big sets\n        if n <= setsize:\n            # An n-length list is smaller than a k-length set.\n            # Invariant:  non-selected at pool[0 : n-i]\n            pool = list(population)\n            for i in range(k):\n                j = randbelow(n - i)\n                result[i] = pool[j]\n                pool[j] = pool[n - i - 1]  # move non-selected item into vacancy\n        else:\n            selected = set()\n            selected_add = selected.add\n            for i in range(k):\n                j = randbelow(n)\n                while j in selected:\n                    j = randbelow(n)\n                selected_add(j)\n                result[i] = population[j]\n        return result\n\n    def choices(self, population, weights=None, *, cum_weights=None, k=1):\n        \"\"\"Return a k sized list of population elements chosen with replacement.\n\n        If the relative weights or cumulative weights are not specified,\n        the selections are made with equal probability.\n\n        \"\"\"\n        random = self.random\n        n = len(population)\n        if cum_weights is None:\n            if weights is None:\n                floor = _floor\n                n += 0.0    # convert to float for a small speed improvement\n                return [population[floor(random() * n)] for i in _repeat(None, k)]\n            try:\n                cum_weights = list(_accumulate(weights))\n            except TypeError:\n                if not isinstance(weights, int):\n                    raise\n                k = weights\n                raise TypeError(\n                    f'The number of choices must be a keyword argument: {k=}'\n                ) from None\n        elif weights is not None:\n            raise TypeError('Cannot specify both weights and cumulative weights')\n        if len(cum_weights) != n:\n            raise ValueError('The number of weights does not match the population')\n        total = cum_weights[-1] + 0.0   # convert to float\n        if total <= 0.0:\n            raise ValueError('Total of weights must be greater than zero')\n        if not _isfinite(total):\n            raise ValueError('Total of weights must be finite')\n        bisect = _bisect\n        hi = n - 1\n        return [population[bisect(cum_weights, random() * total, 0, hi)]\n                for i in _repeat(None, k)]\n\n\n    ## -------------------- real-valued distributions  -------------------\n\n    def uniform(self, a, b):\n        \"\"\"Get a random number in the range [a, b) or [a, b] depending on rounding.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = (a + b) / 2\n            Var[X] = (b - a) ** 2 / 12\n\n        \"\"\"\n        return a + (b - a) * self.random()\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = (low + high + mode) / 3\n            Var[X] = (low**2 + high**2 + mode**2 - low*high - low*mode - high*mode) / 18\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * _sqrt(u * c)\n\n    def normalvariate(self, mu=0.0, sigma=1.0):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while True:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST * (u1 - 0.5) / u2\n            zz = z * z / 4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z * sigma\n\n    def gauss(self, mu=0.0, sigma=1.0):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z * sigma\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n    def expovariate(self, lambd=1.0):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = 1 / lambd\n            Var[X] = 1 / lambd ** 2\n\n        \"\"\"\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n\n        return -_log(1.0 - self.random()) / lambd\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while True:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = alpha * beta\n            Var[X] = alpha * beta ** 2\n\n        \"\"\"\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError('gammavariate: alpha and beta must be > 0.0')\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while True:\n                u1 = random()\n                if not 1e-7 < u1 < 0.9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1 / (1.0 - u1)) / ainv\n                x = alpha * _exp(v)\n                z = u1 * u1 * u2\n                r = bbb + ccc * v - x\n                if r + SG_MAGICCONST - 4.5 * z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1/beta)\n            return -_log(1.0 - random()) * beta\n\n        else:\n            # alpha is between 0 and 1 (exclusive)\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n            while True:\n                u = random()\n                b = (_e + alpha) / _e\n                p = b * u\n                if p <= 1.0:\n                    x = p ** (1.0 / alpha)\n                else:\n                    x = -_log((b - p) / alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = alpha / (alpha + beta)\n            Var[X] = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))\n\n        \"\"\"\n        ## See\n        ## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n        ## for Ivan Frohne's insightful analysis of why the original implementation:\n        ##\n        ##    def betavariate(self, alpha, beta):\n        ##        # Discrete Event Simulation in C, pp 87-88.\n        ##\n        ##        y = self.expovariate(alpha)\n        ##        z = self.expovariate(1.0/beta)\n        ##        return z/(y+z)\n        ##\n        ## was dead wrong, and how it probably got that way.\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.0)\n        if y:\n            return y / (y + self.gammavariate(beta, 1.0))\n        return 0.0\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return u ** (-1.0 / alpha)\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * (-_log(u)) ** (1.0 / beta)\n\n\n    ## -------------------- discrete  distributions  ---------------------\n\n    def binomialvariate(self, n=1, p=0.5):\n        \"\"\"Binomial random variable.\n\n        Gives the number of successes for *n* independent trials\n        with the probability of success in each trial being *p*:\n\n            sum(random() < p for i in range(n))\n\n        Returns an integer in the range:   0 <= X <= n\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = n * p\n            Var[x] = n * p * (1 - p)\n\n        \"\"\"\n        # Error check inputs and handle edge cases\n        if n < 0:\n            raise ValueError(\"n must be non-negative\")\n        if p <= 0.0 or p >= 1.0:\n            if p == 0.0:\n                return 0\n            if p == 1.0:\n                return n\n            raise ValueError(\"p must be in the range 0.0 <= p <= 1.0\")\n\n        random = self.random\n\n        # Fast path for a common case\n        if n == 1:\n            return _index(random() < p)\n\n        # Exploit symmetry to establish:  p <= 0.5\n        if p > 0.5:\n            return n - self.binomialvariate(n, 1.0 - p)\n\n        if n * p < 10.0:\n            # BG: Geometric method by Devroye with running time of O(np).\n            # https://dl.acm.org/doi/pdf/10.1145/42372.42381\n            x = y = 0\n            c = _log2(1.0 - p)\n            if not c:\n                return x\n            while True:\n                y += _floor(_log2(random()) / c) + 1\n                if y > n:\n                    return x\n                x += 1\n\n        # BTRS: Transformed rejection with squeeze method by Wolfgang H\u00f6rmann\n        # https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.8407&rep=rep1&type=pdf\n        assert n*p >= 10.0 and p <= 0.5\n        setup_complete = False\n\n        spq = _sqrt(n * p * (1.0 - p))  # Standard deviation of the distribution\n        b = 1.15 + 2.53 * spq\n        a = -0.0873 + 0.0248 * b + 0.01 * p\n        c = n * p + 0.5\n        vr = 0.92 - 4.2 / b\n\n        while True:\n\n            u = random()\n            u -= 0.5\n            us = 0.5 - _fabs(u)\n            k = _floor((2.0 * a / us + b) * u + c)\n            if k < 0 or k > n:\n                continue\n\n            # The early-out \"squeeze\" test substantially reduces\n            # the number of acceptance condition evaluations.\n            v = random()\n            if us >= 0.07 and v <= vr:\n                return k\n\n            # Acceptance-rejection test.\n            # Note, the original paper erroneously omits the call to log(v)\n            # when comparing to the log of the rescaled binomial distribution.\n            if not setup_complete:\n                alpha = (2.83 + 5.1 / b) * spq\n                lpq = _log(p / (1.0 - p))\n                m = _floor((n + 1) * p)         # Mode of the distribution\n                h = _lgamma(m + 1) + _lgamma(n - m + 1)\n                setup_complete = True           # Only needs to be done once\n            v *= alpha / (a / (us * us) + b)\n            if _log(v) <= h - _lgamma(k + 1) - _lgamma(n - k + 1) + (k - m) * lpq:\n                return k\n\n\n## ------------------------------------------------------------------\n## --------------- Operating System Random Source  ------------------\n\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range 0.0 <= X < 1.0.\"\"\"\n        return (int.from_bytes(_urandom(7)) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates an int with k random bits.\"\"\"\n        if k < 0:\n            raise ValueError('number of bits must be non-negative')\n        numbytes = (k + 7) // 8                       # bits / 8 and rounded up\n        x = int.from_bytes(_urandom(numbytes))\n        return x >> (numbytes * 8 - k)                # trim excess bits\n\n    def randbytes(self, n):\n        \"\"\"Generate n random bytes.\"\"\"\n        # os.urandom(n) fails with ValueError for n < 0\n        # and returns an empty bytes string for n == 0.\n        return _urandom(n)\n\n    def seed(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n\n# ----------------------------------------------------------------------\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n# (both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nchoices = _inst.choices\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nbinomialvariate = _inst.binomialvariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\ngetrandbits = _inst.getrandbits\nrandbytes = _inst.randbytes\n\n\n## ------------------------------------------------------\n## ----------------- test program -----------------------\n\ndef _test_generator(n, func, args):\n    from statistics import stdev, fmean as mean\n    from time import perf_counter\n\n    t0 = perf_counter()\n    data = [func(*args) for i in _repeat(None, n)]\n    t1 = perf_counter()\n\n    xbar = mean(data)\n    sigma = stdev(data, xbar)\n    low = min(data)\n    high = max(data)\n\n    print(f'{t1 - t0:.3f} sec, {n} times {func.__name__}{args!r}')\n    print('avg %g, stddev %g, min %g, max %g\\n' % (xbar, sigma, low, high))\n\n\ndef _test(N=10_000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, binomialvariate, (15, 0.60))\n    _test_generator(N, binomialvariate, (100, 0.75))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0 / 3.0))\n\n\n## ------------------------------------------------------\n## ------------------ fork support  ---------------------\n\nif hasattr(_os, \"fork\"):\n    _os.register_at_fork(after_in_child=_inst.seed)\n\n\nif __name__ == '__main__':\n    _test()\n", 996], "C:\\Users\\naoki\\Documents\\Work\\a_star\\main.py": ["import os\nimport sys\n\nfrom a_star import AStarPathfinder, native_available\n\n# --- \u8a2d\u5b9a ---\nGRID_SIZE = 3000      # \u30b0\u30ea\u30c3\u30c9\u306e\u30b5\u30a4\u30ba (3,000x3,000 = 900\u4e07\u30ce\u30fc\u30c9)\nOBSTACLE_RATIO = 0.1  # \u969c\u5bb3\u7269\u306e\u5272\u5408 (10%)\nSEED = 42             # \u6bce\u56de\u540c\u3058\u30de\u30c3\u30d7\u3092\u751f\u6210\u3059\u308b\u305f\u3081\u306e\u30b7\u30fc\u30c9\u5024\n\nos.environ[\"A_STAR_FORCE_PYTHON\"] = \"1\"\n\nif __name__ == \"__main__\":\n    # \u518d\u5e30\u5236\u9650\u306e\u7de9\u548c\n    sys.setrecursionlimit(20000)\n\n    force_python = os.getenv(\"A_STAR_FORCE_PYTHON\") in {\"1\", \"true\", \"True\"}\n    if force_python:\n        print(\"[*] A_STAR_FORCE_PYTHON \u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081 Python \u5b9f\u88c5\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\")\n    elif not native_available():\n        print(\"[*] Native \u30e2\u30b8\u30e5\u30fc\u30eb\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u305f\u3081 Python \u5b9f\u88c5\u3078\u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\u3057\u307e\u3059\u3002\")\n\n    # \u521d\u671f\u5316\n    finder = AStarPathfinder(GRID_SIZE, OBSTACLE_RATIO, SEED, force_python=force_python)\n    \n    # \u5b9f\u884c\u3068\u8a08\u6e2c\n    path, duration, nodes = finder.find_path()\n\n    # \u7d50\u679c\u51fa\u529b\n    print(\"-\" * 30)\n    if path:\n        print(f\"\u2705 \u7d4c\u8def\u767a\u898b\u6210\u529f\")\n        print(f\"   - \u7d4c\u8def\u9577: {len(path)} \u30b9\u30c6\u30c3\u30d7\")\n    else:\n        print(f\"\u274c \u7d4c\u8def\u306a\u3057\uff08\u30d6\u30ed\u30c3\u30af\u3055\u308c\u3066\u3044\u307e\u3059\uff09\")\n\n    print(f\"   - \u63a2\u7d22\u30ce\u30fc\u30c9\u6570: {nodes:,} \u500b\")\n    print(f\"   - \u5b9f\u884c\u6642\u9593: {duration:.4f} \u79d2\")\n    print(\"-\" * 30)\n", 39], "C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py": ["#\n# Module providing various facilities to other parts of the package\n#\n# multiprocessing/util.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\nimport os\nimport itertools\nimport sys\nimport weakref\nimport atexit\nimport threading        # we want threading to install it's\n                        # cleanup function before multiprocessing does\nfrom subprocess import _args_from_interpreter_flags\n\nfrom . import process\n\n__all__ = [\n    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',\n    'log_to_stderr', 'get_temp_dir', 'register_after_fork',\n    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',\n    'close_all_fds_except', 'SUBDEBUG', 'SUBWARNING',\n    ]\n\n#\n# Logging\n#\n\nNOTSET = 0\nSUBDEBUG = 5\nDEBUG = 10\nINFO = 20\nSUBWARNING = 25\n\nLOGGER_NAME = 'multiprocessing'\nDEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'\n\n_logger = None\n_log_to_stderr = False\n\ndef sub_debug(msg, *args):\n    if _logger:\n        _logger.log(SUBDEBUG, msg, *args, stacklevel=2)\n\ndef debug(msg, *args):\n    if _logger:\n        _logger.log(DEBUG, msg, *args, stacklevel=2)\n\ndef info(msg, *args):\n    if _logger:\n        _logger.log(INFO, msg, *args, stacklevel=2)\n\ndef sub_warning(msg, *args):\n    if _logger:\n        _logger.log(SUBWARNING, msg, *args, stacklevel=2)\n\ndef get_logger():\n    '''\n    Returns logger used by multiprocessing\n    '''\n    global _logger\n    import logging\n\n    logging._acquireLock()\n    try:\n        if not _logger:\n\n            _logger = logging.getLogger(LOGGER_NAME)\n            _logger.propagate = 0\n\n            # XXX multiprocessing should cleanup before logging\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(_exit_function)\n                atexit.register(_exit_function)\n            else:\n                atexit._exithandlers.remove((_exit_function, (), {}))\n                atexit._exithandlers.append((_exit_function, (), {}))\n\n    finally:\n        logging._releaseLock()\n\n    return _logger\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    global _log_to_stderr\n    import logging\n\n    logger = get_logger()\n    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if level:\n        logger.setLevel(level)\n    _log_to_stderr = True\n    return _logger\n\n\n# Abstract socket support\n\ndef _platform_supports_abstract_sockets():\n    if sys.platform == \"linux\":\n        return True\n    if hasattr(sys, 'getandroidapilevel'):\n        return True\n    return False\n\n\ndef is_abstract_socket_namespace(address):\n    if not address:\n        return False\n    if isinstance(address, bytes):\n        return address[0] == 0\n    elif isinstance(address, str):\n        return address[0] == \"\\0\"\n    raise TypeError(f'address type of {address!r} unrecognized')\n\n\nabstract_sockets_supported = _platform_supports_abstract_sockets()\n\n#\n# Function returning a temp directory which will be removed on exit\n#\n\ndef _remove_temp_dir(rmtree, tempdir):\n    def onerror(func, path, err_info):\n        if not issubclass(err_info[0], FileNotFoundError):\n            raise\n    rmtree(tempdir, onerror=onerror)\n\n    current_process = process.current_process()\n    # current_process() can be None if the finalizer is called\n    # late during Python finalization\n    if current_process is not None:\n        current_process._config['tempdir'] = None\n\ndef get_temp_dir():\n    # get name of a temp directory which will be automatically cleaned up\n    tempdir = process.current_process()._config.get('tempdir')\n    if tempdir is None:\n        import shutil, tempfile\n        tempdir = tempfile.mkdtemp(prefix='pymp-')\n        info('created temp directory %s', tempdir)\n        # keep a strong reference to shutil.rmtree(), since the finalizer\n        # can be called late during Python shutdown\n        Finalize(None, _remove_temp_dir, args=(shutil.rmtree, tempdir),\n                 exitpriority=-100)\n        process.current_process()._config['tempdir'] = tempdir\n    return tempdir\n\n#\n# Support for reinitialization of objects when bootstrapping a child process\n#\n\n_afterfork_registry = weakref.WeakValueDictionary()\n_afterfork_counter = itertools.count()\n\ndef _run_after_forkers():\n    items = list(_afterfork_registry.items())\n    items.sort()\n    for (index, ident, func), obj in items:\n        try:\n            func(obj)\n        except Exception as e:\n            info('after forker raised exception %s', e)\n\ndef register_after_fork(obj, func):\n    _afterfork_registry[(next(_afterfork_counter), id(obj), func)] = obj\n\n#\n# Finalization using weakrefs\n#\n\n_finalizer_registry = {}\n_finalizer_counter = itertools.count()\n\n\nclass Finalize(object):\n    '''\n    Class which supports object finalization using weakrefs\n    '''\n    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):\n        if (exitpriority is not None) and not isinstance(exitpriority,int):\n            raise TypeError(\n                \"Exitpriority ({0!r}) must be None or int, not {1!s}\".format(\n                    exitpriority, type(exitpriority)))\n\n        if obj is not None:\n            self._weakref = weakref.ref(obj, self)\n        elif exitpriority is None:\n            raise ValueError(\"Without object, exitpriority cannot be None\")\n\n        self._callback = callback\n        self._args = args\n        self._kwargs = kwargs or {}\n        self._key = (exitpriority, next(_finalizer_counter))\n        self._pid = os.getpid()\n\n        _finalizer_registry[self._key] = self\n\n    def __call__(self, wr=None,\n                 # Need to bind these locally because the globals can have\n                 # been cleared at shutdown\n                 _finalizer_registry=_finalizer_registry,\n                 sub_debug=sub_debug, getpid=os.getpid):\n        '''\n        Run the callback unless it has already been called or cancelled\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            sub_debug('finalizer no longer registered')\n        else:\n            if self._pid != getpid():\n                sub_debug('finalizer ignored because different process')\n                res = None\n            else:\n                sub_debug('finalizer calling %s with args %s and kwargs %s',\n                          self._callback, self._args, self._kwargs)\n                res = self._callback(*self._args, **self._kwargs)\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n            return res\n\n    def cancel(self):\n        '''\n        Cancel finalization of the object\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            pass\n        else:\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n\n    def still_active(self):\n        '''\n        Return whether this finalizer is still waiting to invoke callback\n        '''\n        return self._key in _finalizer_registry\n\n    def __repr__(self):\n        try:\n            obj = self._weakref()\n        except (AttributeError, TypeError):\n            obj = None\n\n        if obj is None:\n            return '<%s object, dead>' % self.__class__.__name__\n\n        x = '<%s object, callback=%s' % (\n                self.__class__.__name__,\n                getattr(self._callback, '__name__', self._callback))\n        if self._args:\n            x += ', args=' + str(self._args)\n        if self._kwargs:\n            x += ', kwargs=' + str(self._kwargs)\n        if self._key[0] is not None:\n            x += ', exitpriority=' + str(self._key[0])\n        return x + '>'\n\n\ndef _run_finalizers(minpriority=None):\n    '''\n    Run all finalizers whose exit priority is not None and at least minpriority\n\n    Finalizers with highest priority are called first; finalizers with\n    the same priority will be called in reverse order of creation.\n    '''\n    if _finalizer_registry is None:\n        # This function may be called after this module's globals are\n        # destroyed.  See the _exit_function function in this module for more\n        # notes.\n        return\n\n    if minpriority is None:\n        f = lambda p : p[0] is not None\n    else:\n        f = lambda p : p[0] is not None and p[0] >= minpriority\n\n    # Careful: _finalizer_registry may be mutated while this function\n    # is running (either by a GC run or by another thread).\n\n    # list(_finalizer_registry) should be atomic, while\n    # list(_finalizer_registry.items()) is not.\n    keys = [key for key in list(_finalizer_registry) if f(key)]\n    keys.sort(reverse=True)\n\n    for key in keys:\n        finalizer = _finalizer_registry.get(key)\n        # key may have been removed from the registry\n        if finalizer is not None:\n            sub_debug('calling %s', finalizer)\n            try:\n                finalizer()\n            except Exception:\n                import traceback\n                traceback.print_exc()\n\n    if minpriority is None:\n        _finalizer_registry.clear()\n\n#\n# Clean up on exit\n#\n\ndef is_exiting():\n    '''\n    Returns true if the process is shutting down\n    '''\n    return _exiting or _exiting is None\n\n_exiting = False\n\ndef _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n                   active_children=process.active_children,\n                   current_process=process.current_process):\n    # We hold on to references to functions in the arglist due to the\n    # situation described below, where this function is called after this\n    # module's globals are destroyed.\n\n    global _exiting\n\n    if not _exiting:\n        _exiting = True\n\n        info('process shutting down')\n        debug('running all \"atexit\" finalizers with priority >= 0')\n        _run_finalizers(0)\n\n        if current_process() is not None:\n            # We check if the current process is None here because if\n            # it's None, any call to ``active_children()`` will raise\n            # an AttributeError (active_children winds up trying to\n            # get attributes from util._current_process).  One\n            # situation where this can happen is if someone has\n            # manipulated sys.modules, causing this module to be\n            # garbage collected.  The destructor for the module type\n            # then replaces all values in the module dict with None.\n            # For instance, after setuptools runs a test it replaces\n            # sys.modules with a copy created earlier.  See issues\n            # #9775 and #15881.  Also related: #4106, #9205, and\n            # #9207.\n\n            for p in active_children():\n                if p.daemon:\n                    info('calling terminate() for daemon %s', p.name)\n                    p._popen.terminate()\n\n            for p in active_children():\n                info('calling join() for process %s', p.name)\n                p.join()\n\n        debug('running the remaining \"atexit\" finalizers')\n        _run_finalizers()\n\natexit.register(_exit_function)\n\n#\n# Some fork aware types\n#\n\nclass ForkAwareThreadLock(object):\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n        register_after_fork(self, ForkAwareThreadLock._at_fork_reinit)\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n\nclass ForkAwareLocal(threading.local):\n    def __init__(self):\n        register_after_fork(self, lambda obj : obj.__dict__.clear())\n    def __reduce__(self):\n        return type(self), ()\n\n#\n# Close fds except those specified\n#\n\ntry:\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\nexcept Exception:\n    MAXFD = 256\n\ndef close_all_fds_except(fds):\n    fds = list(fds) + [-1, MAXFD]\n    fds.sort()\n    assert fds[-1] == MAXFD, 'fd too large'\n    for i in range(len(fds) - 1):\n        os.closerange(fds[i]+1, fds[i+1])\n#\n# Close sys.stdin and replace stdin with os.devnull\n#\n\ndef _close_stdin():\n    if sys.stdin is None:\n        return\n\n    try:\n        sys.stdin.close()\n    except (OSError, ValueError):\n        pass\n\n    try:\n        fd = os.open(os.devnull, os.O_RDONLY)\n        try:\n            sys.stdin = open(fd, encoding=\"utf-8\", closefd=False)\n        except:\n            os.close(fd)\n            raise\n    except (OSError, ValueError):\n        pass\n\n#\n# Flush standard streams, if any\n#\n\ndef _flush_std_streams():\n    try:\n        sys.stdout.flush()\n    except (AttributeError, ValueError):\n        pass\n    try:\n        sys.stderr.flush()\n    except (AttributeError, ValueError):\n        pass\n\n#\n# Start a program with only specified fds kept open\n#\n\ndef spawnv_passfds(path, args, passfds):\n    import _posixsubprocess\n    import subprocess\n    passfds = tuple(sorted(map(int, passfds)))\n    errpipe_read, errpipe_write = os.pipe()\n    try:\n        return _posixsubprocess.fork_exec(\n            args, [path], True, passfds, None, None,\n            -1, -1, -1, -1, -1, -1, errpipe_read, errpipe_write,\n            False, False, -1, None, None, None, -1, None,\n            subprocess._USE_VFORK)\n    finally:\n        os.close(errpipe_read)\n        os.close(errpipe_write)\n\n\ndef close_fds(*fds):\n    \"\"\"Close each file descriptor given as an argument\"\"\"\n    for fd in fds:\n        os.close(fd)\n\n\ndef _cleanup_tests():\n    \"\"\"Cleanup multiprocessing resources when multiprocessing tests\n    completed.\"\"\"\n\n    from test import support\n\n    # cleanup multiprocessing\n    process._cleanup()\n\n    # Stop the ForkServer process if it's running\n    from multiprocessing import forkserver\n    forkserver._forkserver._stop()\n\n    # Stop the ResourceTracker process if it's running\n    from multiprocessing import resource_tracker\n    resource_tracker._resource_tracker._stop()\n\n    # bpo-37421: Explicitly call _run_finalizers() to remove immediately\n    # temporary directories created by multiprocessing.util.get_temp_dir().\n    _run_finalizers()\n    support.gc_collect()\n\n    support.reap_children()\n", 494]}, "functions": {"_LoaderBasics.exec_module (<frozen importlib._bootstrap_external>:993)": ["<frozen importlib._bootstrap_external>", 993], "_load_unlocked (<frozen importlib._bootstrap>:911)": ["<frozen importlib._bootstrap>", 911], "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)": ["<frozen importlib._bootstrap>", 1304], "_find_and_load (<frozen importlib._bootstrap>:1349)": ["<frozen importlib._bootstrap>", 1349], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dis.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dis.py", 1], "_call_with_frames_removed (<frozen importlib._bootstrap>:480)": ["<frozen importlib._bootstrap>", 480], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\inspect.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\inspect.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dataclasses.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\dataclasses.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\urllib\\parse.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\urllib\\parse.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\pathlib.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\pathlib.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\zipfile\\__init__.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\zipfile\\__init__.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_parseaddr.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_parseaddr.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\charset.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\charset.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\utils.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\utils.py", 1], "_handle_fromlist (<frozen importlib._bootstrap>:1390)": ["<frozen importlib._bootstrap>", 1390], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_policybase.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\_policybase.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\message.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\message.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\_adapters.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\_adapters.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\_common.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\_common.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\__init__.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\resources\\__init__.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\abc.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\abc.py", 1], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 1], "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:1)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py", 1], "Distribution.from_name (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:383)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 383], "distribution (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:856)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 856], "<module> (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\parser.py:1)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\parser.py", 1], "message_from_string (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\__init__.py:31)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\email\\__init__.py", 31], "Distribution.metadata (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:436)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 436], "Distribution.version (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:464)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 464], "version (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py:882)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\importlib\\metadata\\__init__.py", 882], "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py:1)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py", 1], "Random._randbelow_with_getrandbits (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:242)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py", 242], "Random.randrange (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:291)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py", 291], "Random.randint (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py:332)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\random.py", 332], "ObstacleStore.from_config (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:28)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py", 28], "AStarPathfinder._generate_grid (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:64)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py", 64], "AStarPathfinder.__init__ (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py:55)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\pure_python.py", 55], "AStarPathfinder.__init__ (C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py:71)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\a_star\\__init__.py", 71], "<module> (C:\\Users\\naoki\\Documents\\Work\\a_star\\main.py:1)": ["C:\\Users\\naoki\\Documents\\Work\\a_star\\main.py", 1], "Finalize.__call__ (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:208)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py", 208], "_run_finalizers (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:271)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py", 271], "_exit_function (C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py:323)": ["C:\\Users\\naoki\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\multiprocessing\\util.py", 323]}}}